[2023-01-07 00:00:02,994] {processor.py:153} INFO - Started process (PID=6836) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:02,995] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:00:02,995] {logging_mixin.py:115} INFO - [2023-01-07 00:00:02,995] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:03,919] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:00:03,921] {logging_mixin.py:115} INFO - [2023-01-07 00:00:03,921] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:00:03,921] {logging_mixin.py:115} INFO - [2023-01-07 00:00:03,921] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:00:03,928] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:03,950] {logging_mixin.py:115} INFO - [2023-01-07 00:00:03,949] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:00:03,976] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.986 seconds
[2023-01-07 00:00:34,058] {processor.py:153} INFO - Started process (PID=6854) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:34,063] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:00:34,063] {logging_mixin.py:115} INFO - [2023-01-07 00:00:34,063] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:35,025] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:00:35,026] {logging_mixin.py:115} INFO - [2023-01-07 00:00:35,026] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:00:35,027] {logging_mixin.py:115} INFO - [2023-01-07 00:00:35,027] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:00:35,034] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:00:35,056] {logging_mixin.py:115} INFO - [2023-01-07 00:00:35,056] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:00:35,082] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.029 seconds
[2023-01-07 00:01:05,157] {processor.py:153} INFO - Started process (PID=6878) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:05,157] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:01:05,158] {logging_mixin.py:115} INFO - [2023-01-07 00:01:05,158] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:06,022] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:01:06,023] {logging_mixin.py:115} INFO - [2023-01-07 00:01:06,023] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:01:06,024] {logging_mixin.py:115} INFO - [2023-01-07 00:01:06,023] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:01:06,030] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:06,052] {logging_mixin.py:115} INFO - [2023-01-07 00:01:06,052] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:01:06,073] {logging_mixin.py:115} INFO - [2023-01-07 00:01:06,073] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:01:06,082] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 00:01:36,160] {processor.py:153} INFO - Started process (PID=6904) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:36,161] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:01:36,161] {logging_mixin.py:115} INFO - [2023-01-07 00:01:36,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:37,056] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:01:37,058] {logging_mixin.py:115} INFO - [2023-01-07 00:01:37,058] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:01:37,059] {logging_mixin.py:115} INFO - [2023-01-07 00:01:37,058] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:01:37,065] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:01:37,087] {logging_mixin.py:115} INFO - [2023-01-07 00:01:37,087] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:01:37,107] {logging_mixin.py:115} INFO - [2023-01-07 00:01:37,107] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:01:37,116] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 00:02:07,190] {processor.py:153} INFO - Started process (PID=6928) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:07,191] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:02:07,192] {logging_mixin.py:115} INFO - [2023-01-07 00:02:07,192] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:08,086] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:02:08,087] {logging_mixin.py:115} INFO - [2023-01-07 00:02:08,087] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:02:08,088] {logging_mixin.py:115} INFO - [2023-01-07 00:02:08,088] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:02:08,094] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:08,115] {logging_mixin.py:115} INFO - [2023-01-07 00:02:08,115] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:02:08,135] {logging_mixin.py:115} INFO - [2023-01-07 00:02:08,135] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:02:08,144] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 00:02:38,213] {processor.py:153} INFO - Started process (PID=6946) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:38,213] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:02:38,214] {logging_mixin.py:115} INFO - [2023-01-07 00:02:38,214] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:39,126] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:02:39,127] {logging_mixin.py:115} INFO - [2023-01-07 00:02:39,127] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:02:39,128] {logging_mixin.py:115} INFO - [2023-01-07 00:02:39,128] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:02:39,134] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:02:39,163] {logging_mixin.py:115} INFO - [2023-01-07 00:02:39,163] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:02:39,192] {logging_mixin.py:115} INFO - [2023-01-07 00:02:39,192] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:02:39,203] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.995 seconds
[2023-01-07 00:03:09,279] {processor.py:153} INFO - Started process (PID=6971) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:09,281] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:03:09,281] {logging_mixin.py:115} INFO - [2023-01-07 00:03:09,281] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:10,125] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:03:10,127] {logging_mixin.py:115} INFO - [2023-01-07 00:03:10,127] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:03:10,127] {logging_mixin.py:115} INFO - [2023-01-07 00:03:10,127] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:03:10,134] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:10,155] {logging_mixin.py:115} INFO - [2023-01-07 00:03:10,155] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:03:10,175] {logging_mixin.py:115} INFO - [2023-01-07 00:03:10,175] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:03:10,184] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.909 seconds
[2023-01-07 00:03:40,251] {processor.py:153} INFO - Started process (PID=6996) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:40,251] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:03:40,252] {logging_mixin.py:115} INFO - [2023-01-07 00:03:40,252] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:41,099] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:03:41,100] {logging_mixin.py:115} INFO - [2023-01-07 00:03:41,100] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:03:41,100] {logging_mixin.py:115} INFO - [2023-01-07 00:03:41,100] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:03:41,107] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:03:41,128] {logging_mixin.py:115} INFO - [2023-01-07 00:03:41,128] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:03:41,151] {logging_mixin.py:115} INFO - [2023-01-07 00:03:41,151] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:03:41,161] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-07 00:04:11,229] {processor.py:153} INFO - Started process (PID=7022) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:11,231] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:04:11,232] {logging_mixin.py:115} INFO - [2023-01-07 00:04:11,232] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:12,083] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:04:12,084] {logging_mixin.py:115} INFO - [2023-01-07 00:04:12,084] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:04:12,085] {logging_mixin.py:115} INFO - [2023-01-07 00:04:12,085] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:04:12,096] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:12,118] {logging_mixin.py:115} INFO - [2023-01-07 00:04:12,118] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:04:12,138] {logging_mixin.py:115} INFO - [2023-01-07 00:04:12,138] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:04:12,147] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 00:04:42,218] {processor.py:153} INFO - Started process (PID=7040) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:42,220] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:04:42,220] {logging_mixin.py:115} INFO - [2023-01-07 00:04:42,220] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:43,109] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:04:43,110] {logging_mixin.py:115} INFO - [2023-01-07 00:04:43,110] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:04:43,111] {logging_mixin.py:115} INFO - [2023-01-07 00:04:43,111] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:04:43,117] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:04:43,139] {logging_mixin.py:115} INFO - [2023-01-07 00:04:43,138] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:04:43,159] {logging_mixin.py:115} INFO - [2023-01-07 00:04:43,159] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:04:43,168] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 00:05:13,239] {processor.py:153} INFO - Started process (PID=7065) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:13,240] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:05:13,241] {logging_mixin.py:115} INFO - [2023-01-07 00:05:13,241] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:14,109] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:05:14,111] {logging_mixin.py:115} INFO - [2023-01-07 00:05:14,111] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:05:14,111] {logging_mixin.py:115} INFO - [2023-01-07 00:05:14,111] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:05:14,118] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:14,140] {logging_mixin.py:115} INFO - [2023-01-07 00:05:14,140] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:05:14,161] {logging_mixin.py:115} INFO - [2023-01-07 00:05:14,160] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:05:14,170] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.936 seconds
[2023-01-07 00:05:44,252] {processor.py:153} INFO - Started process (PID=7090) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:44,253] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:05:44,254] {logging_mixin.py:115} INFO - [2023-01-07 00:05:44,254] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:45,097] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:05:45,099] {logging_mixin.py:115} INFO - [2023-01-07 00:05:45,099] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:05:45,099] {logging_mixin.py:115} INFO - [2023-01-07 00:05:45,099] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:05:45,106] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:05:45,127] {logging_mixin.py:115} INFO - [2023-01-07 00:05:45,127] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:05:45,147] {logging_mixin.py:115} INFO - [2023-01-07 00:05:45,147] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:05:45,156] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.908 seconds
[2023-01-07 00:06:15,226] {processor.py:153} INFO - Started process (PID=7115) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:15,227] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:06:15,229] {logging_mixin.py:115} INFO - [2023-01-07 00:06:15,229] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:16,080] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:06:16,082] {logging_mixin.py:115} INFO - [2023-01-07 00:06:16,081] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:06:16,082] {logging_mixin.py:115} INFO - [2023-01-07 00:06:16,082] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:06:16,089] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:16,110] {logging_mixin.py:115} INFO - [2023-01-07 00:06:16,109] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:06:16,129] {logging_mixin.py:115} INFO - [2023-01-07 00:06:16,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:06:16,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 00:06:46,216] {processor.py:153} INFO - Started process (PID=7133) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:46,219] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:06:46,220] {logging_mixin.py:115} INFO - [2023-01-07 00:06:46,220] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:47,083] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:06:47,084] {logging_mixin.py:115} INFO - [2023-01-07 00:06:47,084] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:06:47,085] {logging_mixin.py:115} INFO - [2023-01-07 00:06:47,084] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:06:47,091] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:06:47,113] {logging_mixin.py:115} INFO - [2023-01-07 00:06:47,113] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:06:47,134] {logging_mixin.py:115} INFO - [2023-01-07 00:06:47,134] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:06:47,149] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 00:07:17,245] {processor.py:153} INFO - Started process (PID=7158) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:17,248] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:07:17,250] {logging_mixin.py:115} INFO - [2023-01-07 00:07:17,250] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:18,116] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:07:18,118] {logging_mixin.py:115} INFO - [2023-01-07 00:07:18,118] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:07:18,118] {logging_mixin.py:115} INFO - [2023-01-07 00:07:18,118] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:07:18,125] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:18,147] {logging_mixin.py:115} INFO - [2023-01-07 00:07:18,146] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:07:18,167] {logging_mixin.py:115} INFO - [2023-01-07 00:07:18,167] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:07:18,176] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.936 seconds
[2023-01-07 00:07:48,274] {processor.py:153} INFO - Started process (PID=7184) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:48,274] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:07:48,276] {logging_mixin.py:115} INFO - [2023-01-07 00:07:48,276] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:49,116] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:07:49,117] {logging_mixin.py:115} INFO - [2023-01-07 00:07:49,117] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:07:49,118] {logging_mixin.py:115} INFO - [2023-01-07 00:07:49,117] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:07:49,124] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:07:49,151] {logging_mixin.py:115} INFO - [2023-01-07 00:07:49,151] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:07:49,171] {logging_mixin.py:115} INFO - [2023-01-07 00:07:49,171] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:07:49,180] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-07 00:08:19,277] {processor.py:153} INFO - Started process (PID=7210) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:19,279] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:08:19,280] {logging_mixin.py:115} INFO - [2023-01-07 00:08:19,280] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:20,165] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:08:20,166] {logging_mixin.py:115} INFO - [2023-01-07 00:08:20,166] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:08:20,166] {logging_mixin.py:115} INFO - [2023-01-07 00:08:20,166] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:08:20,173] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:20,194] {logging_mixin.py:115} INFO - [2023-01-07 00:08:20,194] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:08:20,214] {logging_mixin.py:115} INFO - [2023-01-07 00:08:20,214] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:08:20,223] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.951 seconds
[2023-01-07 00:08:50,398] {processor.py:153} INFO - Started process (PID=7228) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:50,400] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:08:50,401] {logging_mixin.py:115} INFO - [2023-01-07 00:08:50,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:51,347] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:08:51,348] {logging_mixin.py:115} INFO - [2023-01-07 00:08:51,348] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:08:51,348] {logging_mixin.py:115} INFO - [2023-01-07 00:08:51,348] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:08:51,355] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:08:51,377] {logging_mixin.py:115} INFO - [2023-01-07 00:08:51,377] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:08:51,397] {logging_mixin.py:115} INFO - [2023-01-07 00:08:51,397] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:08:51,407] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.013 seconds
[2023-01-07 00:09:21,474] {processor.py:153} INFO - Started process (PID=7254) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:21,475] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:09:21,477] {logging_mixin.py:115} INFO - [2023-01-07 00:09:21,476] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:22,359] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:09:22,360] {logging_mixin.py:115} INFO - [2023-01-07 00:09:22,360] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:09:22,361] {logging_mixin.py:115} INFO - [2023-01-07 00:09:22,361] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:09:22,372] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:22,395] {logging_mixin.py:115} INFO - [2023-01-07 00:09:22,395] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:09:22,416] {logging_mixin.py:115} INFO - [2023-01-07 00:09:22,416] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:09:22,426] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 00:09:52,494] {processor.py:153} INFO - Started process (PID=7280) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:52,495] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:09:52,496] {logging_mixin.py:115} INFO - [2023-01-07 00:09:52,496] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:53,374] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:09:53,376] {logging_mixin.py:115} INFO - [2023-01-07 00:09:53,376] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:09:53,376] {logging_mixin.py:115} INFO - [2023-01-07 00:09:53,376] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:09:53,383] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:09:53,404] {logging_mixin.py:115} INFO - [2023-01-07 00:09:53,404] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:09:53,424] {logging_mixin.py:115} INFO - [2023-01-07 00:09:53,424] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:09:53,433] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.944 seconds
[2023-01-07 00:10:23,503] {processor.py:153} INFO - Started process (PID=7306) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:23,503] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:10:23,505] {logging_mixin.py:115} INFO - [2023-01-07 00:10:23,505] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:24,351] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:10:24,352] {logging_mixin.py:115} INFO - [2023-01-07 00:10:24,352] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:10:24,353] {logging_mixin.py:115} INFO - [2023-01-07 00:10:24,352] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:10:24,359] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:24,382] {logging_mixin.py:115} INFO - [2023-01-07 00:10:24,381] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:10:24,402] {logging_mixin.py:115} INFO - [2023-01-07 00:10:24,402] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:10:24,411] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.913 seconds
[2023-01-07 00:10:54,478] {processor.py:153} INFO - Started process (PID=7331) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:54,481] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:10:54,482] {logging_mixin.py:115} INFO - [2023-01-07 00:10:54,482] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:55,341] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:10:55,342] {logging_mixin.py:115} INFO - [2023-01-07 00:10:55,342] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:10:55,343] {logging_mixin.py:115} INFO - [2023-01-07 00:10:55,343] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:10:55,350] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:10:55,371] {logging_mixin.py:115} INFO - [2023-01-07 00:10:55,371] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:10:55,391] {logging_mixin.py:115} INFO - [2023-01-07 00:10:55,391] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:10:55,400] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-07 00:11:25,469] {processor.py:153} INFO - Started process (PID=7349) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:25,469] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:11:25,470] {logging_mixin.py:115} INFO - [2023-01-07 00:11:25,470] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:26,414] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:11:26,415] {logging_mixin.py:115} INFO - [2023-01-07 00:11:26,415] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:11:26,416] {logging_mixin.py:115} INFO - [2023-01-07 00:11:26,415] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:11:26,422] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:26,451] {logging_mixin.py:115} INFO - [2023-01-07 00:11:26,451] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:11:26,482] {logging_mixin.py:115} INFO - [2023-01-07 00:11:26,481] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:11:26,493] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.029 seconds
[2023-01-07 00:11:56,566] {processor.py:153} INFO - Started process (PID=7374) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:56,567] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:11:56,568] {logging_mixin.py:115} INFO - [2023-01-07 00:11:56,568] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:57,439] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:11:57,440] {logging_mixin.py:115} INFO - [2023-01-07 00:11:57,440] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:11:57,441] {logging_mixin.py:115} INFO - [2023-01-07 00:11:57,440] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:11:57,447] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:11:57,472] {logging_mixin.py:115} INFO - [2023-01-07 00:11:57,472] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:11:57,492] {logging_mixin.py:115} INFO - [2023-01-07 00:11:57,492] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:11:57,503] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 00:12:27,573] {processor.py:153} INFO - Started process (PID=7400) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:27,574] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:12:27,574] {logging_mixin.py:115} INFO - [2023-01-07 00:12:27,574] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:28,420] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:12:28,421] {logging_mixin.py:115} INFO - [2023-01-07 00:12:28,421] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:12:28,422] {logging_mixin.py:115} INFO - [2023-01-07 00:12:28,421] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:12:28,428] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:28,450] {logging_mixin.py:115} INFO - [2023-01-07 00:12:28,449] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:12:28,469] {logging_mixin.py:115} INFO - [2023-01-07 00:12:28,469] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:12:28,478] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.910 seconds
[2023-01-07 00:12:58,543] {processor.py:153} INFO - Started process (PID=7425) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:58,544] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:12:58,544] {logging_mixin.py:115} INFO - [2023-01-07 00:12:58,544] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:59,396] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:12:59,397] {logging_mixin.py:115} INFO - [2023-01-07 00:12:59,397] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:12:59,398] {logging_mixin.py:115} INFO - [2023-01-07 00:12:59,397] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:12:59,404] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:12:59,426] {logging_mixin.py:115} INFO - [2023-01-07 00:12:59,425] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:12:59,445] {logging_mixin.py:115} INFO - [2023-01-07 00:12:59,445] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:12:59,454] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-07 00:13:29,540] {processor.py:153} INFO - Started process (PID=7443) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:13:29,541] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:13:29,542] {logging_mixin.py:115} INFO - [2023-01-07 00:13:29,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:13:30,425] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:13:30,427] {logging_mixin.py:115} INFO - [2023-01-07 00:13:30,427] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:13:30,427] {logging_mixin.py:115} INFO - [2023-01-07 00:13:30,427] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:13:30,434] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:13:30,463] {logging_mixin.py:115} INFO - [2023-01-07 00:13:30,462] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:13:30,492] {logging_mixin.py:115} INFO - [2023-01-07 00:13:30,492] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:13:30,504] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.969 seconds
[2023-01-07 00:14:00,580] {processor.py:153} INFO - Started process (PID=7468) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:00,582] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:14:00,582] {logging_mixin.py:115} INFO - [2023-01-07 00:14:00,582] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:01,435] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:14:01,437] {logging_mixin.py:115} INFO - [2023-01-07 00:14:01,436] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:14:01,437] {logging_mixin.py:115} INFO - [2023-01-07 00:14:01,437] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:14:01,444] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:01,465] {logging_mixin.py:115} INFO - [2023-01-07 00:14:01,465] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:14:01,485] {logging_mixin.py:115} INFO - [2023-01-07 00:14:01,485] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:14:01,494] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 00:14:31,573] {processor.py:153} INFO - Started process (PID=7493) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:31,574] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:14:31,575] {logging_mixin.py:115} INFO - [2023-01-07 00:14:31,575] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:32,434] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:14:32,435] {logging_mixin.py:115} INFO - [2023-01-07 00:14:32,435] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:14:32,436] {logging_mixin.py:115} INFO - [2023-01-07 00:14:32,436] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:14:32,443] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:14:32,464] {logging_mixin.py:115} INFO - [2023-01-07 00:14:32,464] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:14:32,484] {logging_mixin.py:115} INFO - [2023-01-07 00:14:32,484] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:14:32,494] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-07 00:15:02,577] {processor.py:153} INFO - Started process (PID=7518) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:02,578] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:15:02,578] {logging_mixin.py:115} INFO - [2023-01-07 00:15:02,578] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:03,488] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:15:03,489] {logging_mixin.py:115} INFO - [2023-01-07 00:15:03,489] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:15:03,490] {logging_mixin.py:115} INFO - [2023-01-07 00:15:03,489] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:15:03,496] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:03,518] {logging_mixin.py:115} INFO - [2023-01-07 00:15:03,517] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:15:03,538] {logging_mixin.py:115} INFO - [2023-01-07 00:15:03,538] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:15:03,547] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.975 seconds
[2023-01-07 00:15:33,640] {processor.py:153} INFO - Started process (PID=7536) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:33,641] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:15:33,641] {logging_mixin.py:115} INFO - [2023-01-07 00:15:33,641] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:34,512] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:15:34,513] {logging_mixin.py:115} INFO - [2023-01-07 00:15:34,513] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:15:34,514] {logging_mixin.py:115} INFO - [2023-01-07 00:15:34,514] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:15:34,524] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:15:34,553] {logging_mixin.py:115} INFO - [2023-01-07 00:15:34,553] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:15:34,583] {logging_mixin.py:115} INFO - [2023-01-07 00:15:34,583] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:15:34,596] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 00:16:04,654] {processor.py:153} INFO - Started process (PID=7562) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:04,655] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:16:04,655] {logging_mixin.py:115} INFO - [2023-01-07 00:16:04,655] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:05,534] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:16:05,535] {logging_mixin.py:115} INFO - [2023-01-07 00:16:05,535] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:16:05,536] {logging_mixin.py:115} INFO - [2023-01-07 00:16:05,535] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:16:05,542] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:05,564] {logging_mixin.py:115} INFO - [2023-01-07 00:16:05,564] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:16:05,584] {logging_mixin.py:115} INFO - [2023-01-07 00:16:05,584] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:16:05,594] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.945 seconds
[2023-01-07 00:16:35,843] {processor.py:153} INFO - Started process (PID=7587) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:35,844] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:16:35,844] {logging_mixin.py:115} INFO - [2023-01-07 00:16:35,844] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:36,744] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:16:36,746] {logging_mixin.py:115} INFO - [2023-01-07 00:16:36,746] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:16:36,746] {logging_mixin.py:115} INFO - [2023-01-07 00:16:36,746] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:16:36,753] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:16:36,778] {logging_mixin.py:115} INFO - [2023-01-07 00:16:36,778] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:16:36,799] {logging_mixin.py:115} INFO - [2023-01-07 00:16:36,799] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:16:36,808] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.969 seconds
[2023-01-07 00:17:06,883] {processor.py:153} INFO - Started process (PID=7612) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:06,884] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:17:06,884] {logging_mixin.py:115} INFO - [2023-01-07 00:17:06,884] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:07,740] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:17:07,741] {logging_mixin.py:115} INFO - [2023-01-07 00:17:07,741] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:17:07,742] {logging_mixin.py:115} INFO - [2023-01-07 00:17:07,741] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:17:07,748] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:07,770] {logging_mixin.py:115} INFO - [2023-01-07 00:17:07,769] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:17:07,790] {logging_mixin.py:115} INFO - [2023-01-07 00:17:07,789] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:17:07,799] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 00:17:37,864] {processor.py:153} INFO - Started process (PID=7629) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:37,866] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:17:37,866] {logging_mixin.py:115} INFO - [2023-01-07 00:17:37,866] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:38,763] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:17:38,764] {logging_mixin.py:115} INFO - [2023-01-07 00:17:38,764] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:17:38,765] {logging_mixin.py:115} INFO - [2023-01-07 00:17:38,764] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:17:38,771] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:17:38,793] {logging_mixin.py:115} INFO - [2023-01-07 00:17:38,793] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:17:38,813] {logging_mixin.py:115} INFO - [2023-01-07 00:17:38,813] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:17:38,822] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 00:18:08,893] {processor.py:153} INFO - Started process (PID=7654) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:08,894] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:18:08,894] {logging_mixin.py:115} INFO - [2023-01-07 00:18:08,894] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:09,754] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:18:09,755] {logging_mixin.py:115} INFO - [2023-01-07 00:18:09,755] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:18:09,755] {logging_mixin.py:115} INFO - [2023-01-07 00:18:09,755] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:18:09,762] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:09,791] {logging_mixin.py:115} INFO - [2023-01-07 00:18:09,790] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:18:09,819] {logging_mixin.py:115} INFO - [2023-01-07 00:18:09,819] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:18:09,829] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 00:18:39,916] {processor.py:153} INFO - Started process (PID=7679) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:39,917] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:18:39,917] {logging_mixin.py:115} INFO - [2023-01-07 00:18:39,917] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:40,772] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:18:40,774] {logging_mixin.py:115} INFO - [2023-01-07 00:18:40,773] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:18:40,774] {logging_mixin.py:115} INFO - [2023-01-07 00:18:40,774] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:18:40,781] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:18:40,802] {logging_mixin.py:115} INFO - [2023-01-07 00:18:40,802] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:18:40,823] {logging_mixin.py:115} INFO - [2023-01-07 00:18:40,823] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:18:40,832] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 00:19:10,929] {processor.py:153} INFO - Started process (PID=7704) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:10,930] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:19:10,931] {logging_mixin.py:115} INFO - [2023-01-07 00:19:10,930] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:11,783] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:19:11,784] {logging_mixin.py:115} INFO - [2023-01-07 00:19:11,784] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:19:11,784] {logging_mixin.py:115} INFO - [2023-01-07 00:19:11,784] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:19:11,793] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:11,816] {logging_mixin.py:115} INFO - [2023-01-07 00:19:11,816] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:19:11,836] {logging_mixin.py:115} INFO - [2023-01-07 00:19:11,836] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:19:11,845] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 00:19:41,944] {processor.py:153} INFO - Started process (PID=7730) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:41,946] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:19:41,947] {logging_mixin.py:115} INFO - [2023-01-07 00:19:41,946] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:43,081] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:19:43,083] {logging_mixin.py:115} INFO - [2023-01-07 00:19:43,083] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:19:43,083] {logging_mixin.py:115} INFO - [2023-01-07 00:19:43,083] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:19:43,090] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:19:43,111] {logging_mixin.py:115} INFO - [2023-01-07 00:19:43,111] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:19:43,131] {logging_mixin.py:115} INFO - [2023-01-07 00:19:43,131] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:19:43,141] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.202 seconds
[2023-01-07 00:20:13,207] {processor.py:153} INFO - Started process (PID=7748) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:13,208] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:20:13,208] {logging_mixin.py:115} INFO - [2023-01-07 00:20:13,208] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:14,150] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:20:14,151] {logging_mixin.py:115} INFO - [2023-01-07 00:20:14,151] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:20:14,152] {logging_mixin.py:115} INFO - [2023-01-07 00:20:14,152] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:20:14,159] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:14,181] {logging_mixin.py:115} INFO - [2023-01-07 00:20:14,180] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:20:14,201] {logging_mixin.py:115} INFO - [2023-01-07 00:20:14,201] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:20:14,211] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.009 seconds
[2023-01-07 00:20:44,282] {processor.py:153} INFO - Started process (PID=7774) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:44,284] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:20:44,284] {logging_mixin.py:115} INFO - [2023-01-07 00:20:44,284] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:45,131] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:20:45,132] {logging_mixin.py:115} INFO - [2023-01-07 00:20:45,132] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:20:45,133] {logging_mixin.py:115} INFO - [2023-01-07 00:20:45,132] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:20:45,139] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:20:45,161] {logging_mixin.py:115} INFO - [2023-01-07 00:20:45,160] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:20:45,181] {logging_mixin.py:115} INFO - [2023-01-07 00:20:45,181] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:20:45,190] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.914 seconds
[2023-01-07 00:21:15,322] {processor.py:153} INFO - Started process (PID=7800) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:15,324] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:21:15,324] {logging_mixin.py:115} INFO - [2023-01-07 00:21:15,324] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:16,193] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:21:16,194] {logging_mixin.py:115} INFO - [2023-01-07 00:21:16,194] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:21:16,195] {logging_mixin.py:115} INFO - [2023-01-07 00:21:16,194] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:21:16,201] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:16,223] {logging_mixin.py:115} INFO - [2023-01-07 00:21:16,223] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:21:16,243] {logging_mixin.py:115} INFO - [2023-01-07 00:21:16,243] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:21:16,252] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:21:46,318] {processor.py:153} INFO - Started process (PID=7825) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:46,318] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:21:46,319] {logging_mixin.py:115} INFO - [2023-01-07 00:21:46,319] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:47,170] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:21:47,171] {logging_mixin.py:115} INFO - [2023-01-07 00:21:47,171] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:21:47,172] {logging_mixin.py:115} INFO - [2023-01-07 00:21:47,171] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:21:47,178] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:21:47,199] {logging_mixin.py:115} INFO - [2023-01-07 00:21:47,199] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:21:47,220] {logging_mixin.py:115} INFO - [2023-01-07 00:21:47,220] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:21:47,229] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-07 00:22:17,298] {processor.py:153} INFO - Started process (PID=7842) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:17,299] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:22:17,299] {logging_mixin.py:115} INFO - [2023-01-07 00:22:17,299] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:18,177] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:22:18,178] {logging_mixin.py:115} INFO - [2023-01-07 00:22:18,178] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:22:18,178] {logging_mixin.py:115} INFO - [2023-01-07 00:22:18,178] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:22:18,185] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:18,207] {logging_mixin.py:115} INFO - [2023-01-07 00:22:18,207] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:22:18,232] {logging_mixin.py:115} INFO - [2023-01-07 00:22:18,232] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:22:18,241] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.948 seconds
[2023-01-07 00:22:48,309] {processor.py:153} INFO - Started process (PID=7866) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:48,310] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:22:48,311] {logging_mixin.py:115} INFO - [2023-01-07 00:22:48,311] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:49,187] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:22:49,188] {logging_mixin.py:115} INFO - [2023-01-07 00:22:49,188] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:22:49,189] {logging_mixin.py:115} INFO - [2023-01-07 00:22:49,188] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:22:49,195] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:22:49,216] {logging_mixin.py:115} INFO - [2023-01-07 00:22:49,216] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:22:49,236] {logging_mixin.py:115} INFO - [2023-01-07 00:22:49,236] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:22:49,245] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 00:23:19,315] {processor.py:153} INFO - Started process (PID=7891) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:19,316] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:23:19,317] {logging_mixin.py:115} INFO - [2023-01-07 00:23:19,317] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:20,211] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:23:20,212] {logging_mixin.py:115} INFO - [2023-01-07 00:23:20,212] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:23:20,212] {logging_mixin.py:115} INFO - [2023-01-07 00:23:20,212] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:23:20,219] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:20,241] {logging_mixin.py:115} INFO - [2023-01-07 00:23:20,240] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:23:20,261] {logging_mixin.py:115} INFO - [2023-01-07 00:23:20,261] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:23:20,270] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.959 seconds
[2023-01-07 00:23:50,351] {processor.py:153} INFO - Started process (PID=7916) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:50,352] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:23:50,352] {logging_mixin.py:115} INFO - [2023-01-07 00:23:50,352] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:51,217] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:23:51,218] {logging_mixin.py:115} INFO - [2023-01-07 00:23:51,218] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:23:51,219] {logging_mixin.py:115} INFO - [2023-01-07 00:23:51,218] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:23:51,225] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:23:51,247] {logging_mixin.py:115} INFO - [2023-01-07 00:23:51,247] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:23:51,267] {logging_mixin.py:115} INFO - [2023-01-07 00:23:51,267] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:23:51,276] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 00:24:21,375] {processor.py:153} INFO - Started process (PID=7934) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:21,376] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:24:21,377] {logging_mixin.py:115} INFO - [2023-01-07 00:24:21,377] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:22,248] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:24:22,249] {logging_mixin.py:115} INFO - [2023-01-07 00:24:22,249] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:24:22,249] {logging_mixin.py:115} INFO - [2023-01-07 00:24:22,249] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:24:22,256] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:22,284] {logging_mixin.py:115} INFO - [2023-01-07 00:24:22,284] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:24:22,313] {logging_mixin.py:115} INFO - [2023-01-07 00:24:22,313] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:24:22,325] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-07 00:24:52,421] {processor.py:153} INFO - Started process (PID=7961) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:52,423] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:24:52,423] {logging_mixin.py:115} INFO - [2023-01-07 00:24:52,423] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:53,288] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:24:53,289] {logging_mixin.py:115} INFO - [2023-01-07 00:24:53,289] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:24:53,290] {logging_mixin.py:115} INFO - [2023-01-07 00:24:53,290] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:24:53,297] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:24:53,318] {logging_mixin.py:115} INFO - [2023-01-07 00:24:53,318] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:24:53,339] {logging_mixin.py:115} INFO - [2023-01-07 00:24:53,338] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:24:53,348] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.931 seconds
[2023-01-07 00:25:23,434] {processor.py:153} INFO - Started process (PID=7986) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:23,435] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:25:23,435] {logging_mixin.py:115} INFO - [2023-01-07 00:25:23,435] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:24,282] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:25:24,283] {logging_mixin.py:115} INFO - [2023-01-07 00:25:24,283] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:25:24,284] {logging_mixin.py:115} INFO - [2023-01-07 00:25:24,283] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:25:24,290] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:24,312] {logging_mixin.py:115} INFO - [2023-01-07 00:25:24,312] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:25:24,332] {logging_mixin.py:115} INFO - [2023-01-07 00:25:24,332] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:25:24,341] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.913 seconds
[2023-01-07 00:25:54,601] {processor.py:153} INFO - Started process (PID=8011) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:54,603] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:25:54,603] {logging_mixin.py:115} INFO - [2023-01-07 00:25:54,603] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:55,476] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:25:55,478] {logging_mixin.py:115} INFO - [2023-01-07 00:25:55,477] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:25:55,478] {logging_mixin.py:115} INFO - [2023-01-07 00:25:55,478] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:25:55,485] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:25:55,506] {logging_mixin.py:115} INFO - [2023-01-07 00:25:55,506] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:25:55,526] {logging_mixin.py:115} INFO - [2023-01-07 00:25:55,526] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:25:55,535] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 00:26:25,604] {processor.py:153} INFO - Started process (PID=8029) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:25,605] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:26:25,606] {logging_mixin.py:115} INFO - [2023-01-07 00:26:25,605] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:26,469] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:26:26,470] {logging_mixin.py:115} INFO - [2023-01-07 00:26:26,470] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:26:26,471] {logging_mixin.py:115} INFO - [2023-01-07 00:26:26,470] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:26:26,477] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:26,500] {logging_mixin.py:115} INFO - [2023-01-07 00:26:26,499] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:26:26,527] {logging_mixin.py:115} INFO - [2023-01-07 00:26:26,527] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:26:26,536] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 00:26:56,605] {processor.py:153} INFO - Started process (PID=8055) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:56,607] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:26:56,607] {logging_mixin.py:115} INFO - [2023-01-07 00:26:56,607] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:57,467] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:26:57,468] {logging_mixin.py:115} INFO - [2023-01-07 00:26:57,468] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:26:57,468] {logging_mixin.py:115} INFO - [2023-01-07 00:26:57,468] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:26:57,475] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:26:57,497] {logging_mixin.py:115} INFO - [2023-01-07 00:26:57,497] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:26:57,517] {logging_mixin.py:115} INFO - [2023-01-07 00:26:57,517] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:26:57,528] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 00:27:27,617] {processor.py:153} INFO - Started process (PID=8080) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:27,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:27:27,619] {logging_mixin.py:115} INFO - [2023-01-07 00:27:27,619] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:28,456] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:27:28,457] {logging_mixin.py:115} INFO - [2023-01-07 00:27:28,457] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:27:28,458] {logging_mixin.py:115} INFO - [2023-01-07 00:27:28,458] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:27:28,464] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:28,486] {logging_mixin.py:115} INFO - [2023-01-07 00:27:28,486] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:27:28,506] {logging_mixin.py:115} INFO - [2023-01-07 00:27:28,506] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:27:28,516] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-07 00:27:58,597] {processor.py:153} INFO - Started process (PID=8105) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:58,599] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:27:58,599] {logging_mixin.py:115} INFO - [2023-01-07 00:27:58,599] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:59,452] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:27:59,453] {logging_mixin.py:115} INFO - [2023-01-07 00:27:59,453] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:27:59,453] {logging_mixin.py:115} INFO - [2023-01-07 00:27:59,453] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:27:59,460] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:27:59,482] {logging_mixin.py:115} INFO - [2023-01-07 00:27:59,482] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:27:59,502] {logging_mixin.py:115} INFO - [2023-01-07 00:27:59,502] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:27:59,511] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 00:28:29,608] {processor.py:153} INFO - Started process (PID=8129) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:28:29,609] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:28:29,610] {logging_mixin.py:115} INFO - [2023-01-07 00:28:29,610] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:28:30,484] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:28:30,485] {logging_mixin.py:115} INFO - [2023-01-07 00:28:30,485] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:28:30,486] {logging_mixin.py:115} INFO - [2023-01-07 00:28:30,485] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:28:30,492] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:28:30,514] {logging_mixin.py:115} INFO - [2023-01-07 00:28:30,514] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:28:30,536] {logging_mixin.py:115} INFO - [2023-01-07 00:28:30,535] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:28:30,545] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 00:29:00,637] {processor.py:153} INFO - Started process (PID=8147) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:00,638] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:29:00,639] {logging_mixin.py:115} INFO - [2023-01-07 00:29:00,639] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:01,627] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:29:01,628] {logging_mixin.py:115} INFO - [2023-01-07 00:29:01,628] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:29:01,628] {logging_mixin.py:115} INFO - [2023-01-07 00:29:01,628] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:29:01,635] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:01,657] {logging_mixin.py:115} INFO - [2023-01-07 00:29:01,656] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:29:01,677] {logging_mixin.py:115} INFO - [2023-01-07 00:29:01,677] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:29:01,686] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.053 seconds
[2023-01-07 00:29:31,754] {processor.py:153} INFO - Started process (PID=8172) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:31,754] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:29:31,755] {logging_mixin.py:115} INFO - [2023-01-07 00:29:31,755] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:32,628] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:29:32,629] {logging_mixin.py:115} INFO - [2023-01-07 00:29:32,629] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:29:32,629] {logging_mixin.py:115} INFO - [2023-01-07 00:29:32,629] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:29:32,636] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:29:32,657] {logging_mixin.py:115} INFO - [2023-01-07 00:29:32,657] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:29:32,678] {logging_mixin.py:115} INFO - [2023-01-07 00:29:32,678] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:29:32,688] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 00:30:02,755] {processor.py:153} INFO - Started process (PID=8197) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:02,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:30:02,757] {logging_mixin.py:115} INFO - [2023-01-07 00:30:02,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:03,623] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:30:03,624] {logging_mixin.py:115} INFO - [2023-01-07 00:30:03,624] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:30:03,625] {logging_mixin.py:115} INFO - [2023-01-07 00:30:03,624] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:30:03,632] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:03,654] {logging_mixin.py:115} INFO - [2023-01-07 00:30:03,653] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:30:03,674] {logging_mixin.py:115} INFO - [2023-01-07 00:30:03,674] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:30:03,683] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-07 00:30:33,752] {processor.py:153} INFO - Started process (PID=8223) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:33,753] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:30:33,754] {logging_mixin.py:115} INFO - [2023-01-07 00:30:33,754] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:34,609] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:30:34,610] {logging_mixin.py:115} INFO - [2023-01-07 00:30:34,610] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:30:34,611] {logging_mixin.py:115} INFO - [2023-01-07 00:30:34,610] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:30:34,617] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:30:34,638] {logging_mixin.py:115} INFO - [2023-01-07 00:30:34,638] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:30:34,658] {logging_mixin.py:115} INFO - [2023-01-07 00:30:34,658] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:30:34,667] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 00:31:04,738] {processor.py:153} INFO - Started process (PID=8241) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:04,739] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:31:04,740] {logging_mixin.py:115} INFO - [2023-01-07 00:31:04,740] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:05,795] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:31:05,796] {logging_mixin.py:115} INFO - [2023-01-07 00:31:05,796] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:31:05,797] {logging_mixin.py:115} INFO - [2023-01-07 00:31:05,796] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:31:05,803] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:05,825] {logging_mixin.py:115} INFO - [2023-01-07 00:31:05,825] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:31:05,846] {logging_mixin.py:115} INFO - [2023-01-07 00:31:05,845] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:31:05,855] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.122 seconds
[2023-01-07 00:31:35,923] {processor.py:153} INFO - Started process (PID=8266) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:35,924] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:31:35,925] {logging_mixin.py:115} INFO - [2023-01-07 00:31:35,925] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:36,777] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:31:36,778] {logging_mixin.py:115} INFO - [2023-01-07 00:31:36,778] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:31:36,779] {logging_mixin.py:115} INFO - [2023-01-07 00:31:36,779] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:31:36,785] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:31:36,807] {logging_mixin.py:115} INFO - [2023-01-07 00:31:36,806] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:31:36,826] {logging_mixin.py:115} INFO - [2023-01-07 00:31:36,826] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:31:36,835] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 00:32:06,904] {processor.py:153} INFO - Started process (PID=8291) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:06,907] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:32:06,908] {logging_mixin.py:115} INFO - [2023-01-07 00:32:06,907] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:07,772] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:32:07,773] {logging_mixin.py:115} INFO - [2023-01-07 00:32:07,773] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:32:07,773] {logging_mixin.py:115} INFO - [2023-01-07 00:32:07,773] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:32:07,780] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:07,805] {logging_mixin.py:115} INFO - [2023-01-07 00:32:07,804] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:32:07,825] {logging_mixin.py:115} INFO - [2023-01-07 00:32:07,825] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:32:07,834] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:32:37,905] {processor.py:153} INFO - Started process (PID=8316) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:37,906] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:32:37,906] {logging_mixin.py:115} INFO - [2023-01-07 00:32:37,906] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:38,765] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:32:38,767] {logging_mixin.py:115} INFO - [2023-01-07 00:32:38,767] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:32:38,767] {logging_mixin.py:115} INFO - [2023-01-07 00:32:38,767] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:32:38,774] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:32:38,795] {logging_mixin.py:115} INFO - [2023-01-07 00:32:38,795] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:32:38,816] {logging_mixin.py:115} INFO - [2023-01-07 00:32:38,816] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:32:38,830] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 00:33:08,924] {processor.py:153} INFO - Started process (PID=8334) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:08,924] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:33:08,925] {logging_mixin.py:115} INFO - [2023-01-07 00:33:08,925] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:09,818] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:33:09,819] {logging_mixin.py:115} INFO - [2023-01-07 00:33:09,819] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:33:09,820] {logging_mixin.py:115} INFO - [2023-01-07 00:33:09,819] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:33:09,826] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:09,851] {logging_mixin.py:115} INFO - [2023-01-07 00:33:09,851] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:33:09,872] {logging_mixin.py:115} INFO - [2023-01-07 00:33:09,872] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:33:09,881] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 00:33:39,962] {processor.py:153} INFO - Started process (PID=8360) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:39,963] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:33:39,963] {logging_mixin.py:115} INFO - [2023-01-07 00:33:39,963] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:40,823] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:33:40,824] {logging_mixin.py:115} INFO - [2023-01-07 00:33:40,824] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:33:40,824] {logging_mixin.py:115} INFO - [2023-01-07 00:33:40,824] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:33:40,831] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:33:40,853] {logging_mixin.py:115} INFO - [2023-01-07 00:33:40,852] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:33:40,872] {logging_mixin.py:115} INFO - [2023-01-07 00:33:40,872] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:33:40,881] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.924 seconds
[2023-01-07 00:34:10,975] {processor.py:153} INFO - Started process (PID=8385) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:10,976] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:34:10,976] {logging_mixin.py:115} INFO - [2023-01-07 00:34:10,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:11,877] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:34:11,879] {logging_mixin.py:115} INFO - [2023-01-07 00:34:11,879] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:34:11,879] {logging_mixin.py:115} INFO - [2023-01-07 00:34:11,879] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:34:11,886] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:11,908] {logging_mixin.py:115} INFO - [2023-01-07 00:34:11,908] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:34:11,928] {logging_mixin.py:115} INFO - [2023-01-07 00:34:11,928] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:34:11,938] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 00:34:42,031] {processor.py:153} INFO - Started process (PID=8412) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:42,032] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:34:42,033] {logging_mixin.py:115} INFO - [2023-01-07 00:34:42,033] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:42,886] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:34:42,887] {logging_mixin.py:115} INFO - [2023-01-07 00:34:42,887] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:34:42,888] {logging_mixin.py:115} INFO - [2023-01-07 00:34:42,887] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:34:42,894] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:34:42,916] {logging_mixin.py:115} INFO - [2023-01-07 00:34:42,916] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:34:42,936] {logging_mixin.py:115} INFO - [2023-01-07 00:34:42,936] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:34:42,946] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 00:35:13,009] {processor.py:153} INFO - Started process (PID=8430) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:13,011] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:35:13,011] {logging_mixin.py:115} INFO - [2023-01-07 00:35:13,011] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:13,923] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:35:13,924] {logging_mixin.py:115} INFO - [2023-01-07 00:35:13,924] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:35:13,925] {logging_mixin.py:115} INFO - [2023-01-07 00:35:13,924] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:35:13,931] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:13,954] {logging_mixin.py:115} INFO - [2023-01-07 00:35:13,953] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:35:13,974] {logging_mixin.py:115} INFO - [2023-01-07 00:35:13,974] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:35:13,983] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.979 seconds
[2023-01-07 00:35:44,037] {processor.py:153} INFO - Started process (PID=8455) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:44,038] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:35:44,039] {logging_mixin.py:115} INFO - [2023-01-07 00:35:44,038] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:44,902] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:35:44,904] {logging_mixin.py:115} INFO - [2023-01-07 00:35:44,903] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:35:44,904] {logging_mixin.py:115} INFO - [2023-01-07 00:35:44,904] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:35:44,911] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:35:44,932] {logging_mixin.py:115} INFO - [2023-01-07 00:35:44,932] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:35:44,952] {logging_mixin.py:115} INFO - [2023-01-07 00:35:44,952] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:35:44,962] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.929 seconds
[2023-01-07 00:36:15,237] {processor.py:153} INFO - Started process (PID=8481) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:15,238] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:36:15,239] {logging_mixin.py:115} INFO - [2023-01-07 00:36:15,239] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:16,088] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:36:16,089] {logging_mixin.py:115} INFO - [2023-01-07 00:36:16,089] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:36:16,090] {logging_mixin.py:115} INFO - [2023-01-07 00:36:16,089] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:36:16,096] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:16,118] {logging_mixin.py:115} INFO - [2023-01-07 00:36:16,117] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:36:16,138] {logging_mixin.py:115} INFO - [2023-01-07 00:36:16,138] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:36:16,147] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.914 seconds
[2023-01-07 00:36:46,217] {processor.py:153} INFO - Started process (PID=8506) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:46,218] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:36:46,220] {logging_mixin.py:115} INFO - [2023-01-07 00:36:46,220] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:47,076] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:36:47,077] {logging_mixin.py:115} INFO - [2023-01-07 00:36:47,077] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:36:47,077] {logging_mixin.py:115} INFO - [2023-01-07 00:36:47,077] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:36:47,084] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:36:47,108] {logging_mixin.py:115} INFO - [2023-01-07 00:36:47,107] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:36:47,131] {logging_mixin.py:115} INFO - [2023-01-07 00:36:47,131] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:36:47,140] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-07 00:37:17,208] {processor.py:153} INFO - Started process (PID=8532) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:17,209] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:37:17,211] {logging_mixin.py:115} INFO - [2023-01-07 00:37:17,211] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:18,341] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:37:18,343] {logging_mixin.py:115} INFO - [2023-01-07 00:37:18,343] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:37:18,343] {logging_mixin.py:115} INFO - [2023-01-07 00:37:18,343] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:37:18,350] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:18,372] {logging_mixin.py:115} INFO - [2023-01-07 00:37:18,371] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:37:18,392] {logging_mixin.py:115} INFO - [2023-01-07 00:37:18,392] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:37:18,401] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.198 seconds
[2023-01-07 00:37:48,477] {processor.py:153} INFO - Started process (PID=8551) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:48,478] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:37:48,479] {logging_mixin.py:115} INFO - [2023-01-07 00:37:48,479] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:49,386] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:37:49,387] {logging_mixin.py:115} INFO - [2023-01-07 00:37:49,387] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:37:49,387] {logging_mixin.py:115} INFO - [2023-01-07 00:37:49,387] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:37:49,394] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:37:49,416] {logging_mixin.py:115} INFO - [2023-01-07 00:37:49,416] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:37:49,436] {logging_mixin.py:115} INFO - [2023-01-07 00:37:49,436] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:37:49,446] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.973 seconds
[2023-01-07 00:38:19,520] {processor.py:153} INFO - Started process (PID=8575) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:19,521] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:38:19,523] {logging_mixin.py:115} INFO - [2023-01-07 00:38:19,523] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:20,375] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:38:20,376] {logging_mixin.py:115} INFO - [2023-01-07 00:38:20,376] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:38:20,376] {logging_mixin.py:115} INFO - [2023-01-07 00:38:20,376] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:38:20,383] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:20,405] {logging_mixin.py:115} INFO - [2023-01-07 00:38:20,405] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:38:20,425] {logging_mixin.py:115} INFO - [2023-01-07 00:38:20,425] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:38:20,434] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 00:38:50,503] {processor.py:153} INFO - Started process (PID=8599) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:50,504] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:38:50,505] {logging_mixin.py:115} INFO - [2023-01-07 00:38:50,505] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:51,360] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:38:51,362] {logging_mixin.py:115} INFO - [2023-01-07 00:38:51,361] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:38:51,362] {logging_mixin.py:115} INFO - [2023-01-07 00:38:51,362] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:38:51,369] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:38:51,391] {logging_mixin.py:115} INFO - [2023-01-07 00:38:51,391] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:38:51,412] {logging_mixin.py:115} INFO - [2023-01-07 00:38:51,412] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:38:51,421] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 00:39:21,494] {processor.py:153} INFO - Started process (PID=8623) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:21,495] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:39:21,496] {logging_mixin.py:115} INFO - [2023-01-07 00:39:21,496] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:22,343] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:39:22,345] {logging_mixin.py:115} INFO - [2023-01-07 00:39:22,345] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:39:22,345] {logging_mixin.py:115} INFO - [2023-01-07 00:39:22,345] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:39:22,353] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:22,379] {logging_mixin.py:115} INFO - [2023-01-07 00:39:22,378] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:39:22,399] {logging_mixin.py:115} INFO - [2023-01-07 00:39:22,399] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:39:22,408] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 00:39:52,480] {processor.py:153} INFO - Started process (PID=8641) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:52,481] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:39:52,482] {logging_mixin.py:115} INFO - [2023-01-07 00:39:52,482] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:53,342] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:39:53,343] {logging_mixin.py:115} INFO - [2023-01-07 00:39:53,343] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:39:53,344] {logging_mixin.py:115} INFO - [2023-01-07 00:39:53,344] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:39:53,351] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:39:53,380] {logging_mixin.py:115} INFO - [2023-01-07 00:39:53,379] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:39:53,400] {logging_mixin.py:115} INFO - [2023-01-07 00:39:53,400] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:39:53,409] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:40:23,498] {processor.py:153} INFO - Started process (PID=8667) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:23,499] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:40:23,500] {logging_mixin.py:115} INFO - [2023-01-07 00:40:23,500] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:24,371] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:40:24,372] {logging_mixin.py:115} INFO - [2023-01-07 00:40:24,372] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:40:24,373] {logging_mixin.py:115} INFO - [2023-01-07 00:40:24,373] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:40:24,380] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:24,402] {logging_mixin.py:115} INFO - [2023-01-07 00:40:24,401] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:40:24,422] {logging_mixin.py:115} INFO - [2023-01-07 00:40:24,422] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:40:24,431] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 00:40:54,537] {processor.py:153} INFO - Started process (PID=8690) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:54,538] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:40:54,538] {logging_mixin.py:115} INFO - [2023-01-07 00:40:54,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:55,387] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:40:55,388] {logging_mixin.py:115} INFO - [2023-01-07 00:40:55,388] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:40:55,388] {logging_mixin.py:115} INFO - [2023-01-07 00:40:55,388] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:40:55,395] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:40:55,417] {logging_mixin.py:115} INFO - [2023-01-07 00:40:55,416] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:40:55,437] {logging_mixin.py:115} INFO - [2023-01-07 00:40:55,437] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:40:55,446] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.914 seconds
[2023-01-07 00:41:25,543] {processor.py:153} INFO - Started process (PID=8714) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:25,544] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:41:25,545] {logging_mixin.py:115} INFO - [2023-01-07 00:41:25,545] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:26,396] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:41:26,397] {logging_mixin.py:115} INFO - [2023-01-07 00:41:26,397] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:41:26,398] {logging_mixin.py:115} INFO - [2023-01-07 00:41:26,398] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:41:26,406] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:26,433] {logging_mixin.py:115} INFO - [2023-01-07 00:41:26,433] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:41:26,456] {logging_mixin.py:115} INFO - [2023-01-07 00:41:26,456] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:41:26,466] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 00:41:56,501] {processor.py:153} INFO - Started process (PID=8732) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:56,502] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:41:56,503] {logging_mixin.py:115} INFO - [2023-01-07 00:41:56,503] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:57,582] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:41:57,583] {logging_mixin.py:115} INFO - [2023-01-07 00:41:57,583] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:41:57,584] {logging_mixin.py:115} INFO - [2023-01-07 00:41:57,584] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:41:57,595] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:41:57,626] {logging_mixin.py:115} INFO - [2023-01-07 00:41:57,626] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:41:57,656] {logging_mixin.py:115} INFO - [2023-01-07 00:41:57,656] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:41:57,669] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.174 seconds
[2023-01-07 00:42:27,756] {processor.py:153} INFO - Started process (PID=8755) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:27,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:42:27,757] {logging_mixin.py:115} INFO - [2023-01-07 00:42:27,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:28,624] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:42:28,625] {logging_mixin.py:115} INFO - [2023-01-07 00:42:28,625] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:42:28,626] {logging_mixin.py:115} INFO - [2023-01-07 00:42:28,625] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:42:28,634] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:28,657] {logging_mixin.py:115} INFO - [2023-01-07 00:42:28,657] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:42:28,677] {logging_mixin.py:115} INFO - [2023-01-07 00:42:28,677] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:42:28,686] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.935 seconds
[2023-01-07 00:42:58,756] {processor.py:153} INFO - Started process (PID=8780) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:58,758] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:42:58,759] {logging_mixin.py:115} INFO - [2023-01-07 00:42:58,759] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:59,608] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:42:59,609] {logging_mixin.py:115} INFO - [2023-01-07 00:42:59,609] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:42:59,609] {logging_mixin.py:115} INFO - [2023-01-07 00:42:59,609] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:42:59,616] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:42:59,638] {logging_mixin.py:115} INFO - [2023-01-07 00:42:59,638] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:42:59,658] {logging_mixin.py:115} INFO - [2023-01-07 00:42:59,658] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:42:59,667] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-07 00:43:29,733] {processor.py:153} INFO - Started process (PID=8806) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:43:29,734] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:43:29,734] {logging_mixin.py:115} INFO - [2023-01-07 00:43:29,734] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:43:30,588] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:43:30,590] {logging_mixin.py:115} INFO - [2023-01-07 00:43:30,590] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:43:30,590] {logging_mixin.py:115} INFO - [2023-01-07 00:43:30,590] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:43:30,597] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:43:30,619] {logging_mixin.py:115} INFO - [2023-01-07 00:43:30,619] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:43:30,640] {logging_mixin.py:115} INFO - [2023-01-07 00:43:30,640] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:43:30,650] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 00:44:00,722] {processor.py:153} INFO - Started process (PID=8824) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:00,724] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:44:00,724] {logging_mixin.py:115} INFO - [2023-01-07 00:44:00,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:01,626] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:44:01,627] {logging_mixin.py:115} INFO - [2023-01-07 00:44:01,627] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:44:01,628] {logging_mixin.py:115} INFO - [2023-01-07 00:44:01,627] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:44:01,635] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:01,664] {logging_mixin.py:115} INFO - [2023-01-07 00:44:01,664] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:44:01,693] {logging_mixin.py:115} INFO - [2023-01-07 00:44:01,693] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:44:01,704] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.986 seconds
[2023-01-07 00:44:31,783] {processor.py:153} INFO - Started process (PID=8849) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:31,784] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:44:31,784] {logging_mixin.py:115} INFO - [2023-01-07 00:44:31,784] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:32,650] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:44:32,651] {logging_mixin.py:115} INFO - [2023-01-07 00:44:32,651] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:44:32,651] {logging_mixin.py:115} INFO - [2023-01-07 00:44:32,651] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:44:32,658] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:44:32,680] {logging_mixin.py:115} INFO - [2023-01-07 00:44:32,679] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:44:32,700] {logging_mixin.py:115} INFO - [2023-01-07 00:44:32,699] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:44:32,709] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 00:45:02,775] {processor.py:153} INFO - Started process (PID=8874) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:02,776] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:45:02,777] {logging_mixin.py:115} INFO - [2023-01-07 00:45:02,777] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:03,689] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:45:03,690] {logging_mixin.py:115} INFO - [2023-01-07 00:45:03,690] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:45:03,690] {logging_mixin.py:115} INFO - [2023-01-07 00:45:03,690] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:45:03,697] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:03,719] {logging_mixin.py:115} INFO - [2023-01-07 00:45:03,718] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:45:03,739] {logging_mixin.py:115} INFO - [2023-01-07 00:45:03,739] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:45:03,748] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 00:45:33,822] {processor.py:153} INFO - Started process (PID=8900) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:33,823] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:45:33,824] {logging_mixin.py:115} INFO - [2023-01-07 00:45:33,824] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:34,691] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:45:34,692] {logging_mixin.py:115} INFO - [2023-01-07 00:45:34,692] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:45:34,693] {logging_mixin.py:115} INFO - [2023-01-07 00:45:34,692] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:45:34,699] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:45:34,721] {logging_mixin.py:115} INFO - [2023-01-07 00:45:34,721] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:45:34,741] {logging_mixin.py:115} INFO - [2023-01-07 00:45:34,741] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:45:34,751] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 00:46:05,510] {processor.py:153} INFO - Started process (PID=8924) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:05,511] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:46:05,511] {logging_mixin.py:115} INFO - [2023-01-07 00:46:05,511] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:06,412] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:46:06,414] {logging_mixin.py:115} INFO - [2023-01-07 00:46:06,414] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:46:06,414] {logging_mixin.py:115} INFO - [2023-01-07 00:46:06,414] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:46:06,421] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:06,444] {logging_mixin.py:115} INFO - [2023-01-07 00:46:06,443] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:46:06,464] {logging_mixin.py:115} INFO - [2023-01-07 00:46:06,464] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:46:06,475] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-07 00:46:36,619] {processor.py:153} INFO - Started process (PID=8942) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:36,619] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:46:36,620] {logging_mixin.py:115} INFO - [2023-01-07 00:46:36,620] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:37,496] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:46:37,497] {logging_mixin.py:115} INFO - [2023-01-07 00:46:37,497] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:46:37,497] {logging_mixin.py:115} INFO - [2023-01-07 00:46:37,497] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:46:37,504] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:46:37,525] {logging_mixin.py:115} INFO - [2023-01-07 00:46:37,525] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:46:37,552] {logging_mixin.py:115} INFO - [2023-01-07 00:46:37,552] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:46:37,562] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.948 seconds
[2023-01-07 00:47:07,632] {processor.py:153} INFO - Started process (PID=8968) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:07,633] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:47:07,634] {logging_mixin.py:115} INFO - [2023-01-07 00:47:07,634] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:08,536] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:47:08,538] {logging_mixin.py:115} INFO - [2023-01-07 00:47:08,538] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:47:08,538] {logging_mixin.py:115} INFO - [2023-01-07 00:47:08,538] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:47:08,545] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:08,567] {logging_mixin.py:115} INFO - [2023-01-07 00:47:08,566] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:47:08,586] {logging_mixin.py:115} INFO - [2023-01-07 00:47:08,586] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:47:08,596] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.969 seconds
[2023-01-07 00:47:38,666] {processor.py:153} INFO - Started process (PID=8994) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:38,666] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:47:38,667] {logging_mixin.py:115} INFO - [2023-01-07 00:47:38,667] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:39,518] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:47:39,519] {logging_mixin.py:115} INFO - [2023-01-07 00:47:39,519] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:47:39,520] {logging_mixin.py:115} INFO - [2023-01-07 00:47:39,519] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:47:39,526] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:47:39,548] {logging_mixin.py:115} INFO - [2023-01-07 00:47:39,548] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:47:39,570] {logging_mixin.py:115} INFO - [2023-01-07 00:47:39,570] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:47:39,584] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 00:48:10,390] {processor.py:153} INFO - Started process (PID=9020) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:10,392] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:48:10,393] {logging_mixin.py:115} INFO - [2023-01-07 00:48:10,392] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:11,259] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:48:11,260] {logging_mixin.py:115} INFO - [2023-01-07 00:48:11,260] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:48:11,261] {logging_mixin.py:115} INFO - [2023-01-07 00:48:11,260] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:48:11,268] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:11,289] {logging_mixin.py:115} INFO - [2023-01-07 00:48:11,289] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:48:11,310] {logging_mixin.py:115} INFO - [2023-01-07 00:48:11,309] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:48:11,319] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:48:41,377] {processor.py:153} INFO - Started process (PID=9039) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:41,378] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:48:41,379] {logging_mixin.py:115} INFO - [2023-01-07 00:48:41,379] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:42,240] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:48:42,241] {logging_mixin.py:115} INFO - [2023-01-07 00:48:42,241] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:48:42,242] {logging_mixin.py:115} INFO - [2023-01-07 00:48:42,241] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:48:42,248] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:48:42,270] {logging_mixin.py:115} INFO - [2023-01-07 00:48:42,270] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:48:42,290] {logging_mixin.py:115} INFO - [2023-01-07 00:48:42,290] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:48:42,300] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 00:49:12,541] {processor.py:153} INFO - Started process (PID=9066) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:12,542] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:49:12,543] {logging_mixin.py:115} INFO - [2023-01-07 00:49:12,543] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:13,387] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:49:13,388] {logging_mixin.py:115} INFO - [2023-01-07 00:49:13,388] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:49:13,388] {logging_mixin.py:115} INFO - [2023-01-07 00:49:13,388] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:49:13,395] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:13,416] {logging_mixin.py:115} INFO - [2023-01-07 00:49:13,416] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:49:13,436] {logging_mixin.py:115} INFO - [2023-01-07 00:49:13,436] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:49:13,446] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.909 seconds
[2023-01-07 00:49:44,325] {processor.py:153} INFO - Started process (PID=9090) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:44,326] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:49:44,327] {logging_mixin.py:115} INFO - [2023-01-07 00:49:44,327] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:45,208] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:49:45,209] {logging_mixin.py:115} INFO - [2023-01-07 00:49:45,209] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:49:45,210] {logging_mixin.py:115} INFO - [2023-01-07 00:49:45,210] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:49:45,216] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:49:45,238] {logging_mixin.py:115} INFO - [2023-01-07 00:49:45,237] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:49:45,257] {logging_mixin.py:115} INFO - [2023-01-07 00:49:45,257] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:49:45,266] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.945 seconds
[2023-01-07 00:50:15,358] {processor.py:153} INFO - Started process (PID=9115) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:15,359] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:50:15,360] {logging_mixin.py:115} INFO - [2023-01-07 00:50:15,360] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:16,215] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:50:16,216] {logging_mixin.py:115} INFO - [2023-01-07 00:50:16,216] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:50:16,217] {logging_mixin.py:115} INFO - [2023-01-07 00:50:16,216] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:50:16,223] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:16,245] {logging_mixin.py:115} INFO - [2023-01-07 00:50:16,245] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:50:16,265] {logging_mixin.py:115} INFO - [2023-01-07 00:50:16,265] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:50:16,274] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 00:50:46,303] {processor.py:153} INFO - Started process (PID=9133) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:46,304] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:50:46,305] {logging_mixin.py:115} INFO - [2023-01-07 00:50:46,304] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:47,194] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:50:47,196] {logging_mixin.py:115} INFO - [2023-01-07 00:50:47,196] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:50:47,196] {logging_mixin.py:115} INFO - [2023-01-07 00:50:47,196] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:50:47,203] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:50:47,230] {logging_mixin.py:115} INFO - [2023-01-07 00:50:47,229] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:50:47,251] {logging_mixin.py:115} INFO - [2023-01-07 00:50:47,250] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:50:47,260] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 00:51:17,457] {processor.py:153} INFO - Started process (PID=9157) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:17,459] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:51:17,459] {logging_mixin.py:115} INFO - [2023-01-07 00:51:17,459] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:18,317] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:51:18,319] {logging_mixin.py:115} INFO - [2023-01-07 00:51:18,318] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:51:18,319] {logging_mixin.py:115} INFO - [2023-01-07 00:51:18,319] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:51:18,326] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:18,348] {logging_mixin.py:115} INFO - [2023-01-07 00:51:18,348] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:51:18,368] {logging_mixin.py:115} INFO - [2023-01-07 00:51:18,368] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:51:18,378] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-07 00:51:36,180] {processor.py:153} INFO - Started process (PID=9176) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:36,181] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:51:36,181] {logging_mixin.py:115} INFO - [2023-01-07 00:51:36,181] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:37,062] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:51:37,063] {logging_mixin.py:115} INFO - [2023-01-07 00:51:37,063] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:51:37,063] {logging_mixin.py:115} INFO - [2023-01-07 00:51:37,063] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:51:37,070] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:51:37,091] {logging_mixin.py:115} INFO - [2023-01-07 00:51:37,091] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:51:37,111] {logging_mixin.py:115} INFO - [2023-01-07 00:51:37,111] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:51:37,121] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.946 seconds
[2023-01-07 00:52:07,471] {processor.py:153} INFO - Started process (PID=9202) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:07,472] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:52:07,473] {logging_mixin.py:115} INFO - [2023-01-07 00:52:07,472] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:08,341] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:52:08,342] {logging_mixin.py:115} INFO - [2023-01-07 00:52:08,342] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:52:08,343] {logging_mixin.py:115} INFO - [2023-01-07 00:52:08,342] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:52:08,349] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:08,371] {logging_mixin.py:115} INFO - [2023-01-07 00:52:08,370] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:52:08,391] {logging_mixin.py:115} INFO - [2023-01-07 00:52:08,391] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:52:08,400] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:52:38,469] {processor.py:153} INFO - Started process (PID=9221) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:38,470] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:52:38,470] {logging_mixin.py:115} INFO - [2023-01-07 00:52:38,470] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:39,469] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:52:39,471] {logging_mixin.py:115} INFO - [2023-01-07 00:52:39,471] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:52:39,471] {logging_mixin.py:115} INFO - [2023-01-07 00:52:39,471] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:52:39,482] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:52:39,511] {logging_mixin.py:115} INFO - [2023-01-07 00:52:39,511] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:52:39,532] {logging_mixin.py:115} INFO - [2023-01-07 00:52:39,532] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:52:39,542] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.078 seconds
[2023-01-07 00:53:09,612] {processor.py:153} INFO - Started process (PID=9246) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:09,613] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:53:09,614] {logging_mixin.py:115} INFO - [2023-01-07 00:53:09,614] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:10,469] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:53:10,470] {logging_mixin.py:115} INFO - [2023-01-07 00:53:10,470] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:53:10,471] {logging_mixin.py:115} INFO - [2023-01-07 00:53:10,470] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:53:10,477] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:10,499] {logging_mixin.py:115} INFO - [2023-01-07 00:53:10,498] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:53:10,519] {logging_mixin.py:115} INFO - [2023-01-07 00:53:10,519] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:53:10,528] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 00:53:40,595] {processor.py:153} INFO - Started process (PID=9271) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:40,597] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:53:40,597] {logging_mixin.py:115} INFO - [2023-01-07 00:53:40,597] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:41,435] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:53:41,437] {logging_mixin.py:115} INFO - [2023-01-07 00:53:41,436] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:53:41,437] {logging_mixin.py:115} INFO - [2023-01-07 00:53:41,437] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:53:41,444] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:53:41,466] {logging_mixin.py:115} INFO - [2023-01-07 00:53:41,466] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:53:41,486] {logging_mixin.py:115} INFO - [2023-01-07 00:53:41,486] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:53:41,497] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.906 seconds
[2023-01-07 00:54:11,578] {processor.py:153} INFO - Started process (PID=9297) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:11,579] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:54:11,579] {logging_mixin.py:115} INFO - [2023-01-07 00:54:11,579] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:12,464] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:54:12,465] {logging_mixin.py:115} INFO - [2023-01-07 00:54:12,465] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:54:12,466] {logging_mixin.py:115} INFO - [2023-01-07 00:54:12,465] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:54:12,472] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:12,494] {logging_mixin.py:115} INFO - [2023-01-07 00:54:12,493] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:54:12,515] {logging_mixin.py:115} INFO - [2023-01-07 00:54:12,515] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:54:12,525] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.951 seconds
[2023-01-07 00:54:42,626] {processor.py:153} INFO - Started process (PID=9315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:42,628] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:54:42,628] {logging_mixin.py:115} INFO - [2023-01-07 00:54:42,628] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:43,551] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:54:43,552] {logging_mixin.py:115} INFO - [2023-01-07 00:54:43,552] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:54:43,553] {logging_mixin.py:115} INFO - [2023-01-07 00:54:43,553] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:54:43,560] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:54:43,582] {logging_mixin.py:115} INFO - [2023-01-07 00:54:43,582] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:54:43,602] {logging_mixin.py:115} INFO - [2023-01-07 00:54:43,602] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:54:43,611] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.990 seconds
[2023-01-07 00:55:13,694] {processor.py:153} INFO - Started process (PID=9339) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:13,695] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:55:13,695] {logging_mixin.py:115} INFO - [2023-01-07 00:55:13,695] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:14,582] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:55:14,584] {logging_mixin.py:115} INFO - [2023-01-07 00:55:14,583] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:55:14,584] {logging_mixin.py:115} INFO - [2023-01-07 00:55:14,584] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:55:14,591] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:14,613] {logging_mixin.py:115} INFO - [2023-01-07 00:55:14,613] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:55:14,633] {logging_mixin.py:115} INFO - [2023-01-07 00:55:14,633] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:55:14,643] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.955 seconds
[2023-01-07 00:55:44,709] {processor.py:153} INFO - Started process (PID=9363) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:44,711] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:55:44,712] {logging_mixin.py:115} INFO - [2023-01-07 00:55:44,712] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:45,682] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:55:45,683] {logging_mixin.py:115} INFO - [2023-01-07 00:55:45,683] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:55:45,683] {logging_mixin.py:115} INFO - [2023-01-07 00:55:45,683] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:55:45,690] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:55:45,712] {logging_mixin.py:115} INFO - [2023-01-07 00:55:45,711] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:55:45,732] {logging_mixin.py:115} INFO - [2023-01-07 00:55:45,732] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:55:45,742] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.038 seconds
[2023-01-07 00:56:15,821] {processor.py:153} INFO - Started process (PID=9387) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:15,822] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:56:15,822] {logging_mixin.py:115} INFO - [2023-01-07 00:56:15,822] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:16,673] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:56:16,674] {logging_mixin.py:115} INFO - [2023-01-07 00:56:16,674] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:56:16,675] {logging_mixin.py:115} INFO - [2023-01-07 00:56:16,674] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:56:16,681] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:16,703] {logging_mixin.py:115} INFO - [2023-01-07 00:56:16,703] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:56:16,731] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-07 00:56:46,873] {processor.py:153} INFO - Started process (PID=9405) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:46,873] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:56:46,874] {logging_mixin.py:115} INFO - [2023-01-07 00:56:46,874] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:47,768] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:56:47,769] {logging_mixin.py:115} INFO - [2023-01-07 00:56:47,769] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:56:47,769] {logging_mixin.py:115} INFO - [2023-01-07 00:56:47,769] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:56:47,776] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:56:47,798] {logging_mixin.py:115} INFO - [2023-01-07 00:56:47,798] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:56:47,818] {logging_mixin.py:115} INFO - [2023-01-07 00:56:47,818] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:56:47,828] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 00:57:17,900] {processor.py:153} INFO - Started process (PID=9430) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:17,901] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:57:17,902] {logging_mixin.py:115} INFO - [2023-01-07 00:57:17,901] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:18,742] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:57:18,743] {logging_mixin.py:115} INFO - [2023-01-07 00:57:18,743] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:57:18,744] {logging_mixin.py:115} INFO - [2023-01-07 00:57:18,743] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:57:18,750] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:18,772] {logging_mixin.py:115} INFO - [2023-01-07 00:57:18,771] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:57:18,792] {logging_mixin.py:115} INFO - [2023-01-07 00:57:18,792] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:57:18,802] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.907 seconds
[2023-01-07 00:57:48,866] {processor.py:153} INFO - Started process (PID=9456) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:48,867] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:57:48,867] {logging_mixin.py:115} INFO - [2023-01-07 00:57:48,867] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:49,721] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:57:49,723] {logging_mixin.py:115} INFO - [2023-01-07 00:57:49,722] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:57:49,723] {logging_mixin.py:115} INFO - [2023-01-07 00:57:49,723] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:57:49,730] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:57:49,751] {logging_mixin.py:115} INFO - [2023-01-07 00:57:49,751] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:57:49,771] {logging_mixin.py:115} INFO - [2023-01-07 00:57:49,771] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:57:49,781] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 00:58:19,850] {processor.py:153} INFO - Started process (PID=9481) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:19,852] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:58:19,852] {logging_mixin.py:115} INFO - [2023-01-07 00:58:19,852] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:20,717] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:58:20,719] {logging_mixin.py:115} INFO - [2023-01-07 00:58:20,719] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:58:20,719] {logging_mixin.py:115} INFO - [2023-01-07 00:58:20,719] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:58:20,726] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:20,747] {logging_mixin.py:115} INFO - [2023-01-07 00:58:20,747] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:58:20,767] {logging_mixin.py:115} INFO - [2023-01-07 00:58:20,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:58:20,778] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-07 00:58:50,843] {processor.py:153} INFO - Started process (PID=9501) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:50,846] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:58:50,846] {logging_mixin.py:115} INFO - [2023-01-07 00:58:50,846] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:51,729] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:58:51,731] {logging_mixin.py:115} INFO - [2023-01-07 00:58:51,731] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:58:51,731] {logging_mixin.py:115} INFO - [2023-01-07 00:58:51,731] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:58:51,741] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:58:51,766] {logging_mixin.py:115} INFO - [2023-01-07 00:58:51,766] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:58:51,788] {logging_mixin.py:115} INFO - [2023-01-07 00:58:51,788] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:58:51,800] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 00:59:21,872] {processor.py:153} INFO - Started process (PID=9526) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:21,873] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:59:21,873] {logging_mixin.py:115} INFO - [2023-01-07 00:59:21,873] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:22,740] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:59:22,742] {logging_mixin.py:115} INFO - [2023-01-07 00:59:22,742] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:59:22,742] {logging_mixin.py:115} INFO - [2023-01-07 00:59:22,742] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:59:22,749] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:22,771] {logging_mixin.py:115} INFO - [2023-01-07 00:59:22,770] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:59:22,791] {logging_mixin.py:115} INFO - [2023-01-07 00:59:22,791] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:59:22,801] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 00:59:52,881] {processor.py:153} INFO - Started process (PID=9552) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:52,882] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 00:59:52,882] {logging_mixin.py:115} INFO - [2023-01-07 00:59:52,882] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:53,764] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 00:59:53,766] {logging_mixin.py:115} INFO - [2023-01-07 00:59:53,766] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 00:59:53,766] {logging_mixin.py:115} INFO - [2023-01-07 00:59:53,766] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 00:59:53,773] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 00:59:53,794] {logging_mixin.py:115} INFO - [2023-01-07 00:59:53,794] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 00:59:53,814] {logging_mixin.py:115} INFO - [2023-01-07 00:59:53,814] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 00:59:53,824] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.948 seconds
[2023-01-07 01:00:23,910] {processor.py:153} INFO - Started process (PID=9578) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:23,911] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:00:23,911] {logging_mixin.py:115} INFO - [2023-01-07 01:00:23,911] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:24,783] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:00:24,784] {logging_mixin.py:115} INFO - [2023-01-07 01:00:24,784] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:00:24,785] {logging_mixin.py:115} INFO - [2023-01-07 01:00:24,785] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:00:24,791] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:24,813] {logging_mixin.py:115} INFO - [2023-01-07 01:00:24,812] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:00:24,833] {logging_mixin.py:115} INFO - [2023-01-07 01:00:24,832] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:00:24,843] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 01:00:54,955] {processor.py:153} INFO - Started process (PID=9597) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:54,957] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:00:54,957] {logging_mixin.py:115} INFO - [2023-01-07 01:00:54,957] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:55,824] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:00:55,825] {logging_mixin.py:115} INFO - [2023-01-07 01:00:55,825] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:00:55,826] {logging_mixin.py:115} INFO - [2023-01-07 01:00:55,825] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:00:55,832] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:00:55,854] {logging_mixin.py:115} INFO - [2023-01-07 01:00:55,854] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:00:55,874] {logging_mixin.py:115} INFO - [2023-01-07 01:00:55,874] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:00:55,884] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-07 01:01:25,984] {processor.py:153} INFO - Started process (PID=9623) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:25,986] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:01:25,986] {logging_mixin.py:115} INFO - [2023-01-07 01:01:25,986] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:26,847] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:01:26,848] {logging_mixin.py:115} INFO - [2023-01-07 01:01:26,848] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:01:26,848] {logging_mixin.py:115} INFO - [2023-01-07 01:01:26,848] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:01:26,855] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:26,879] {logging_mixin.py:115} INFO - [2023-01-07 01:01:26,879] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:01:26,899] {logging_mixin.py:115} INFO - [2023-01-07 01:01:26,899] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:01:26,910] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 01:01:57,062] {processor.py:153} INFO - Started process (PID=9649) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:57,063] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:01:57,064] {logging_mixin.py:115} INFO - [2023-01-07 01:01:57,063] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:57,945] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:01:57,946] {logging_mixin.py:115} INFO - [2023-01-07 01:01:57,946] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:01:57,947] {logging_mixin.py:115} INFO - [2023-01-07 01:01:57,946] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:01:57,953] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:01:57,975] {logging_mixin.py:115} INFO - [2023-01-07 01:01:57,975] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:01:57,995] {logging_mixin.py:115} INFO - [2023-01-07 01:01:57,995] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:01:58,005] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.949 seconds
[2023-01-07 01:02:28,072] {processor.py:153} INFO - Started process (PID=9673) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:28,073] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:02:28,074] {logging_mixin.py:115} INFO - [2023-01-07 01:02:28,074] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:28,933] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:02:28,934] {logging_mixin.py:115} INFO - [2023-01-07 01:02:28,934] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:02:28,934] {logging_mixin.py:115} INFO - [2023-01-07 01:02:28,934] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:02:28,941] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:28,963] {logging_mixin.py:115} INFO - [2023-01-07 01:02:28,962] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:02:28,983] {logging_mixin.py:115} INFO - [2023-01-07 01:02:28,983] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:02:28,993] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 01:02:59,060] {processor.py:153} INFO - Started process (PID=9691) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:59,062] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:02:59,062] {logging_mixin.py:115} INFO - [2023-01-07 01:02:59,062] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:59,936] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:02:59,937] {logging_mixin.py:115} INFO - [2023-01-07 01:02:59,937] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:02:59,938] {logging_mixin.py:115} INFO - [2023-01-07 01:02:59,938] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:02:59,945] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:02:59,967] {logging_mixin.py:115} INFO - [2023-01-07 01:02:59,967] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:02:59,988] {logging_mixin.py:115} INFO - [2023-01-07 01:02:59,987] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:02:59,999] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 01:03:30,070] {processor.py:153} INFO - Started process (PID=9717) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:03:30,071] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:03:30,071] {logging_mixin.py:115} INFO - [2023-01-07 01:03:30,071] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:03:30,937] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:03:30,939] {logging_mixin.py:115} INFO - [2023-01-07 01:03:30,939] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:03:30,939] {logging_mixin.py:115} INFO - [2023-01-07 01:03:30,939] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:03:30,950] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:03:30,976] {logging_mixin.py:115} INFO - [2023-01-07 01:03:30,976] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:03:30,998] {logging_mixin.py:115} INFO - [2023-01-07 01:03:30,998] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:03:31,010] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.944 seconds
[2023-01-07 01:04:01,077] {processor.py:153} INFO - Started process (PID=9742) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:01,079] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:04:01,079] {logging_mixin.py:115} INFO - [2023-01-07 01:04:01,079] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:01,937] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:04:01,938] {logging_mixin.py:115} INFO - [2023-01-07 01:04:01,938] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:04:01,939] {logging_mixin.py:115} INFO - [2023-01-07 01:04:01,938] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:04:01,945] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:01,967] {logging_mixin.py:115} INFO - [2023-01-07 01:04:01,967] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:04:01,987] {logging_mixin.py:115} INFO - [2023-01-07 01:04:01,987] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:04:01,997] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-07 01:04:32,067] {processor.py:153} INFO - Started process (PID=9768) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:32,067] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:04:32,068] {logging_mixin.py:115} INFO - [2023-01-07 01:04:32,068] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:32,943] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:04:32,945] {logging_mixin.py:115} INFO - [2023-01-07 01:04:32,944] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:04:32,945] {logging_mixin.py:115} INFO - [2023-01-07 01:04:32,945] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:04:32,952] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:04:32,973] {logging_mixin.py:115} INFO - [2023-01-07 01:04:32,973] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:04:32,993] {logging_mixin.py:115} INFO - [2023-01-07 01:04:32,993] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:04:33,003] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 01:05:03,073] {processor.py:153} INFO - Started process (PID=9793) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:03,074] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:05:03,075] {logging_mixin.py:115} INFO - [2023-01-07 01:05:03,075] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:04,042] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:05:04,043] {logging_mixin.py:115} INFO - [2023-01-07 01:05:04,043] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:05:04,044] {logging_mixin.py:115} INFO - [2023-01-07 01:05:04,043] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:05:04,050] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:04,072] {logging_mixin.py:115} INFO - [2023-01-07 01:05:04,072] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:05:04,092] {logging_mixin.py:115} INFO - [2023-01-07 01:05:04,092] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:05:04,102] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.035 seconds
[2023-01-07 01:05:34,168] {processor.py:153} INFO - Started process (PID=9811) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:34,170] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:05:34,171] {logging_mixin.py:115} INFO - [2023-01-07 01:05:34,171] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:35,035] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:05:35,036] {logging_mixin.py:115} INFO - [2023-01-07 01:05:35,036] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:05:35,036] {logging_mixin.py:115} INFO - [2023-01-07 01:05:35,036] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:05:35,043] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:05:35,065] {logging_mixin.py:115} INFO - [2023-01-07 01:05:35,065] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:05:35,085] {logging_mixin.py:115} INFO - [2023-01-07 01:05:35,085] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:05:35,096] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 01:06:05,162] {processor.py:153} INFO - Started process (PID=9836) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:05,163] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:06:05,164] {logging_mixin.py:115} INFO - [2023-01-07 01:06:05,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:06,015] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:06:06,017] {logging_mixin.py:115} INFO - [2023-01-07 01:06:06,017] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:06:06,017] {logging_mixin.py:115} INFO - [2023-01-07 01:06:06,017] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:06:06,024] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:06,046] {logging_mixin.py:115} INFO - [2023-01-07 01:06:06,045] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:06:06,066] {logging_mixin.py:115} INFO - [2023-01-07 01:06:06,066] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:06:06,076] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 01:06:36,145] {processor.py:153} INFO - Started process (PID=9861) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:36,146] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:06:36,148] {logging_mixin.py:115} INFO - [2023-01-07 01:06:36,148] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:37,017] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:06:37,018] {logging_mixin.py:115} INFO - [2023-01-07 01:06:37,018] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:06:37,018] {logging_mixin.py:115} INFO - [2023-01-07 01:06:37,018] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:06:37,025] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:06:37,049] {logging_mixin.py:115} INFO - [2023-01-07 01:06:37,048] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:06:37,070] {logging_mixin.py:115} INFO - [2023-01-07 01:06:37,069] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:06:37,080] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 01:07:07,147] {processor.py:153} INFO - Started process (PID=9886) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:07,147] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:07:07,149] {logging_mixin.py:115} INFO - [2023-01-07 01:07:07,149] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:08,056] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:07:08,057] {logging_mixin.py:115} INFO - [2023-01-07 01:07:08,057] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:07:08,058] {logging_mixin.py:115} INFO - [2023-01-07 01:07:08,057] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:07:08,064] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:08,086] {logging_mixin.py:115} INFO - [2023-01-07 01:07:08,085] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:07:08,106] {logging_mixin.py:115} INFO - [2023-01-07 01:07:08,105] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:07:08,115] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-07 01:07:38,191] {processor.py:153} INFO - Started process (PID=9904) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:38,192] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:07:38,194] {logging_mixin.py:115} INFO - [2023-01-07 01:07:38,193] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:39,261] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:07:39,263] {logging_mixin.py:115} INFO - [2023-01-07 01:07:39,263] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:07:39,264] {logging_mixin.py:115} INFO - [2023-01-07 01:07:39,263] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:07:39,275] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:07:39,299] {logging_mixin.py:115} INFO - [2023-01-07 01:07:39,298] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:07:39,319] {logging_mixin.py:115} INFO - [2023-01-07 01:07:39,319] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:07:39,330] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.145 seconds
[2023-01-07 01:08:09,397] {processor.py:153} INFO - Started process (PID=9930) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:09,398] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:08:09,400] {logging_mixin.py:115} INFO - [2023-01-07 01:08:09,400] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:10,275] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:08:10,276] {logging_mixin.py:115} INFO - [2023-01-07 01:08:10,276] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:08:10,277] {logging_mixin.py:115} INFO - [2023-01-07 01:08:10,277] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:08:10,283] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:10,305] {logging_mixin.py:115} INFO - [2023-01-07 01:08:10,305] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:08:10,325] {logging_mixin.py:115} INFO - [2023-01-07 01:08:10,325] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:08:10,335] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 01:08:40,404] {processor.py:153} INFO - Started process (PID=9956) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:40,405] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:08:40,407] {logging_mixin.py:115} INFO - [2023-01-07 01:08:40,407] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:41,269] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:08:41,271] {logging_mixin.py:115} INFO - [2023-01-07 01:08:41,271] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:08:41,271] {logging_mixin.py:115} INFO - [2023-01-07 01:08:41,271] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:08:41,278] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:08:41,300] {logging_mixin.py:115} INFO - [2023-01-07 01:08:41,300] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:08:41,321] {logging_mixin.py:115} INFO - [2023-01-07 01:08:41,321] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:08:41,331] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-07 01:09:11,400] {processor.py:153} INFO - Started process (PID=9982) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:11,401] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:09:11,403] {logging_mixin.py:115} INFO - [2023-01-07 01:09:11,403] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:12,279] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:09:12,281] {logging_mixin.py:115} INFO - [2023-01-07 01:09:12,280] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:09:12,281] {logging_mixin.py:115} INFO - [2023-01-07 01:09:12,281] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:09:12,288] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:12,309] {logging_mixin.py:115} INFO - [2023-01-07 01:09:12,309] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:09:12,329] {logging_mixin.py:115} INFO - [2023-01-07 01:09:12,329] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:09:12,339] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.944 seconds
[2023-01-07 01:09:42,409] {processor.py:153} INFO - Started process (PID=10000) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:42,409] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:09:42,411] {logging_mixin.py:115} INFO - [2023-01-07 01:09:42,411] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:43,281] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:09:43,283] {logging_mixin.py:115} INFO - [2023-01-07 01:09:43,283] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:09:43,283] {logging_mixin.py:115} INFO - [2023-01-07 01:09:43,283] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:09:43,290] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:09:43,312] {logging_mixin.py:115} INFO - [2023-01-07 01:09:43,311] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:09:43,331] {logging_mixin.py:115} INFO - [2023-01-07 01:09:43,331] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:09:43,342] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 01:10:13,411] {processor.py:153} INFO - Started process (PID=10024) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:13,411] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:10:13,413] {logging_mixin.py:115} INFO - [2023-01-07 01:10:13,413] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:14,263] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:10:14,264] {logging_mixin.py:115} INFO - [2023-01-07 01:10:14,264] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:10:14,265] {logging_mixin.py:115} INFO - [2023-01-07 01:10:14,265] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:10:14,272] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:14,294] {logging_mixin.py:115} INFO - [2023-01-07 01:10:14,294] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:10:14,314] {logging_mixin.py:115} INFO - [2023-01-07 01:10:14,314] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:10:14,324] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:10:44,419] {processor.py:153} INFO - Started process (PID=10048) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:44,419] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:10:44,420] {logging_mixin.py:115} INFO - [2023-01-07 01:10:44,420] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:45,274] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:10:45,275] {logging_mixin.py:115} INFO - [2023-01-07 01:10:45,275] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:10:45,276] {logging_mixin.py:115} INFO - [2023-01-07 01:10:45,276] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:10:45,283] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:10:45,304] {logging_mixin.py:115} INFO - [2023-01-07 01:10:45,304] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:10:45,325] {logging_mixin.py:115} INFO - [2023-01-07 01:10:45,325] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:10:45,336] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 01:11:15,830] {processor.py:153} INFO - Started process (PID=10074) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:15,831] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:11:15,831] {logging_mixin.py:115} INFO - [2023-01-07 01:11:15,831] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:16,769] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:11:16,770] {logging_mixin.py:115} INFO - [2023-01-07 01:11:16,770] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:11:16,771] {logging_mixin.py:115} INFO - [2023-01-07 01:11:16,771] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:11:16,778] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:16,799] {logging_mixin.py:115} INFO - [2023-01-07 01:11:16,799] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:11:16,819] {logging_mixin.py:115} INFO - [2023-01-07 01:11:16,819] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:11:16,829] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.004 seconds
[2023-01-07 01:11:46,900] {processor.py:153} INFO - Started process (PID=10092) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:46,901] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:11:46,902] {logging_mixin.py:115} INFO - [2023-01-07 01:11:46,902] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:47,760] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:11:47,761] {logging_mixin.py:115} INFO - [2023-01-07 01:11:47,761] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:11:47,761] {logging_mixin.py:115} INFO - [2023-01-07 01:11:47,761] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:11:47,770] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:11:47,800] {logging_mixin.py:115} INFO - [2023-01-07 01:11:47,799] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:11:47,827] {logging_mixin.py:115} INFO - [2023-01-07 01:11:47,827] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:11:47,840] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.944 seconds
[2023-01-07 01:12:17,904] {processor.py:153} INFO - Started process (PID=10117) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:17,905] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:12:17,906] {logging_mixin.py:115} INFO - [2023-01-07 01:12:17,906] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:18,765] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:12:18,766] {logging_mixin.py:115} INFO - [2023-01-07 01:12:18,766] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:12:18,766] {logging_mixin.py:115} INFO - [2023-01-07 01:12:18,766] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:12:18,773] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:18,795] {logging_mixin.py:115} INFO - [2023-01-07 01:12:18,794] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:12:18,815] {logging_mixin.py:115} INFO - [2023-01-07 01:12:18,814] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:12:18,825] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-07 01:12:48,891] {processor.py:153} INFO - Started process (PID=10141) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:48,892] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:12:48,892] {logging_mixin.py:115} INFO - [2023-01-07 01:12:48,892] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:49,766] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:12:49,767] {logging_mixin.py:115} INFO - [2023-01-07 01:12:49,767] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:12:49,768] {logging_mixin.py:115} INFO - [2023-01-07 01:12:49,768] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:12:49,775] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:12:49,798] {logging_mixin.py:115} INFO - [2023-01-07 01:12:49,797] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:12:49,819] {logging_mixin.py:115} INFO - [2023-01-07 01:12:49,819] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:12:49,830] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 01:13:19,902] {processor.py:153} INFO - Started process (PID=10166) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:19,903] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:13:19,904] {logging_mixin.py:115} INFO - [2023-01-07 01:13:19,904] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:20,787] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:13:20,788] {logging_mixin.py:115} INFO - [2023-01-07 01:13:20,788] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:13:20,789] {logging_mixin.py:115} INFO - [2023-01-07 01:13:20,789] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:13:20,795] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:20,817] {logging_mixin.py:115} INFO - [2023-01-07 01:13:20,816] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:13:20,837] {logging_mixin.py:115} INFO - [2023-01-07 01:13:20,837] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:13:20,847] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.950 seconds
[2023-01-07 01:13:50,918] {processor.py:153} INFO - Started process (PID=10184) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:50,919] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:13:50,920] {logging_mixin.py:115} INFO - [2023-01-07 01:13:50,920] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:51,800] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:13:51,801] {logging_mixin.py:115} INFO - [2023-01-07 01:13:51,801] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:13:51,802] {logging_mixin.py:115} INFO - [2023-01-07 01:13:51,802] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:13:51,809] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:13:51,831] {logging_mixin.py:115} INFO - [2023-01-07 01:13:51,830] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:13:51,851] {logging_mixin.py:115} INFO - [2023-01-07 01:13:51,851] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:13:51,861] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.948 seconds
[2023-01-07 01:14:21,944] {processor.py:153} INFO - Started process (PID=10209) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:21,944] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:14:21,945] {logging_mixin.py:115} INFO - [2023-01-07 01:14:21,945] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:22,849] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:14:22,851] {logging_mixin.py:115} INFO - [2023-01-07 01:14:22,850] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:14:22,851] {logging_mixin.py:115} INFO - [2023-01-07 01:14:22,851] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:14:22,858] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:22,884] {logging_mixin.py:115} INFO - [2023-01-07 01:14:22,884] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:14:22,905] {logging_mixin.py:115} INFO - [2023-01-07 01:14:22,905] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:14:22,918] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 01:14:52,997] {processor.py:153} INFO - Started process (PID=10236) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:53,000] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:14:53,001] {logging_mixin.py:115} INFO - [2023-01-07 01:14:53,001] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:53,910] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:14:53,911] {logging_mixin.py:115} INFO - [2023-01-07 01:14:53,911] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:14:53,912] {logging_mixin.py:115} INFO - [2023-01-07 01:14:53,912] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:14:53,923] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:14:53,952] {logging_mixin.py:115} INFO - [2023-01-07 01:14:53,951] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:14:53,973] {logging_mixin.py:115} INFO - [2023-01-07 01:14:53,973] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:14:53,984] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.992 seconds
[2023-01-07 01:15:24,072] {processor.py:153} INFO - Started process (PID=10260) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:24,073] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:15:24,074] {logging_mixin.py:115} INFO - [2023-01-07 01:15:24,074] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:24,947] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:15:24,948] {logging_mixin.py:115} INFO - [2023-01-07 01:15:24,948] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:15:24,948] {logging_mixin.py:115} INFO - [2023-01-07 01:15:24,948] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:15:24,955] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:24,976] {logging_mixin.py:115} INFO - [2023-01-07 01:15:24,976] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:15:24,996] {logging_mixin.py:115} INFO - [2023-01-07 01:15:24,996] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:15:25,006] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 01:15:55,080] {processor.py:153} INFO - Started process (PID=10278) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:55,081] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:15:55,081] {logging_mixin.py:115} INFO - [2023-01-07 01:15:55,081] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:55,983] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:15:55,984] {logging_mixin.py:115} INFO - [2023-01-07 01:15:55,984] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:15:55,985] {logging_mixin.py:115} INFO - [2023-01-07 01:15:55,985] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:15:55,991] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:15:56,014] {logging_mixin.py:115} INFO - [2023-01-07 01:15:56,014] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:15:56,035] {logging_mixin.py:115} INFO - [2023-01-07 01:15:56,035] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:15:56,046] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 01:16:26,124] {processor.py:153} INFO - Started process (PID=10303) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:26,126] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:16:26,126] {logging_mixin.py:115} INFO - [2023-01-07 01:16:26,126] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:26,998] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:16:26,999] {logging_mixin.py:115} INFO - [2023-01-07 01:16:26,999] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:16:26,999] {logging_mixin.py:115} INFO - [2023-01-07 01:16:26,999] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:16:27,006] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:27,028] {logging_mixin.py:115} INFO - [2023-01-07 01:16:27,028] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:16:27,048] {logging_mixin.py:115} INFO - [2023-01-07 01:16:27,048] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:16:27,059] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 01:16:57,125] {processor.py:153} INFO - Started process (PID=10328) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:57,125] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:16:57,126] {logging_mixin.py:115} INFO - [2023-01-07 01:16:57,126] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:57,981] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:16:57,982] {logging_mixin.py:115} INFO - [2023-01-07 01:16:57,982] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:16:57,983] {logging_mixin.py:115} INFO - [2023-01-07 01:16:57,982] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:16:57,989] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:16:58,010] {logging_mixin.py:115} INFO - [2023-01-07 01:16:58,010] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:16:58,031] {logging_mixin.py:115} INFO - [2023-01-07 01:16:58,030] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:16:58,041] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 01:17:28,111] {processor.py:153} INFO - Started process (PID=10353) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:17:28,112] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:17:28,113] {logging_mixin.py:115} INFO - [2023-01-07 01:17:28,113] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:17:28,963] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:17:28,964] {logging_mixin.py:115} INFO - [2023-01-07 01:17:28,964] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:17:28,965] {logging_mixin.py:115} INFO - [2023-01-07 01:17:28,965] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:17:28,971] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:17:28,994] {logging_mixin.py:115} INFO - [2023-01-07 01:17:28,994] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:17:29,015] {logging_mixin.py:115} INFO - [2023-01-07 01:17:29,014] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:17:29,025] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:17:59,091] {processor.py:153} INFO - Started process (PID=10371) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:17:59,093] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:17:59,093] {logging_mixin.py:115} INFO - [2023-01-07 01:17:59,093] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:18:00,077] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:18:00,078] {logging_mixin.py:115} INFO - [2023-01-07 01:18:00,078] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:18:00,079] {logging_mixin.py:115} INFO - [2023-01-07 01:18:00,078] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:18:00,085] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:18:00,107] {logging_mixin.py:115} INFO - [2023-01-07 01:18:00,107] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:18:00,128] {logging_mixin.py:115} INFO - [2023-01-07 01:18:00,128] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:18:00,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.052 seconds
[2023-01-07 01:18:30,204] {processor.py:153} INFO - Started process (PID=10397) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:18:30,205] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:18:30,206] {logging_mixin.py:115} INFO - [2023-01-07 01:18:30,206] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:18:31,102] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:18:31,103] {logging_mixin.py:115} INFO - [2023-01-07 01:18:31,103] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:18:31,104] {logging_mixin.py:115} INFO - [2023-01-07 01:18:31,104] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:18:31,115] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:18:31,145] {logging_mixin.py:115} INFO - [2023-01-07 01:18:31,144] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:18:31,168] {logging_mixin.py:115} INFO - [2023-01-07 01:18:31,168] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:18:31,179] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.980 seconds
[2023-01-07 01:19:01,266] {processor.py:153} INFO - Started process (PID=10422) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:01,266] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:19:01,267] {logging_mixin.py:115} INFO - [2023-01-07 01:19:01,267] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:02,115] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:19:02,117] {logging_mixin.py:115} INFO - [2023-01-07 01:19:02,117] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:19:02,117] {logging_mixin.py:115} INFO - [2023-01-07 01:19:02,117] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:19:02,124] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:02,145] {logging_mixin.py:115} INFO - [2023-01-07 01:19:02,145] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:19:02,166] {logging_mixin.py:115} INFO - [2023-01-07 01:19:02,165] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:19:02,176] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-07 01:19:32,244] {processor.py:153} INFO - Started process (PID=10447) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:32,245] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:19:32,246] {logging_mixin.py:115} INFO - [2023-01-07 01:19:32,246] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:33,095] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:19:33,096] {logging_mixin.py:115} INFO - [2023-01-07 01:19:33,096] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:19:33,097] {logging_mixin.py:115} INFO - [2023-01-07 01:19:33,097] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:19:33,104] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:19:33,126] {logging_mixin.py:115} INFO - [2023-01-07 01:19:33,126] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:19:33,147] {logging_mixin.py:115} INFO - [2023-01-07 01:19:33,147] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:19:33,158] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:20:03,225] {processor.py:153} INFO - Started process (PID=10473) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:03,226] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:20:03,227] {logging_mixin.py:115} INFO - [2023-01-07 01:20:03,227] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:04,089] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:20:04,091] {logging_mixin.py:115} INFO - [2023-01-07 01:20:04,091] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:20:04,091] {logging_mixin.py:115} INFO - [2023-01-07 01:20:04,091] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:20:04,099] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:04,122] {logging_mixin.py:115} INFO - [2023-01-07 01:20:04,122] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:20:04,143] {logging_mixin.py:115} INFO - [2023-01-07 01:20:04,143] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:20:04,153] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 01:20:34,222] {processor.py:153} INFO - Started process (PID=10492) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:34,224] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:20:34,224] {logging_mixin.py:115} INFO - [2023-01-07 01:20:34,224] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:35,141] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:20:35,143] {logging_mixin.py:115} INFO - [2023-01-07 01:20:35,142] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:20:35,143] {logging_mixin.py:115} INFO - [2023-01-07 01:20:35,143] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:20:35,150] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:20:35,172] {logging_mixin.py:115} INFO - [2023-01-07 01:20:35,171] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:20:35,192] {logging_mixin.py:115} INFO - [2023-01-07 01:20:35,192] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:20:35,202] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 01:21:05,297] {processor.py:153} INFO - Started process (PID=10516) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:05,298] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:21:05,298] {logging_mixin.py:115} INFO - [2023-01-07 01:21:05,298] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:06,155] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:21:06,156] {logging_mixin.py:115} INFO - [2023-01-07 01:21:06,156] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:21:06,157] {logging_mixin.py:115} INFO - [2023-01-07 01:21:06,156] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:21:06,163] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:06,185] {logging_mixin.py:115} INFO - [2023-01-07 01:21:06,185] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:21:06,205] {logging_mixin.py:115} INFO - [2023-01-07 01:21:06,205] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:21:06,216] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.924 seconds
[2023-01-07 01:21:36,284] {processor.py:153} INFO - Started process (PID=10541) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:36,286] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:21:36,286] {logging_mixin.py:115} INFO - [2023-01-07 01:21:36,286] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:37,156] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:21:37,157] {logging_mixin.py:115} INFO - [2023-01-07 01:21:37,157] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:21:37,158] {logging_mixin.py:115} INFO - [2023-01-07 01:21:37,158] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:21:37,165] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:21:37,187] {logging_mixin.py:115} INFO - [2023-01-07 01:21:37,186] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:21:37,206] {logging_mixin.py:115} INFO - [2023-01-07 01:21:37,206] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:21:37,217] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 01:22:08,191] {processor.py:153} INFO - Started process (PID=10566) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:08,194] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:22:08,195] {logging_mixin.py:115} INFO - [2023-01-07 01:22:08,195] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:09,076] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:22:09,077] {logging_mixin.py:115} INFO - [2023-01-07 01:22:09,077] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:22:09,078] {logging_mixin.py:115} INFO - [2023-01-07 01:22:09,077] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:22:09,084] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:09,107] {logging_mixin.py:115} INFO - [2023-01-07 01:22:09,107] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:22:09,127] {logging_mixin.py:115} INFO - [2023-01-07 01:22:09,127] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:22:09,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.951 seconds
[2023-01-07 01:22:39,280] {processor.py:153} INFO - Started process (PID=10584) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:39,282] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:22:39,283] {logging_mixin.py:115} INFO - [2023-01-07 01:22:39,283] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:40,280] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:22:40,281] {logging_mixin.py:115} INFO - [2023-01-07 01:22:40,281] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:22:40,281] {logging_mixin.py:115} INFO - [2023-01-07 01:22:40,281] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:22:40,288] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:22:40,310] {logging_mixin.py:115} INFO - [2023-01-07 01:22:40,310] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:22:40,337] {logging_mixin.py:115} INFO - [2023-01-07 01:22:40,337] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:22:40,353] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.077 seconds
[2023-01-07 01:23:10,436] {processor.py:153} INFO - Started process (PID=10609) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:10,437] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:23:10,438] {logging_mixin.py:115} INFO - [2023-01-07 01:23:10,437] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:11,288] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:23:11,289] {logging_mixin.py:115} INFO - [2023-01-07 01:23:11,289] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:23:11,289] {logging_mixin.py:115} INFO - [2023-01-07 01:23:11,289] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:23:11,296] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:11,318] {logging_mixin.py:115} INFO - [2023-01-07 01:23:11,318] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:23:11,338] {logging_mixin.py:115} INFO - [2023-01-07 01:23:11,338] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:23:11,348] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 01:23:41,419] {processor.py:153} INFO - Started process (PID=10634) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:41,420] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:23:41,421] {logging_mixin.py:115} INFO - [2023-01-07 01:23:41,421] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:42,279] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:23:42,280] {logging_mixin.py:115} INFO - [2023-01-07 01:23:42,280] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:23:42,281] {logging_mixin.py:115} INFO - [2023-01-07 01:23:42,281] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:23:42,292] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:23:42,314] {logging_mixin.py:115} INFO - [2023-01-07 01:23:42,314] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:23:42,334] {logging_mixin.py:115} INFO - [2023-01-07 01:23:42,334] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:23:42,347] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 01:24:12,421] {processor.py:153} INFO - Started process (PID=10659) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:12,422] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:24:12,423] {logging_mixin.py:115} INFO - [2023-01-07 01:24:12,423] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:13,290] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:24:13,292] {logging_mixin.py:115} INFO - [2023-01-07 01:24:13,292] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:24:13,292] {logging_mixin.py:115} INFO - [2023-01-07 01:24:13,292] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:24:13,299] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:13,321] {logging_mixin.py:115} INFO - [2023-01-07 01:24:13,321] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:24:13,342] {logging_mixin.py:115} INFO - [2023-01-07 01:24:13,342] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:24:13,354] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 01:24:43,429] {processor.py:153} INFO - Started process (PID=10679) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:43,430] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:24:43,431] {logging_mixin.py:115} INFO - [2023-01-07 01:24:43,431] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:44,284] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:24:44,285] {logging_mixin.py:115} INFO - [2023-01-07 01:24:44,285] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:24:44,285] {logging_mixin.py:115} INFO - [2023-01-07 01:24:44,285] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:24:44,292] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:24:44,314] {logging_mixin.py:115} INFO - [2023-01-07 01:24:44,314] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:24:44,334] {logging_mixin.py:115} INFO - [2023-01-07 01:24:44,334] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:24:44,345] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 01:25:14,420] {processor.py:153} INFO - Started process (PID=10704) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:14,421] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:25:14,421] {logging_mixin.py:115} INFO - [2023-01-07 01:25:14,421] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:15,264] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:25:15,265] {logging_mixin.py:115} INFO - [2023-01-07 01:25:15,265] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:25:15,266] {logging_mixin.py:115} INFO - [2023-01-07 01:25:15,265] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:25:15,272] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:15,294] {logging_mixin.py:115} INFO - [2023-01-07 01:25:15,294] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:25:15,314] {logging_mixin.py:115} INFO - [2023-01-07 01:25:15,314] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:25:15,325] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.909 seconds
[2023-01-07 01:25:45,403] {processor.py:153} INFO - Started process (PID=10729) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:45,404] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:25:45,405] {logging_mixin.py:115} INFO - [2023-01-07 01:25:45,405] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:46,332] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:25:46,334] {logging_mixin.py:115} INFO - [2023-01-07 01:25:46,334] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:25:46,334] {logging_mixin.py:115} INFO - [2023-01-07 01:25:46,334] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:25:46,345] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:25:46,374] {logging_mixin.py:115} INFO - [2023-01-07 01:25:46,373] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:25:46,403] {logging_mixin.py:115} INFO - [2023-01-07 01:25:46,403] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:25:46,414] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.016 seconds
[2023-01-07 01:26:16,480] {processor.py:153} INFO - Started process (PID=10754) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:16,482] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:26:16,482] {logging_mixin.py:115} INFO - [2023-01-07 01:26:16,482] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:17,354] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:26:17,355] {logging_mixin.py:115} INFO - [2023-01-07 01:26:17,355] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:26:17,356] {logging_mixin.py:115} INFO - [2023-01-07 01:26:17,355] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:26:17,362] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:17,384] {logging_mixin.py:115} INFO - [2023-01-07 01:26:17,384] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:26:17,405] {logging_mixin.py:115} INFO - [2023-01-07 01:26:17,405] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:26:17,414] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 01:26:47,480] {processor.py:153} INFO - Started process (PID=10773) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:47,481] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:26:47,481] {logging_mixin.py:115} INFO - [2023-01-07 01:26:47,481] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:48,371] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:26:48,373] {logging_mixin.py:115} INFO - [2023-01-07 01:26:48,373] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:26:48,373] {logging_mixin.py:115} INFO - [2023-01-07 01:26:48,373] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:26:48,380] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:26:48,403] {logging_mixin.py:115} INFO - [2023-01-07 01:26:48,402] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:26:48,424] {logging_mixin.py:115} INFO - [2023-01-07 01:26:48,424] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:26:48,433] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.958 seconds
[2023-01-07 01:27:18,508] {processor.py:153} INFO - Started process (PID=10798) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:18,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:27:18,510] {logging_mixin.py:115} INFO - [2023-01-07 01:27:18,510] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:19,448] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:27:19,449] {logging_mixin.py:115} INFO - [2023-01-07 01:27:19,449] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:27:19,449] {logging_mixin.py:115} INFO - [2023-01-07 01:27:19,449] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:27:19,456] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:19,488] {logging_mixin.py:115} INFO - [2023-01-07 01:27:19,487] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:27:19,517] {logging_mixin.py:115} INFO - [2023-01-07 01:27:19,517] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:27:19,528] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.025 seconds
[2023-01-07 01:27:49,614] {processor.py:153} INFO - Started process (PID=10822) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:49,615] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:27:49,616] {logging_mixin.py:115} INFO - [2023-01-07 01:27:49,616] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:50,508] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:27:50,510] {logging_mixin.py:115} INFO - [2023-01-07 01:27:50,510] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:27:50,510] {logging_mixin.py:115} INFO - [2023-01-07 01:27:50,510] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:27:50,517] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:27:50,538] {logging_mixin.py:115} INFO - [2023-01-07 01:27:50,538] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:27:50,559] {logging_mixin.py:115} INFO - [2023-01-07 01:27:50,559] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:27:50,570] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 01:28:20,647] {processor.py:153} INFO - Started process (PID=10846) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:20,647] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:28:20,648] {logging_mixin.py:115} INFO - [2023-01-07 01:28:20,648] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:21,519] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:28:21,520] {logging_mixin.py:115} INFO - [2023-01-07 01:28:21,520] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:28:21,520] {logging_mixin.py:115} INFO - [2023-01-07 01:28:21,520] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:28:21,527] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:21,549] {logging_mixin.py:115} INFO - [2023-01-07 01:28:21,549] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:28:21,571] {logging_mixin.py:115} INFO - [2023-01-07 01:28:21,571] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:28:21,580] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 01:28:51,650] {processor.py:153} INFO - Started process (PID=10867) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:51,651] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:28:51,652] {logging_mixin.py:115} INFO - [2023-01-07 01:28:51,652] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:52,859] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:28:52,860] {logging_mixin.py:115} INFO - [2023-01-07 01:28:52,860] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:28:52,861] {logging_mixin.py:115} INFO - [2023-01-07 01:28:52,861] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:28:52,872] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:28:52,902] {logging_mixin.py:115} INFO - [2023-01-07 01:28:52,902] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:28:52,926] {logging_mixin.py:115} INFO - [2023-01-07 01:28:52,926] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:28:52,936] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.292 seconds
[2023-01-07 01:29:23,005] {processor.py:153} INFO - Started process (PID=10889) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:23,006] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:29:23,007] {logging_mixin.py:115} INFO - [2023-01-07 01:29:23,007] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:23,945] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:29:23,946] {logging_mixin.py:115} INFO - [2023-01-07 01:29:23,946] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:29:23,947] {logging_mixin.py:115} INFO - [2023-01-07 01:29:23,947] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:29:23,953] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:23,975] {logging_mixin.py:115} INFO - [2023-01-07 01:29:23,975] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:29:23,997] {logging_mixin.py:115} INFO - [2023-01-07 01:29:23,996] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:29:24,006] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.005 seconds
[2023-01-07 01:29:54,074] {processor.py:153} INFO - Started process (PID=10914) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:54,075] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:29:54,076] {logging_mixin.py:115} INFO - [2023-01-07 01:29:54,076] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:54,936] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:29:54,937] {logging_mixin.py:115} INFO - [2023-01-07 01:29:54,937] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:29:54,938] {logging_mixin.py:115} INFO - [2023-01-07 01:29:54,937] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:29:54,944] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:29:54,966] {logging_mixin.py:115} INFO - [2023-01-07 01:29:54,966] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:29:54,987] {logging_mixin.py:115} INFO - [2023-01-07 01:29:54,987] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:29:54,997] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 01:30:25,062] {processor.py:153} INFO - Started process (PID=10939) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:25,063] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:30:25,064] {logging_mixin.py:115} INFO - [2023-01-07 01:30:25,064] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:25,957] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:30:25,959] {logging_mixin.py:115} INFO - [2023-01-07 01:30:25,959] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:30:25,959] {logging_mixin.py:115} INFO - [2023-01-07 01:30:25,959] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:30:25,966] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:25,987] {logging_mixin.py:115} INFO - [2023-01-07 01:30:25,987] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:30:26,009] {logging_mixin.py:115} INFO - [2023-01-07 01:30:26,009] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:30:26,018] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-07 01:30:56,086] {processor.py:153} INFO - Started process (PID=10964) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:56,087] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:30:56,087] {logging_mixin.py:115} INFO - [2023-01-07 01:30:56,087] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:56,954] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:30:56,955] {logging_mixin.py:115} INFO - [2023-01-07 01:30:56,955] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:30:56,956] {logging_mixin.py:115} INFO - [2023-01-07 01:30:56,955] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:30:56,962] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:30:56,984] {logging_mixin.py:115} INFO - [2023-01-07 01:30:56,984] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:30:57,005] {logging_mixin.py:115} INFO - [2023-01-07 01:30:57,005] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:30:57,014] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 01:31:27,085] {processor.py:153} INFO - Started process (PID=10982) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:27,086] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:31:27,086] {logging_mixin.py:115} INFO - [2023-01-07 01:31:27,086] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:27,941] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:31:27,942] {logging_mixin.py:115} INFO - [2023-01-07 01:31:27,942] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:31:27,943] {logging_mixin.py:115} INFO - [2023-01-07 01:31:27,942] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:31:27,949] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:27,972] {logging_mixin.py:115} INFO - [2023-01-07 01:31:27,971] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:31:27,994] {logging_mixin.py:115} INFO - [2023-01-07 01:31:27,993] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:31:28,003] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 01:31:58,072] {processor.py:153} INFO - Started process (PID=11008) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:58,073] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:31:58,074] {logging_mixin.py:115} INFO - [2023-01-07 01:31:58,073] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:58,946] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:31:58,947] {logging_mixin.py:115} INFO - [2023-01-07 01:31:58,947] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:31:58,948] {logging_mixin.py:115} INFO - [2023-01-07 01:31:58,947] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:31:58,954] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:31:58,982] {logging_mixin.py:115} INFO - [2023-01-07 01:31:58,982] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:31:59,013] {logging_mixin.py:115} INFO - [2023-01-07 01:31:59,012] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:31:59,024] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 01:32:29,094] {processor.py:153} INFO - Started process (PID=11033) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:32:29,096] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:32:29,097] {logging_mixin.py:115} INFO - [2023-01-07 01:32:29,096] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:32:29,949] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:32:29,951] {logging_mixin.py:115} INFO - [2023-01-07 01:32:29,951] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:32:29,951] {logging_mixin.py:115} INFO - [2023-01-07 01:32:29,951] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:32:29,958] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:32:29,979] {logging_mixin.py:115} INFO - [2023-01-07 01:32:29,978] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:32:30,000] {logging_mixin.py:115} INFO - [2023-01-07 01:32:29,999] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:32:30,009] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.919 seconds
[2023-01-07 01:33:00,075] {processor.py:153} INFO - Started process (PID=11057) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:00,076] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:33:00,077] {logging_mixin.py:115} INFO - [2023-01-07 01:33:00,077] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:00,955] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:33:00,956] {logging_mixin.py:115} INFO - [2023-01-07 01:33:00,956] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:33:00,956] {logging_mixin.py:115} INFO - [2023-01-07 01:33:00,956] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:33:00,963] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:00,985] {logging_mixin.py:115} INFO - [2023-01-07 01:33:00,984] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:33:01,006] {logging_mixin.py:115} INFO - [2023-01-07 01:33:01,005] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:33:01,015] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.944 seconds
[2023-01-07 01:33:31,090] {processor.py:153} INFO - Started process (PID=11075) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:31,091] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:33:31,092] {logging_mixin.py:115} INFO - [2023-01-07 01:33:31,091] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:32,149] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:33:32,150] {logging_mixin.py:115} INFO - [2023-01-07 01:33:32,150] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:33:32,151] {logging_mixin.py:115} INFO - [2023-01-07 01:33:32,150] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:33:32,161] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:33:32,184] {logging_mixin.py:115} INFO - [2023-01-07 01:33:32,183] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:33:32,205] {logging_mixin.py:115} INFO - [2023-01-07 01:33:32,204] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:33:32,215] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.131 seconds
[2023-01-07 01:34:02,290] {processor.py:153} INFO - Started process (PID=11100) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:02,294] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:34:02,294] {logging_mixin.py:115} INFO - [2023-01-07 01:34:02,294] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:03,155] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:34:03,156] {logging_mixin.py:115} INFO - [2023-01-07 01:34:03,156] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:34:03,157] {logging_mixin.py:115} INFO - [2023-01-07 01:34:03,157] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:34:03,164] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:03,185] {logging_mixin.py:115} INFO - [2023-01-07 01:34:03,185] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:34:03,207] {logging_mixin.py:115} INFO - [2023-01-07 01:34:03,207] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:34:03,216] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.931 seconds
[2023-01-07 01:34:34,089] {processor.py:153} INFO - Started process (PID=11124) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:34,089] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:34:34,090] {logging_mixin.py:115} INFO - [2023-01-07 01:34:34,090] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:34,971] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:34:34,972] {logging_mixin.py:115} INFO - [2023-01-07 01:34:34,972] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:34:34,973] {logging_mixin.py:115} INFO - [2023-01-07 01:34:34,972] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:34:34,980] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:34:35,001] {logging_mixin.py:115} INFO - [2023-01-07 01:34:35,001] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:34:35,022] {logging_mixin.py:115} INFO - [2023-01-07 01:34:35,022] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:34:35,032] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.948 seconds
[2023-01-07 01:35:05,064] {processor.py:153} INFO - Started process (PID=11148) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:05,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:35:05,066] {logging_mixin.py:115} INFO - [2023-01-07 01:35:05,066] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:05,977] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:35:05,979] {logging_mixin.py:115} INFO - [2023-01-07 01:35:05,979] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:35:05,979] {logging_mixin.py:115} INFO - [2023-01-07 01:35:05,979] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:35:05,986] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:06,007] {logging_mixin.py:115} INFO - [2023-01-07 01:35:06,007] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:35:06,028] {logging_mixin.py:115} INFO - [2023-01-07 01:35:06,028] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:35:06,037] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 01:35:36,189] {processor.py:153} INFO - Started process (PID=11166) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:36,190] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:35:36,191] {logging_mixin.py:115} INFO - [2023-01-07 01:35:36,191] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:37,082] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:35:37,083] {logging_mixin.py:115} INFO - [2023-01-07 01:35:37,083] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:35:37,084] {logging_mixin.py:115} INFO - [2023-01-07 01:35:37,083] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:35:37,090] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:35:37,119] {logging_mixin.py:115} INFO - [2023-01-07 01:35:37,118] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:35:37,147] {logging_mixin.py:115} INFO - [2023-01-07 01:35:37,147] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:35:37,156] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 01:36:07,230] {processor.py:153} INFO - Started process (PID=11191) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:07,231] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:36:07,232] {logging_mixin.py:115} INFO - [2023-01-07 01:36:07,232] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:08,102] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:36:08,104] {logging_mixin.py:115} INFO - [2023-01-07 01:36:08,104] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:36:08,104] {logging_mixin.py:115} INFO - [2023-01-07 01:36:08,104] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:36:08,111] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:08,134] {logging_mixin.py:115} INFO - [2023-01-07 01:36:08,133] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:36:08,159] {logging_mixin.py:115} INFO - [2023-01-07 01:36:08,159] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:36:08,169] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 01:36:38,254] {processor.py:153} INFO - Started process (PID=11217) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:38,255] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:36:38,257] {logging_mixin.py:115} INFO - [2023-01-07 01:36:38,257] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:39,145] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:36:39,146] {logging_mixin.py:115} INFO - [2023-01-07 01:36:39,146] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:36:39,147] {logging_mixin.py:115} INFO - [2023-01-07 01:36:39,147] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:36:39,154] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:36:39,176] {logging_mixin.py:115} INFO - [2023-01-07 01:36:39,175] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:36:39,198] {logging_mixin.py:115} INFO - [2023-01-07 01:36:39,197] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:36:39,207] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 01:37:09,280] {processor.py:153} INFO - Started process (PID=11241) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:09,281] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:37:09,283] {logging_mixin.py:115} INFO - [2023-01-07 01:37:09,283] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:10,133] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:37:10,134] {logging_mixin.py:115} INFO - [2023-01-07 01:37:10,134] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:37:10,135] {logging_mixin.py:115} INFO - [2023-01-07 01:37:10,134] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:37:10,141] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:10,163] {logging_mixin.py:115} INFO - [2023-01-07 01:37:10,162] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:37:10,183] {logging_mixin.py:115} INFO - [2023-01-07 01:37:10,183] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:37:10,192] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 01:37:40,266] {processor.py:153} INFO - Started process (PID=11265) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:40,267] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:37:40,269] {logging_mixin.py:115} INFO - [2023-01-07 01:37:40,269] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:41,135] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:37:41,136] {logging_mixin.py:115} INFO - [2023-01-07 01:37:41,136] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:37:41,137] {logging_mixin.py:115} INFO - [2023-01-07 01:37:41,137] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:37:41,144] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:37:41,166] {logging_mixin.py:115} INFO - [2023-01-07 01:37:41,165] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:37:41,187] {logging_mixin.py:115} INFO - [2023-01-07 01:37:41,187] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:37:41,196] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.936 seconds
[2023-01-07 01:38:11,265] {processor.py:153} INFO - Started process (PID=11284) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:11,267] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:38:11,268] {logging_mixin.py:115} INFO - [2023-01-07 01:38:11,268] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:12,355] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:38:12,357] {logging_mixin.py:115} INFO - [2023-01-07 01:38:12,357] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:38:12,358] {logging_mixin.py:115} INFO - [2023-01-07 01:38:12,358] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:38:12,369] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:12,395] {logging_mixin.py:115} INFO - [2023-01-07 01:38:12,395] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:38:12,420] {logging_mixin.py:115} INFO - [2023-01-07 01:38:12,420] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:38:12,430] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.169 seconds
[2023-01-07 01:38:42,511] {processor.py:153} INFO - Started process (PID=11309) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:42,519] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:38:42,520] {logging_mixin.py:115} INFO - [2023-01-07 01:38:42,520] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:43,385] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:38:43,387] {logging_mixin.py:115} INFO - [2023-01-07 01:38:43,387] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:38:43,387] {logging_mixin.py:115} INFO - [2023-01-07 01:38:43,387] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:38:43,394] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:38:43,415] {logging_mixin.py:115} INFO - [2023-01-07 01:38:43,415] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:38:43,436] {logging_mixin.py:115} INFO - [2023-01-07 01:38:43,436] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:38:43,445] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 01:39:14,254] {processor.py:153} INFO - Started process (PID=11333) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:14,255] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:39:14,256] {logging_mixin.py:115} INFO - [2023-01-07 01:39:14,256] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:15,095] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:39:15,096] {logging_mixin.py:115} INFO - [2023-01-07 01:39:15,096] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:39:15,096] {logging_mixin.py:115} INFO - [2023-01-07 01:39:15,096] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:39:15,103] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:15,124] {logging_mixin.py:115} INFO - [2023-01-07 01:39:15,124] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:39:15,145] {logging_mixin.py:115} INFO - [2023-01-07 01:39:15,145] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:39:15,154] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.904 seconds
[2023-01-07 01:39:45,248] {processor.py:153} INFO - Started process (PID=11358) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:45,249] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:39:45,251] {logging_mixin.py:115} INFO - [2023-01-07 01:39:45,251] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:46,227] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:39:46,228] {logging_mixin.py:115} INFO - [2023-01-07 01:39:46,228] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:39:46,229] {logging_mixin.py:115} INFO - [2023-01-07 01:39:46,229] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:39:46,236] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:39:46,259] {logging_mixin.py:115} INFO - [2023-01-07 01:39:46,258] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:39:46,281] {logging_mixin.py:115} INFO - [2023-01-07 01:39:46,281] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:39:46,290] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.048 seconds
[2023-01-07 01:40:16,354] {processor.py:153} INFO - Started process (PID=11376) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:16,355] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:40:16,357] {logging_mixin.py:115} INFO - [2023-01-07 01:40:16,357] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:17,297] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:40:17,299] {logging_mixin.py:115} INFO - [2023-01-07 01:40:17,298] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:40:17,299] {logging_mixin.py:115} INFO - [2023-01-07 01:40:17,299] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:40:17,306] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:17,327] {logging_mixin.py:115} INFO - [2023-01-07 01:40:17,327] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:40:17,348] {logging_mixin.py:115} INFO - [2023-01-07 01:40:17,348] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:40:17,357] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.008 seconds
[2023-01-07 01:40:47,427] {processor.py:153} INFO - Started process (PID=11402) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:47,427] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:40:47,428] {logging_mixin.py:115} INFO - [2023-01-07 01:40:47,428] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:48,289] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:40:48,290] {logging_mixin.py:115} INFO - [2023-01-07 01:40:48,290] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:40:48,290] {logging_mixin.py:115} INFO - [2023-01-07 01:40:48,290] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:40:48,298] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:40:48,320] {logging_mixin.py:115} INFO - [2023-01-07 01:40:48,319] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:40:48,340] {logging_mixin.py:115} INFO - [2023-01-07 01:40:48,340] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:40:48,350] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-07 01:41:18,418] {processor.py:153} INFO - Started process (PID=11427) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:18,420] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:41:18,420] {logging_mixin.py:115} INFO - [2023-01-07 01:41:18,420] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:19,278] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:41:19,280] {logging_mixin.py:115} INFO - [2023-01-07 01:41:19,280] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:41:19,280] {logging_mixin.py:115} INFO - [2023-01-07 01:41:19,280] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:41:19,287] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:19,308] {logging_mixin.py:115} INFO - [2023-01-07 01:41:19,308] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:41:19,329] {logging_mixin.py:115} INFO - [2023-01-07 01:41:19,329] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:41:19,338] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 01:41:49,405] {processor.py:153} INFO - Started process (PID=11452) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:49,406] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:41:49,406] {logging_mixin.py:115} INFO - [2023-01-07 01:41:49,406] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:50,252] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:41:50,254] {logging_mixin.py:115} INFO - [2023-01-07 01:41:50,254] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:41:50,254] {logging_mixin.py:115} INFO - [2023-01-07 01:41:50,254] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:41:50,261] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:41:50,282] {logging_mixin.py:115} INFO - [2023-01-07 01:41:50,281] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:41:50,303] {logging_mixin.py:115} INFO - [2023-01-07 01:41:50,303] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:41:50,311] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-07 01:42:20,386] {processor.py:153} INFO - Started process (PID=11471) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:20,388] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:42:20,388] {logging_mixin.py:115} INFO - [2023-01-07 01:42:20,388] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:21,281] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:42:21,282] {logging_mixin.py:115} INFO - [2023-01-07 01:42:21,282] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:42:21,282] {logging_mixin.py:115} INFO - [2023-01-07 01:42:21,282] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:42:21,289] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:21,313] {logging_mixin.py:115} INFO - [2023-01-07 01:42:21,313] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:42:21,337] {logging_mixin.py:115} INFO - [2023-01-07 01:42:21,337] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:42:21,347] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.965 seconds
[2023-01-07 01:42:51,421] {processor.py:153} INFO - Started process (PID=11496) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:51,422] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:42:51,422] {logging_mixin.py:115} INFO - [2023-01-07 01:42:51,422] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:52,322] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:42:52,323] {logging_mixin.py:115} INFO - [2023-01-07 01:42:52,323] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:42:52,324] {logging_mixin.py:115} INFO - [2023-01-07 01:42:52,324] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:42:52,330] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:42:52,352] {logging_mixin.py:115} INFO - [2023-01-07 01:42:52,351] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:42:52,372] {logging_mixin.py:115} INFO - [2023-01-07 01:42:52,372] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:42:52,381] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.965 seconds
[2023-01-07 01:43:22,451] {processor.py:153} INFO - Started process (PID=11520) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:22,452] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:43:22,453] {logging_mixin.py:115} INFO - [2023-01-07 01:43:22,453] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:23,304] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:43:23,306] {logging_mixin.py:115} INFO - [2023-01-07 01:43:23,306] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:43:23,306] {logging_mixin.py:115} INFO - [2023-01-07 01:43:23,306] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:43:23,313] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:23,334] {logging_mixin.py:115} INFO - [2023-01-07 01:43:23,334] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:43:23,355] {logging_mixin.py:115} INFO - [2023-01-07 01:43:23,355] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:43:23,364] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:43:53,433] {processor.py:153} INFO - Started process (PID=11546) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:53,434] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:43:53,435] {logging_mixin.py:115} INFO - [2023-01-07 01:43:53,434] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:54,295] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:43:54,296] {logging_mixin.py:115} INFO - [2023-01-07 01:43:54,296] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:43:54,297] {logging_mixin.py:115} INFO - [2023-01-07 01:43:54,296] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:43:54,303] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:43:54,325] {logging_mixin.py:115} INFO - [2023-01-07 01:43:54,324] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:43:54,346] {logging_mixin.py:115} INFO - [2023-01-07 01:43:54,346] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:43:54,355] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 01:44:24,428] {processor.py:153} INFO - Started process (PID=11563) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:24,431] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:44:24,432] {logging_mixin.py:115} INFO - [2023-01-07 01:44:24,432] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:25,387] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:44:25,388] {logging_mixin.py:115} INFO - [2023-01-07 01:44:25,388] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:44:25,389] {logging_mixin.py:115} INFO - [2023-01-07 01:44:25,388] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:44:25,395] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:25,417] {logging_mixin.py:115} INFO - [2023-01-07 01:44:25,417] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:44:25,439] {logging_mixin.py:115} INFO - [2023-01-07 01:44:25,439] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:44:25,448] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.024 seconds
[2023-01-07 01:44:55,515] {processor.py:153} INFO - Started process (PID=11588) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:55,515] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:44:55,516] {logging_mixin.py:115} INFO - [2023-01-07 01:44:55,516] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:56,354] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:44:56,355] {logging_mixin.py:115} INFO - [2023-01-07 01:44:56,355] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:44:56,356] {logging_mixin.py:115} INFO - [2023-01-07 01:44:56,355] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:44:56,363] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:44:56,384] {logging_mixin.py:115} INFO - [2023-01-07 01:44:56,384] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:44:56,405] {logging_mixin.py:115} INFO - [2023-01-07 01:44:56,405] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:44:56,415] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.904 seconds
[2023-01-07 01:45:26,482] {processor.py:153} INFO - Started process (PID=11613) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:26,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:45:26,483] {logging_mixin.py:115} INFO - [2023-01-07 01:45:26,483] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:27,337] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:45:27,338] {logging_mixin.py:115} INFO - [2023-01-07 01:45:27,338] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:45:27,339] {logging_mixin.py:115} INFO - [2023-01-07 01:45:27,338] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:45:27,345] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:27,366] {logging_mixin.py:115} INFO - [2023-01-07 01:45:27,366] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:45:27,388] {logging_mixin.py:115} INFO - [2023-01-07 01:45:27,388] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:45:27,397] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 01:45:57,479] {processor.py:153} INFO - Started process (PID=11638) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:57,480] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:45:57,480] {logging_mixin.py:115} INFO - [2023-01-07 01:45:57,480] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:58,356] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:45:58,357] {logging_mixin.py:115} INFO - [2023-01-07 01:45:58,357] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:45:58,358] {logging_mixin.py:115} INFO - [2023-01-07 01:45:58,358] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:45:58,364] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:45:58,386] {logging_mixin.py:115} INFO - [2023-01-07 01:45:58,385] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:45:58,407] {logging_mixin.py:115} INFO - [2023-01-07 01:45:58,407] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:45:58,416] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 01:46:28,487] {processor.py:153} INFO - Started process (PID=11656) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:46:28,488] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:46:28,489] {logging_mixin.py:115} INFO - [2023-01-07 01:46:28,489] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:46:29,454] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:46:29,455] {logging_mixin.py:115} INFO - [2023-01-07 01:46:29,455] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:46:29,455] {logging_mixin.py:115} INFO - [2023-01-07 01:46:29,455] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:46:29,462] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:46:29,483] {logging_mixin.py:115} INFO - [2023-01-07 01:46:29,483] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:46:29,504] {logging_mixin.py:115} INFO - [2023-01-07 01:46:29,504] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:46:29,513] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.031 seconds
[2023-01-07 01:46:59,581] {processor.py:153} INFO - Started process (PID=11680) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:46:59,584] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:46:59,584] {logging_mixin.py:115} INFO - [2023-01-07 01:46:59,584] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:47:00,471] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:47:00,473] {logging_mixin.py:115} INFO - [2023-01-07 01:47:00,473] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:47:00,473] {logging_mixin.py:115} INFO - [2023-01-07 01:47:00,473] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:47:00,480] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:47:00,501] {logging_mixin.py:115} INFO - [2023-01-07 01:47:00,501] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:47:00,530] {logging_mixin.py:115} INFO - [2023-01-07 01:47:00,530] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:47:00,541] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.964 seconds
[2023-01-07 01:47:31,504] {processor.py:153} INFO - Started process (PID=11706) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:47:31,505] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:47:31,505] {logging_mixin.py:115} INFO - [2023-01-07 01:47:31,505] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:47:32,349] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:47:32,351] {logging_mixin.py:115} INFO - [2023-01-07 01:47:32,351] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:47:32,351] {logging_mixin.py:115} INFO - [2023-01-07 01:47:32,351] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:47:32,358] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:47:32,380] {logging_mixin.py:115} INFO - [2023-01-07 01:47:32,380] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:47:32,402] {logging_mixin.py:115} INFO - [2023-01-07 01:47:32,402] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:47:32,411] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.912 seconds
[2023-01-07 01:48:02,507] {processor.py:153} INFO - Started process (PID=11732) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:02,508] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:48:02,508] {logging_mixin.py:115} INFO - [2023-01-07 01:48:02,508] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:03,388] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:48:03,389] {logging_mixin.py:115} INFO - [2023-01-07 01:48:03,389] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:48:03,390] {logging_mixin.py:115} INFO - [2023-01-07 01:48:03,389] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:48:03,396] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:03,418] {logging_mixin.py:115} INFO - [2023-01-07 01:48:03,417] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:48:03,439] {logging_mixin.py:115} INFO - [2023-01-07 01:48:03,438] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:48:03,448] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.946 seconds
[2023-01-07 01:48:33,478] {processor.py:153} INFO - Started process (PID=11757) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:33,479] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:48:33,480] {logging_mixin.py:115} INFO - [2023-01-07 01:48:33,479] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:34,334] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:48:34,335] {logging_mixin.py:115} INFO - [2023-01-07 01:48:34,335] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:48:34,336] {logging_mixin.py:115} INFO - [2023-01-07 01:48:34,335] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:48:34,342] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:48:34,363] {logging_mixin.py:115} INFO - [2023-01-07 01:48:34,363] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:48:34,384] {logging_mixin.py:115} INFO - [2023-01-07 01:48:34,384] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:48:34,393] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 01:49:04,593] {processor.py:153} INFO - Started process (PID=11775) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:04,594] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:49:04,594] {logging_mixin.py:115} INFO - [2023-01-07 01:49:04,594] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:05,444] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:49:05,445] {logging_mixin.py:115} INFO - [2023-01-07 01:49:05,445] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:49:05,445] {logging_mixin.py:115} INFO - [2023-01-07 01:49:05,445] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:49:05,452] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:05,474] {logging_mixin.py:115} INFO - [2023-01-07 01:49:05,473] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:49:05,495] {logging_mixin.py:115} INFO - [2023-01-07 01:49:05,494] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:49:05,504] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-07 01:49:35,579] {processor.py:153} INFO - Started process (PID=11800) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:35,580] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:49:35,581] {logging_mixin.py:115} INFO - [2023-01-07 01:49:35,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:36,433] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:49:36,434] {logging_mixin.py:115} INFO - [2023-01-07 01:49:36,434] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:49:36,435] {logging_mixin.py:115} INFO - [2023-01-07 01:49:36,435] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:49:36,441] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:49:36,463] {logging_mixin.py:115} INFO - [2023-01-07 01:49:36,462] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:49:36,483] {logging_mixin.py:115} INFO - [2023-01-07 01:49:36,483] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:49:36,492] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:50:06,560] {processor.py:153} INFO - Started process (PID=11825) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:06,561] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:50:06,561] {logging_mixin.py:115} INFO - [2023-01-07 01:50:06,561] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:07,410] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:50:07,411] {logging_mixin.py:115} INFO - [2023-01-07 01:50:07,411] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:50:07,411] {logging_mixin.py:115} INFO - [2023-01-07 01:50:07,411] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:50:07,418] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:07,439] {logging_mixin.py:115} INFO - [2023-01-07 01:50:07,439] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:50:07,460] {logging_mixin.py:115} INFO - [2023-01-07 01:50:07,460] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:50:07,469] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.913 seconds
[2023-01-07 01:50:37,536] {processor.py:153} INFO - Started process (PID=11851) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:37,538] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:50:37,538] {logging_mixin.py:115} INFO - [2023-01-07 01:50:37,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:38,392] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:50:38,393] {logging_mixin.py:115} INFO - [2023-01-07 01:50:38,393] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:50:38,394] {logging_mixin.py:115} INFO - [2023-01-07 01:50:38,393] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:50:38,400] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:50:38,421] {logging_mixin.py:115} INFO - [2023-01-07 01:50:38,421] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:50:38,445] {logging_mixin.py:115} INFO - [2023-01-07 01:50:38,445] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:50:38,456] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.924 seconds
[2023-01-07 01:51:08,540] {processor.py:153} INFO - Started process (PID=11870) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:08,543] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:51:08,544] {logging_mixin.py:115} INFO - [2023-01-07 01:51:08,544] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:09,424] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:51:09,425] {logging_mixin.py:115} INFO - [2023-01-07 01:51:09,425] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:51:09,426] {logging_mixin.py:115} INFO - [2023-01-07 01:51:09,426] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:51:09,432] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:09,458] {logging_mixin.py:115} INFO - [2023-01-07 01:51:09,457] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:51:09,482] {logging_mixin.py:115} INFO - [2023-01-07 01:51:09,482] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:51:09,491] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 01:51:40,215] {processor.py:153} INFO - Started process (PID=11894) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:40,217] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:51:40,217] {logging_mixin.py:115} INFO - [2023-01-07 01:51:40,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:41,063] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:51:41,064] {logging_mixin.py:115} INFO - [2023-01-07 01:51:41,064] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:51:41,065] {logging_mixin.py:115} INFO - [2023-01-07 01:51:41,065] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:51:41,071] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:51:41,093] {logging_mixin.py:115} INFO - [2023-01-07 01:51:41,092] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:51:41,119] {logging_mixin.py:115} INFO - [2023-01-07 01:51:41,119] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:51:41,128] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 01:52:11,221] {processor.py:153} INFO - Started process (PID=11919) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:11,222] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:52:11,223] {logging_mixin.py:115} INFO - [2023-01-07 01:52:11,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:12,079] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:52:12,080] {logging_mixin.py:115} INFO - [2023-01-07 01:52:12,080] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:52:12,080] {logging_mixin.py:115} INFO - [2023-01-07 01:52:12,080] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:52:12,087] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:12,108] {logging_mixin.py:115} INFO - [2023-01-07 01:52:12,108] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:52:12,129] {logging_mixin.py:115} INFO - [2023-01-07 01:52:12,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:52:12,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.922 seconds
[2023-01-07 01:52:42,172] {processor.py:153} INFO - Started process (PID=11945) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:42,174] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:52:42,174] {logging_mixin.py:115} INFO - [2023-01-07 01:52:42,174] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:43,052] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:52:43,054] {logging_mixin.py:115} INFO - [2023-01-07 01:52:43,054] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:52:43,054] {logging_mixin.py:115} INFO - [2023-01-07 01:52:43,054] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:52:43,061] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:52:43,087] {logging_mixin.py:115} INFO - [2023-01-07 01:52:43,087] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:52:43,109] {logging_mixin.py:115} INFO - [2023-01-07 01:52:43,109] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:52:43,119] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 01:53:13,353] {processor.py:153} INFO - Started process (PID=11963) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:13,355] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:53:13,355] {logging_mixin.py:115} INFO - [2023-01-07 01:53:13,355] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:14,280] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:53:14,281] {logging_mixin.py:115} INFO - [2023-01-07 01:53:14,281] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:53:14,282] {logging_mixin.py:115} INFO - [2023-01-07 01:53:14,282] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:53:14,289] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:14,310] {logging_mixin.py:115} INFO - [2023-01-07 01:53:14,310] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:53:14,332] {logging_mixin.py:115} INFO - [2023-01-07 01:53:14,332] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:53:14,341] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.992 seconds
[2023-01-07 01:53:44,415] {processor.py:153} INFO - Started process (PID=11990) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:44,416] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:53:44,416] {logging_mixin.py:115} INFO - [2023-01-07 01:53:44,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:45,347] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:53:45,349] {logging_mixin.py:115} INFO - [2023-01-07 01:53:45,349] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:53:45,349] {logging_mixin.py:115} INFO - [2023-01-07 01:53:45,349] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:53:45,356] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:53:45,377] {logging_mixin.py:115} INFO - [2023-01-07 01:53:45,377] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:53:45,398] {logging_mixin.py:115} INFO - [2023-01-07 01:53:45,398] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:53:45,407] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.997 seconds
[2023-01-07 01:54:15,476] {processor.py:153} INFO - Started process (PID=12017) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:15,477] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:54:15,478] {logging_mixin.py:115} INFO - [2023-01-07 01:54:15,478] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:16,353] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:54:16,354] {logging_mixin.py:115} INFO - [2023-01-07 01:54:16,354] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:54:16,355] {logging_mixin.py:115} INFO - [2023-01-07 01:54:16,354] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:54:16,361] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:16,384] {logging_mixin.py:115} INFO - [2023-01-07 01:54:16,383] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:54:16,405] {logging_mixin.py:115} INFO - [2023-01-07 01:54:16,405] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:54:16,415] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 01:54:46,482] {processor.py:153} INFO - Started process (PID=12042) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:46,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:54:46,484] {logging_mixin.py:115} INFO - [2023-01-07 01:54:46,484] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:47,342] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:54:47,343] {logging_mixin.py:115} INFO - [2023-01-07 01:54:47,343] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:54:47,344] {logging_mixin.py:115} INFO - [2023-01-07 01:54:47,344] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:54:47,351] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:54:47,372] {logging_mixin.py:115} INFO - [2023-01-07 01:54:47,372] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:54:47,393] {logging_mixin.py:115} INFO - [2023-01-07 01:54:47,393] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:54:47,402] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-07 01:55:17,466] {processor.py:153} INFO - Started process (PID=12067) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:17,468] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:55:17,468] {logging_mixin.py:115} INFO - [2023-01-07 01:55:17,468] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:18,321] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:55:18,322] {logging_mixin.py:115} INFO - [2023-01-07 01:55:18,322] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:55:18,322] {logging_mixin.py:115} INFO - [2023-01-07 01:55:18,322] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:55:18,329] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:18,350] {logging_mixin.py:115} INFO - [2023-01-07 01:55:18,350] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:55:18,371] {logging_mixin.py:115} INFO - [2023-01-07 01:55:18,371] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:55:18,380] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 01:55:48,447] {processor.py:153} INFO - Started process (PID=12083) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:48,448] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:55:48,449] {logging_mixin.py:115} INFO - [2023-01-07 01:55:48,449] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:49,335] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:55:49,336] {logging_mixin.py:115} INFO - [2023-01-07 01:55:49,336] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:55:49,337] {logging_mixin.py:115} INFO - [2023-01-07 01:55:49,337] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:55:49,347] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:55:49,370] {logging_mixin.py:115} INFO - [2023-01-07 01:55:49,370] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:55:49,392] {logging_mixin.py:115} INFO - [2023-01-07 01:55:49,392] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:55:49,401] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.958 seconds
[2023-01-07 01:56:19,471] {processor.py:153} INFO - Started process (PID=12108) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:19,473] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:56:19,474] {logging_mixin.py:115} INFO - [2023-01-07 01:56:19,474] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:20,332] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:56:20,333] {logging_mixin.py:115} INFO - [2023-01-07 01:56:20,333] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:56:20,333] {logging_mixin.py:115} INFO - [2023-01-07 01:56:20,333] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:56:20,340] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:20,362] {logging_mixin.py:115} INFO - [2023-01-07 01:56:20,361] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:56:20,383] {logging_mixin.py:115} INFO - [2023-01-07 01:56:20,383] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:56:20,392] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 01:56:50,461] {processor.py:153} INFO - Started process (PID=12133) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:50,463] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:56:50,463] {logging_mixin.py:115} INFO - [2023-01-07 01:56:50,463] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:51,319] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:56:51,321] {logging_mixin.py:115} INFO - [2023-01-07 01:56:51,320] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:56:51,321] {logging_mixin.py:115} INFO - [2023-01-07 01:56:51,321] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:56:51,328] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:56:51,349] {logging_mixin.py:115} INFO - [2023-01-07 01:56:51,349] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:56:51,370] {logging_mixin.py:115} INFO - [2023-01-07 01:56:51,370] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:56:51,379] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 01:57:21,450] {processor.py:153} INFO - Started process (PID=12158) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:21,451] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:57:21,451] {logging_mixin.py:115} INFO - [2023-01-07 01:57:21,451] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:22,331] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:57:22,332] {logging_mixin.py:115} INFO - [2023-01-07 01:57:22,332] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:57:22,333] {logging_mixin.py:115} INFO - [2023-01-07 01:57:22,332] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:57:22,339] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:22,361] {logging_mixin.py:115} INFO - [2023-01-07 01:57:22,361] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:57:22,383] {logging_mixin.py:115} INFO - [2023-01-07 01:57:22,383] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:57:22,392] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.947 seconds
[2023-01-07 01:57:52,463] {processor.py:153} INFO - Started process (PID=12176) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:52,464] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:57:52,464] {logging_mixin.py:115} INFO - [2023-01-07 01:57:52,464] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:53,319] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:57:53,320] {logging_mixin.py:115} INFO - [2023-01-07 01:57:53,320] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:57:53,321] {logging_mixin.py:115} INFO - [2023-01-07 01:57:53,321] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:57:53,328] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:57:53,350] {logging_mixin.py:115} INFO - [2023-01-07 01:57:53,350] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:57:53,371] {logging_mixin.py:115} INFO - [2023-01-07 01:57:53,371] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:57:53,380] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 01:58:23,460] {processor.py:153} INFO - Started process (PID=12200) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:23,461] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:58:23,462] {logging_mixin.py:115} INFO - [2023-01-07 01:58:23,462] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:24,317] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:58:24,318] {logging_mixin.py:115} INFO - [2023-01-07 01:58:24,318] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:58:24,319] {logging_mixin.py:115} INFO - [2023-01-07 01:58:24,318] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:58:24,326] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:24,347] {logging_mixin.py:115} INFO - [2023-01-07 01:58:24,347] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:58:24,368] {logging_mixin.py:115} INFO - [2023-01-07 01:58:24,368] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:58:24,377] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.922 seconds
[2023-01-07 01:58:54,471] {processor.py:153} INFO - Started process (PID=12224) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:54,471] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:58:54,472] {logging_mixin.py:115} INFO - [2023-01-07 01:58:54,472] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:55,358] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:58:55,359] {logging_mixin.py:115} INFO - [2023-01-07 01:58:55,359] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:58:55,360] {logging_mixin.py:115} INFO - [2023-01-07 01:58:55,359] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:58:55,366] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:58:55,389] {logging_mixin.py:115} INFO - [2023-01-07 01:58:55,389] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:58:55,410] {logging_mixin.py:115} INFO - [2023-01-07 01:58:55,410] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:58:55,420] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-07 01:59:25,514] {processor.py:153} INFO - Started process (PID=12248) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:25,516] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:59:25,517] {logging_mixin.py:115} INFO - [2023-01-07 01:59:25,517] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:26,381] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:59:26,383] {logging_mixin.py:115} INFO - [2023-01-07 01:59:26,383] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:59:26,383] {logging_mixin.py:115} INFO - [2023-01-07 01:59:26,383] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:59:26,390] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:26,412] {logging_mixin.py:115} INFO - [2023-01-07 01:59:26,411] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:59:26,433] {logging_mixin.py:115} INFO - [2023-01-07 01:59:26,432] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:59:26,442] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 01:59:56,539] {processor.py:153} INFO - Started process (PID=12266) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:56,539] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 01:59:56,540] {logging_mixin.py:115} INFO - [2023-01-07 01:59:56,540] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:57,416] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 01:59:57,417] {logging_mixin.py:115} INFO - [2023-01-07 01:59:57,417] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 01:59:57,418] {logging_mixin.py:115} INFO - [2023-01-07 01:59:57,418] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 01:59:57,428] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 01:59:57,461] {logging_mixin.py:115} INFO - [2023-01-07 01:59:57,461] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 01:59:57,485] {logging_mixin.py:115} INFO - [2023-01-07 01:59:57,485] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 01:59:57,495] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 02:00:27,586] {processor.py:153} INFO - Started process (PID=12290) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:27,587] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:00:27,588] {logging_mixin.py:115} INFO - [2023-01-07 02:00:27,587] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:28,489] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:00:28,491] {logging_mixin.py:115} INFO - [2023-01-07 02:00:28,491] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:00:28,492] {logging_mixin.py:115} INFO - [2023-01-07 02:00:28,491] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:00:28,502] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:28,526] {logging_mixin.py:115} INFO - [2023-01-07 02:00:28,525] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:00:28,549] {logging_mixin.py:115} INFO - [2023-01-07 02:00:28,549] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:00:28,559] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 02:00:58,641] {processor.py:153} INFO - Started process (PID=12315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:58,642] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:00:58,642] {logging_mixin.py:115} INFO - [2023-01-07 02:00:58,642] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:59,523] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:00:59,525] {logging_mixin.py:115} INFO - [2023-01-07 02:00:59,525] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:00:59,525] {logging_mixin.py:115} INFO - [2023-01-07 02:00:59,525] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:00:59,532] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:00:59,553] {logging_mixin.py:115} INFO - [2023-01-07 02:00:59,553] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:00:59,574] {logging_mixin.py:115} INFO - [2023-01-07 02:00:59,574] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:00:59,583] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.947 seconds
[2023-01-07 02:01:29,655] {processor.py:153} INFO - Started process (PID=12340) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:01:29,657] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:01:29,657] {logging_mixin.py:115} INFO - [2023-01-07 02:01:29,657] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:01:30,518] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:01:30,520] {logging_mixin.py:115} INFO - [2023-01-07 02:01:30,519] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:01:30,520] {logging_mixin.py:115} INFO - [2023-01-07 02:01:30,520] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:01:30,527] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:01:30,548] {logging_mixin.py:115} INFO - [2023-01-07 02:01:30,547] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:01:30,569] {logging_mixin.py:115} INFO - [2023-01-07 02:01:30,569] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:01:30,578] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 02:02:00,644] {processor.py:153} INFO - Started process (PID=12359) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:00,645] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:02:00,646] {logging_mixin.py:115} INFO - [2023-01-07 02:02:00,645] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:01,619] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:02:01,621] {logging_mixin.py:115} INFO - [2023-01-07 02:02:01,620] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:02:01,621] {logging_mixin.py:115} INFO - [2023-01-07 02:02:01,621] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:02:01,633] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:01,662] {logging_mixin.py:115} INFO - [2023-01-07 02:02:01,662] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:02:01,693] {logging_mixin.py:115} INFO - [2023-01-07 02:02:01,693] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:02:01,705] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.066 seconds
[2023-01-07 02:02:31,780] {processor.py:153} INFO - Started process (PID=12384) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:31,781] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:02:31,782] {logging_mixin.py:115} INFO - [2023-01-07 02:02:31,782] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:32,642] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:02:32,644] {logging_mixin.py:115} INFO - [2023-01-07 02:02:32,643] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:02:32,644] {logging_mixin.py:115} INFO - [2023-01-07 02:02:32,644] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:02:32,651] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:02:32,672] {logging_mixin.py:115} INFO - [2023-01-07 02:02:32,672] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:02:32,693] {logging_mixin.py:115} INFO - [2023-01-07 02:02:32,693] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:02:32,702] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 02:03:02,765] {processor.py:153} INFO - Started process (PID=12409) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:02,766] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:03:02,767] {logging_mixin.py:115} INFO - [2023-01-07 02:03:02,767] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:03,610] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:03:03,611] {logging_mixin.py:115} INFO - [2023-01-07 02:03:03,611] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:03:03,612] {logging_mixin.py:115} INFO - [2023-01-07 02:03:03,611] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:03:03,618] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:03,640] {logging_mixin.py:115} INFO - [2023-01-07 02:03:03,640] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:03:03,662] {logging_mixin.py:115} INFO - [2023-01-07 02:03:03,662] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:03:03,672] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-07 02:03:33,738] {processor.py:153} INFO - Started process (PID=12434) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:33,739] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:03:33,739] {logging_mixin.py:115} INFO - [2023-01-07 02:03:33,739] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:34,657] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:03:34,658] {logging_mixin.py:115} INFO - [2023-01-07 02:03:34,658] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:03:34,659] {logging_mixin.py:115} INFO - [2023-01-07 02:03:34,658] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:03:34,665] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:03:34,691] {logging_mixin.py:115} INFO - [2023-01-07 02:03:34,691] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:03:34,720] {logging_mixin.py:115} INFO - [2023-01-07 02:03:34,720] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:03:34,732] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.999 seconds
[2023-01-07 02:04:04,805] {processor.py:153} INFO - Started process (PID=12458) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:04,806] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:04:04,807] {logging_mixin.py:115} INFO - [2023-01-07 02:04:04,807] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:05,729] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:04:05,730] {logging_mixin.py:115} INFO - [2023-01-07 02:04:05,730] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:04:05,731] {logging_mixin.py:115} INFO - [2023-01-07 02:04:05,730] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:04:05,737] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:05,761] {logging_mixin.py:115} INFO - [2023-01-07 02:04:05,760] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:04:05,783] {logging_mixin.py:115} INFO - [2023-01-07 02:04:05,783] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:04:05,793] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.993 seconds
[2023-01-07 02:04:35,864] {processor.py:153} INFO - Started process (PID=12476) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:35,865] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:04:35,865] {logging_mixin.py:115} INFO - [2023-01-07 02:04:35,865] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:36,724] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:04:36,725] {logging_mixin.py:115} INFO - [2023-01-07 02:04:36,725] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:04:36,726] {logging_mixin.py:115} INFO - [2023-01-07 02:04:36,725] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:04:36,732] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:04:36,760] {logging_mixin.py:115} INFO - [2023-01-07 02:04:36,760] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:04:36,785] {logging_mixin.py:115} INFO - [2023-01-07 02:04:36,785] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:04:36,795] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.935 seconds
[2023-01-07 02:05:06,861] {processor.py:153} INFO - Started process (PID=12501) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:06,863] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:05:06,864] {logging_mixin.py:115} INFO - [2023-01-07 02:05:06,864] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:07,776] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:05:07,778] {logging_mixin.py:115} INFO - [2023-01-07 02:05:07,778] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:05:07,778] {logging_mixin.py:115} INFO - [2023-01-07 02:05:07,778] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:05:07,785] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:07,806] {logging_mixin.py:115} INFO - [2023-01-07 02:05:07,806] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:05:07,828] {logging_mixin.py:115} INFO - [2023-01-07 02:05:07,827] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:05:07,837] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.980 seconds
[2023-01-07 02:05:37,908] {processor.py:153} INFO - Started process (PID=12526) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:37,909] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:05:37,910] {logging_mixin.py:115} INFO - [2023-01-07 02:05:37,909] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:38,754] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:05:38,755] {logging_mixin.py:115} INFO - [2023-01-07 02:05:38,755] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:05:38,756] {logging_mixin.py:115} INFO - [2023-01-07 02:05:38,755] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:05:38,762] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:05:38,788] {logging_mixin.py:115} INFO - [2023-01-07 02:05:38,788] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:05:38,809] {logging_mixin.py:115} INFO - [2023-01-07 02:05:38,809] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:05:38,818] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.914 seconds
[2023-01-07 02:06:08,885] {processor.py:153} INFO - Started process (PID=12551) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:08,888] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:06:08,889] {logging_mixin.py:115} INFO - [2023-01-07 02:06:08,888] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:09,743] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:06:09,745] {logging_mixin.py:115} INFO - [2023-01-07 02:06:09,745] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:06:09,745] {logging_mixin.py:115} INFO - [2023-01-07 02:06:09,745] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:06:09,752] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:09,773] {logging_mixin.py:115} INFO - [2023-01-07 02:06:09,773] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:06:09,794] {logging_mixin.py:115} INFO - [2023-01-07 02:06:09,794] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:06:09,803] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 02:06:39,880] {processor.py:153} INFO - Started process (PID=12570) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:39,883] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:06:39,886] {logging_mixin.py:115} INFO - [2023-01-07 02:06:39,886] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:41,049] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:06:41,050] {logging_mixin.py:115} INFO - [2023-01-07 02:06:41,050] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:06:41,051] {logging_mixin.py:115} INFO - [2023-01-07 02:06:41,051] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:06:41,061] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:06:41,087] {logging_mixin.py:115} INFO - [2023-01-07 02:06:41,086] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:06:41,109] {logging_mixin.py:115} INFO - [2023-01-07 02:06:41,109] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:06:41,118] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.243 seconds
[2023-01-07 02:07:11,953] {processor.py:153} INFO - Started process (PID=12595) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:11,954] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:07:11,956] {logging_mixin.py:115} INFO - [2023-01-07 02:07:11,956] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:12,826] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:07:12,828] {logging_mixin.py:115} INFO - [2023-01-07 02:07:12,828] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:07:12,828] {logging_mixin.py:115} INFO - [2023-01-07 02:07:12,828] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:07:12,835] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:12,856] {logging_mixin.py:115} INFO - [2023-01-07 02:07:12,856] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:07:12,877] {logging_mixin.py:115} INFO - [2023-01-07 02:07:12,877] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:07:12,886] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 02:07:43,012] {processor.py:153} INFO - Started process (PID=12620) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:43,013] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:07:43,015] {logging_mixin.py:115} INFO - [2023-01-07 02:07:43,015] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:43,867] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:07:43,868] {logging_mixin.py:115} INFO - [2023-01-07 02:07:43,868] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:07:43,869] {logging_mixin.py:115} INFO - [2023-01-07 02:07:43,868] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:07:43,875] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:07:43,897] {logging_mixin.py:115} INFO - [2023-01-07 02:07:43,896] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:07:43,918] {logging_mixin.py:115} INFO - [2023-01-07 02:07:43,917] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:07:43,927] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 02:08:13,996] {processor.py:153} INFO - Started process (PID=12644) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:13,996] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:08:13,998] {logging_mixin.py:115} INFO - [2023-01-07 02:08:13,998] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:14,868] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:08:14,869] {logging_mixin.py:115} INFO - [2023-01-07 02:08:14,869] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:08:14,869] {logging_mixin.py:115} INFO - [2023-01-07 02:08:14,869] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:08:14,876] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:14,897] {logging_mixin.py:115} INFO - [2023-01-07 02:08:14,897] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:08:14,919] {logging_mixin.py:115} INFO - [2023-01-07 02:08:14,918] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:08:14,928] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.937 seconds
[2023-01-07 02:08:44,994] {processor.py:153} INFO - Started process (PID=12662) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:44,995] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:08:44,998] {logging_mixin.py:115} INFO - [2023-01-07 02:08:44,997] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:45,869] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:08:45,870] {logging_mixin.py:115} INFO - [2023-01-07 02:08:45,870] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:08:45,871] {logging_mixin.py:115} INFO - [2023-01-07 02:08:45,871] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:08:45,878] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:08:45,899] {logging_mixin.py:115} INFO - [2023-01-07 02:08:45,899] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:08:45,920] {logging_mixin.py:115} INFO - [2023-01-07 02:08:45,920] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:08:45,930] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.940 seconds
[2023-01-07 02:09:16,010] {processor.py:153} INFO - Started process (PID=12687) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:16,010] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:09:16,012] {logging_mixin.py:115} INFO - [2023-01-07 02:09:16,012] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:16,853] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:09:16,855] {logging_mixin.py:115} INFO - [2023-01-07 02:09:16,855] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:09:16,855] {logging_mixin.py:115} INFO - [2023-01-07 02:09:16,855] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:09:16,862] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:16,883] {logging_mixin.py:115} INFO - [2023-01-07 02:09:16,883] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:09:16,904] {logging_mixin.py:115} INFO - [2023-01-07 02:09:16,904] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:09:16,913] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.908 seconds
[2023-01-07 02:09:46,981] {processor.py:153} INFO - Started process (PID=12712) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:46,982] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:09:46,984] {logging_mixin.py:115} INFO - [2023-01-07 02:09:46,984] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:47,844] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:09:47,845] {logging_mixin.py:115} INFO - [2023-01-07 02:09:47,845] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:09:47,846] {logging_mixin.py:115} INFO - [2023-01-07 02:09:47,845] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:09:47,852] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:09:47,873] {logging_mixin.py:115} INFO - [2023-01-07 02:09:47,873] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:09:47,899] {logging_mixin.py:115} INFO - [2023-01-07 02:09:47,899] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:09:47,908] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-07 02:10:17,979] {processor.py:153} INFO - Started process (PID=12738) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:17,980] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:10:17,982] {logging_mixin.py:115} INFO - [2023-01-07 02:10:17,982] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:18,836] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:10:18,838] {logging_mixin.py:115} INFO - [2023-01-07 02:10:18,838] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:10:18,838] {logging_mixin.py:115} INFO - [2023-01-07 02:10:18,838] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:10:18,845] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:18,867] {logging_mixin.py:115} INFO - [2023-01-07 02:10:18,866] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:10:18,888] {logging_mixin.py:115} INFO - [2023-01-07 02:10:18,888] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:10:18,897] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.922 seconds
[2023-01-07 02:10:48,976] {processor.py:153} INFO - Started process (PID=12763) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:48,977] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:10:48,978] {logging_mixin.py:115} INFO - [2023-01-07 02:10:48,978] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:49,882] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:10:49,883] {logging_mixin.py:115} INFO - [2023-01-07 02:10:49,883] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:10:49,883] {logging_mixin.py:115} INFO - [2023-01-07 02:10:49,883] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:10:49,890] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:10:49,912] {logging_mixin.py:115} INFO - [2023-01-07 02:10:49,911] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:10:49,934] {logging_mixin.py:115} INFO - [2023-01-07 02:10:49,934] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:10:49,943] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 02:11:20,025] {processor.py:153} INFO - Started process (PID=12781) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:20,026] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:11:20,026] {logging_mixin.py:115} INFO - [2023-01-07 02:11:20,026] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:20,911] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:11:20,912] {logging_mixin.py:115} INFO - [2023-01-07 02:11:20,912] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:11:20,913] {logging_mixin.py:115} INFO - [2023-01-07 02:11:20,913] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:11:20,920] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:20,942] {logging_mixin.py:115} INFO - [2023-01-07 02:11:20,942] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:11:20,963] {logging_mixin.py:115} INFO - [2023-01-07 02:11:20,963] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:11:20,972] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 02:11:51,069] {processor.py:153} INFO - Started process (PID=12806) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:51,070] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:11:51,071] {logging_mixin.py:115} INFO - [2023-01-07 02:11:51,071] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:51,931] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:11:51,932] {logging_mixin.py:115} INFO - [2023-01-07 02:11:51,932] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:11:51,933] {logging_mixin.py:115} INFO - [2023-01-07 02:11:51,932] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:11:51,939] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:11:51,960] {logging_mixin.py:115} INFO - [2023-01-07 02:11:51,960] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:11:51,981] {logging_mixin.py:115} INFO - [2023-01-07 02:11:51,981] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:11:51,990] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 02:12:22,082] {processor.py:153} INFO - Started process (PID=12831) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:22,083] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:12:22,083] {logging_mixin.py:115} INFO - [2023-01-07 02:12:22,083] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:22,936] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:12:22,937] {logging_mixin.py:115} INFO - [2023-01-07 02:12:22,937] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:12:22,938] {logging_mixin.py:115} INFO - [2023-01-07 02:12:22,937] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:12:22,944] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:22,965] {logging_mixin.py:115} INFO - [2023-01-07 02:12:22,965] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:12:22,986] {logging_mixin.py:115} INFO - [2023-01-07 02:12:22,986] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:12:22,995] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 02:12:53,094] {processor.py:153} INFO - Started process (PID=12856) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:53,096] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:12:53,096] {logging_mixin.py:115} INFO - [2023-01-07 02:12:53,096] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:53,949] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:12:53,950] {logging_mixin.py:115} INFO - [2023-01-07 02:12:53,950] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:12:53,951] {logging_mixin.py:115} INFO - [2023-01-07 02:12:53,950] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:12:53,957] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:12:53,978] {logging_mixin.py:115} INFO - [2023-01-07 02:12:53,978] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:12:53,999] {logging_mixin.py:115} INFO - [2023-01-07 02:12:53,999] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:12:54,008] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 02:13:24,101] {processor.py:153} INFO - Started process (PID=12874) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:24,102] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:13:24,102] {logging_mixin.py:115} INFO - [2023-01-07 02:13:24,102] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:24,981] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:13:24,983] {logging_mixin.py:115} INFO - [2023-01-07 02:13:24,982] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:13:24,983] {logging_mixin.py:115} INFO - [2023-01-07 02:13:24,983] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:13:24,990] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:25,012] {logging_mixin.py:115} INFO - [2023-01-07 02:13:25,012] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:13:25,034] {logging_mixin.py:115} INFO - [2023-01-07 02:13:25,034] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:13:25,043] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.946 seconds
[2023-01-07 02:13:55,178] {processor.py:153} INFO - Started process (PID=12898) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:55,180] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:13:55,180] {logging_mixin.py:115} INFO - [2023-01-07 02:13:55,180] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:56,047] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:13:56,049] {logging_mixin.py:115} INFO - [2023-01-07 02:13:56,048] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:13:56,049] {logging_mixin.py:115} INFO - [2023-01-07 02:13:56,049] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:13:56,056] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:13:56,080] {logging_mixin.py:115} INFO - [2023-01-07 02:13:56,079] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:13:56,101] {logging_mixin.py:115} INFO - [2023-01-07 02:13:56,101] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:13:56,110] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.936 seconds
[2023-01-07 02:14:26,177] {processor.py:153} INFO - Started process (PID=12923) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:26,179] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:14:26,179] {logging_mixin.py:115} INFO - [2023-01-07 02:14:26,179] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:27,034] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:14:27,035] {logging_mixin.py:115} INFO - [2023-01-07 02:14:27,035] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:14:27,036] {logging_mixin.py:115} INFO - [2023-01-07 02:14:27,035] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:14:27,042] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:27,063] {logging_mixin.py:115} INFO - [2023-01-07 02:14:27,063] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:14:27,084] {logging_mixin.py:115} INFO - [2023-01-07 02:14:27,084] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:14:27,093] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 02:14:57,165] {processor.py:153} INFO - Started process (PID=12949) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:57,165] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:14:57,166] {logging_mixin.py:115} INFO - [2023-01-07 02:14:57,166] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:58,028] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:14:58,029] {logging_mixin.py:115} INFO - [2023-01-07 02:14:58,029] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:14:58,030] {logging_mixin.py:115} INFO - [2023-01-07 02:14:58,029] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:14:58,036] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:14:58,058] {logging_mixin.py:115} INFO - [2023-01-07 02:14:58,058] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:14:58,079] {logging_mixin.py:115} INFO - [2023-01-07 02:14:58,078] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:14:58,087] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 02:15:28,159] {processor.py:153} INFO - Started process (PID=12968) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:15:28,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:15:28,161] {logging_mixin.py:115} INFO - [2023-01-07 02:15:28,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:15:29,038] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:15:29,039] {logging_mixin.py:115} INFO - [2023-01-07 02:15:29,039] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:15:29,040] {logging_mixin.py:115} INFO - [2023-01-07 02:15:29,039] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:15:29,046] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:15:29,070] {logging_mixin.py:115} INFO - [2023-01-07 02:15:29,070] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:15:29,094] {logging_mixin.py:115} INFO - [2023-01-07 02:15:29,094] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:15:29,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 02:15:59,188] {processor.py:153} INFO - Started process (PID=12994) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:15:59,189] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:15:59,190] {logging_mixin.py:115} INFO - [2023-01-07 02:15:59,190] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:16:00,038] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:16:00,039] {logging_mixin.py:115} INFO - [2023-01-07 02:16:00,039] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:16:00,040] {logging_mixin.py:115} INFO - [2023-01-07 02:16:00,039] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:16:00,046] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:16:00,068] {logging_mixin.py:115} INFO - [2023-01-07 02:16:00,068] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:16:00,089] {logging_mixin.py:115} INFO - [2023-01-07 02:16:00,089] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:16:00,098] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-07 02:16:30,170] {processor.py:153} INFO - Started process (PID=13019) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:16:30,171] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:16:30,171] {logging_mixin.py:115} INFO - [2023-01-07 02:16:30,171] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:16:31,046] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:16:31,048] {logging_mixin.py:115} INFO - [2023-01-07 02:16:31,048] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:16:31,048] {logging_mixin.py:115} INFO - [2023-01-07 02:16:31,048] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:16:31,055] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:16:31,076] {logging_mixin.py:115} INFO - [2023-01-07 02:16:31,076] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:16:31,097] {logging_mixin.py:115} INFO - [2023-01-07 02:16:31,097] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:16:31,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.941 seconds
[2023-01-07 02:17:01,175] {processor.py:153} INFO - Started process (PID=13045) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:01,176] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:17:01,177] {logging_mixin.py:115} INFO - [2023-01-07 02:17:01,177] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:02,033] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:17:02,035] {logging_mixin.py:115} INFO - [2023-01-07 02:17:02,035] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:17:02,035] {logging_mixin.py:115} INFO - [2023-01-07 02:17:02,035] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:17:02,042] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:02,063] {logging_mixin.py:115} INFO - [2023-01-07 02:17:02,063] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:17:02,084] {logging_mixin.py:115} INFO - [2023-01-07 02:17:02,084] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:17:02,093] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-07 02:17:32,162] {processor.py:153} INFO - Started process (PID=13062) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:32,163] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:17:32,164] {logging_mixin.py:115} INFO - [2023-01-07 02:17:32,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:33,093] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:17:33,094] {logging_mixin.py:115} INFO - [2023-01-07 02:17:33,094] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:17:33,095] {logging_mixin.py:115} INFO - [2023-01-07 02:17:33,094] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:17:33,101] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:17:33,123] {logging_mixin.py:115} INFO - [2023-01-07 02:17:33,123] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:17:33,145] {logging_mixin.py:115} INFO - [2023-01-07 02:17:33,144] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:17:33,154] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.996 seconds
[2023-01-07 02:18:03,226] {processor.py:153} INFO - Started process (PID=13086) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:03,227] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:18:03,228] {logging_mixin.py:115} INFO - [2023-01-07 02:18:03,228] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:04,081] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:18:04,082] {logging_mixin.py:115} INFO - [2023-01-07 02:18:04,082] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:18:04,083] {logging_mixin.py:115} INFO - [2023-01-07 02:18:04,082] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:18:04,089] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:04,111] {logging_mixin.py:115} INFO - [2023-01-07 02:18:04,111] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:18:04,140] {logging_mixin.py:115} INFO - [2023-01-07 02:18:04,139] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:18:04,150] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-07 02:18:34,235] {processor.py:153} INFO - Started process (PID=13111) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:34,237] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 02:18:34,237] {logging_mixin.py:115} INFO - [2023-01-07 02:18:34,237] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:35,091] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 02:18:35,093] {logging_mixin.py:115} INFO - [2023-01-07 02:18:35,093] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 02:18:35,093] {logging_mixin.py:115} INFO - [2023-01-07 02:18:35,093] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 02:18:35,100] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 02:18:35,121] {logging_mixin.py:115} INFO - [2023-01-07 02:18:35,121] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 02:18:35,143] {logging_mixin.py:115} INFO - [2023-01-07 02:18:35,143] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 02:18:35,152] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 21:09:06,597] {processor.py:153} INFO - Started process (PID=33) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:06,613] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:09:06,613] {logging_mixin.py:115} INFO - [2023-01-07 21:09:06,613] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:10,423] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:09:10,425] {logging_mixin.py:115} INFO - [2023-01-07 21:09:10,425] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:09:10,426] {logging_mixin.py:115} INFO - [2023-01-07 21:09:10,425] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:09:10,437] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:10,469] {logging_mixin.py:115} INFO - [2023-01-07 21:09:10,468] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:09:10,504] {logging_mixin.py:115} INFO - [2023-01-07 21:09:10,504] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:09:10,523] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 3.931 seconds
[2023-01-07 21:09:40,627] {processor.py:153} INFO - Started process (PID=60) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:40,629] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:09:40,630] {logging_mixin.py:115} INFO - [2023-01-07 21:09:40,630] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:41,647] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:09:41,648] {logging_mixin.py:115} INFO - [2023-01-07 21:09:41,648] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:09:41,649] {logging_mixin.py:115} INFO - [2023-01-07 21:09:41,648] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:09:41,655] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:09:41,679] {logging_mixin.py:115} INFO - [2023-01-07 21:09:41,679] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:09:41,700] {logging_mixin.py:115} INFO - [2023-01-07 21:09:41,700] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:09:41,710] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.088 seconds
[2023-01-07 21:10:11,806] {processor.py:153} INFO - Started process (PID=84) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:11,807] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:10:11,807] {logging_mixin.py:115} INFO - [2023-01-07 21:10:11,807] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:12,755] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:10:12,756] {logging_mixin.py:115} INFO - [2023-01-07 21:10:12,756] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:10:12,756] {logging_mixin.py:115} INFO - [2023-01-07 21:10:12,756] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:10:12,763] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:12,786] {logging_mixin.py:115} INFO - [2023-01-07 21:10:12,786] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:10:12,806] {logging_mixin.py:115} INFO - [2023-01-07 21:10:12,806] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:10:12,815] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.014 seconds
[2023-01-07 21:10:42,880] {processor.py:153} INFO - Started process (PID=102) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:42,886] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:10:42,887] {logging_mixin.py:115} INFO - [2023-01-07 21:10:42,887] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:44,016] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:10:44,018] {logging_mixin.py:115} INFO - [2023-01-07 21:10:44,018] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:10:44,019] {logging_mixin.py:115} INFO - [2023-01-07 21:10:44,018] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:10:44,030] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:10:44,062] {logging_mixin.py:115} INFO - [2023-01-07 21:10:44,062] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:10:44,093] {logging_mixin.py:115} INFO - [2023-01-07 21:10:44,092] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:10:44,105] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.229 seconds
[2023-01-07 21:11:14,185] {processor.py:153} INFO - Started process (PID=130) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:14,186] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:11:14,186] {logging_mixin.py:115} INFO - [2023-01-07 21:11:14,186] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:15,200] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:11:15,201] {logging_mixin.py:115} INFO - [2023-01-07 21:11:15,201] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:11:15,202] {logging_mixin.py:115} INFO - [2023-01-07 21:11:15,201] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:11:15,208] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:15,231] {logging_mixin.py:115} INFO - [2023-01-07 21:11:15,231] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:11:15,252] {logging_mixin.py:115} INFO - [2023-01-07 21:11:15,252] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:11:15,262] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.082 seconds
[2023-01-07 21:11:45,329] {processor.py:153} INFO - Started process (PID=154) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:45,331] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:11:45,331] {logging_mixin.py:115} INFO - [2023-01-07 21:11:45,331] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:46,221] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:11:46,222] {logging_mixin.py:115} INFO - [2023-01-07 21:11:46,222] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:11:46,222] {logging_mixin.py:115} INFO - [2023-01-07 21:11:46,222] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:11:46,229] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:11:46,252] {logging_mixin.py:115} INFO - [2023-01-07 21:11:46,252] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:11:46,272] {logging_mixin.py:115} INFO - [2023-01-07 21:11:46,272] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:11:46,282] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 21:12:16,350] {processor.py:153} INFO - Started process (PID=181) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:16,351] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:12:16,352] {logging_mixin.py:115} INFO - [2023-01-07 21:12:16,352] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:17,241] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:12:17,242] {logging_mixin.py:115} INFO - [2023-01-07 21:12:17,242] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:12:17,243] {logging_mixin.py:115} INFO - [2023-01-07 21:12:17,242] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:12:17,250] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:17,273] {logging_mixin.py:115} INFO - [2023-01-07 21:12:17,273] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:12:17,293] {logging_mixin.py:115} INFO - [2023-01-07 21:12:17,293] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:12:17,303] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 21:12:47,373] {processor.py:153} INFO - Started process (PID=200) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:47,373] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:12:47,374] {logging_mixin.py:115} INFO - [2023-01-07 21:12:47,374] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:48,534] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:12:48,536] {logging_mixin.py:115} INFO - [2023-01-07 21:12:48,536] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:12:48,536] {logging_mixin.py:115} INFO - [2023-01-07 21:12:48,536] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:12:48,548] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:12:48,583] {logging_mixin.py:115} INFO - [2023-01-07 21:12:48,583] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:12:48,613] {logging_mixin.py:115} INFO - [2023-01-07 21:12:48,613] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:12:48,626] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.259 seconds
[2023-01-07 21:13:18,703] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:18,705] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:13:18,705] {logging_mixin.py:115} INFO - [2023-01-07 21:13:18,705] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:19,625] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:13:19,626] {logging_mixin.py:115} INFO - [2023-01-07 21:13:19,626] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:13:19,626] {logging_mixin.py:115} INFO - [2023-01-07 21:13:19,626] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:13:19,633] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:19,657] {logging_mixin.py:115} INFO - [2023-01-07 21:13:19,657] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:13:19,678] {logging_mixin.py:115} INFO - [2023-01-07 21:13:19,678] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:13:19,688] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.990 seconds
[2023-01-07 21:13:49,774] {processor.py:153} INFO - Started process (PID=247) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:49,776] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:13:49,776] {logging_mixin.py:115} INFO - [2023-01-07 21:13:49,776] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:50,663] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:13:50,664] {logging_mixin.py:115} INFO - [2023-01-07 21:13:50,664] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:13:50,665] {logging_mixin.py:115} INFO - [2023-01-07 21:13:50,665] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:13:50,672] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:13:50,694] {logging_mixin.py:115} INFO - [2023-01-07 21:13:50,694] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:13:50,715] {logging_mixin.py:115} INFO - [2023-01-07 21:13:50,714] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:13:50,724] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-07 21:14:20,795] {processor.py:153} INFO - Started process (PID=273) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:20,796] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:14:20,797] {logging_mixin.py:115} INFO - [2023-01-07 21:14:20,797] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:22,070] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:14:22,072] {logging_mixin.py:115} INFO - [2023-01-07 21:14:22,072] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:14:22,072] {logging_mixin.py:115} INFO - [2023-01-07 21:14:22,072] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:14:22,079] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:22,102] {logging_mixin.py:115} INFO - [2023-01-07 21:14:22,101] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:14:22,122] {logging_mixin.py:115} INFO - [2023-01-07 21:14:22,122] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:14:22,131] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.341 seconds
[2023-01-07 21:14:52,201] {processor.py:153} INFO - Started process (PID=292) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:52,202] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:14:52,202] {logging_mixin.py:115} INFO - [2023-01-07 21:14:52,202] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:53,120] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:14:53,121] {logging_mixin.py:115} INFO - [2023-01-07 21:14:53,121] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:14:53,122] {logging_mixin.py:115} INFO - [2023-01-07 21:14:53,121] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:14:53,128] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:14:53,151] {logging_mixin.py:115} INFO - [2023-01-07 21:14:53,151] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:14:53,172] {logging_mixin.py:115} INFO - [2023-01-07 21:14:53,172] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:14:53,181] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-07 21:15:23,269] {processor.py:153} INFO - Started process (PID=315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:23,270] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:15:23,271] {logging_mixin.py:115} INFO - [2023-01-07 21:15:23,271] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:24,208] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:15:24,209] {logging_mixin.py:115} INFO - [2023-01-07 21:15:24,209] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:15:24,210] {logging_mixin.py:115} INFO - [2023-01-07 21:15:24,209] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:15:24,217] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:24,240] {logging_mixin.py:115} INFO - [2023-01-07 21:15:24,239] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:15:24,260] {logging_mixin.py:115} INFO - [2023-01-07 21:15:24,260] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:15:24,270] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.006 seconds
[2023-01-07 21:15:54,345] {processor.py:153} INFO - Started process (PID=342) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:54,346] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:15:54,347] {logging_mixin.py:115} INFO - [2023-01-07 21:15:54,347] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:55,461] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:15:55,463] {logging_mixin.py:115} INFO - [2023-01-07 21:15:55,462] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:15:55,463] {logging_mixin.py:115} INFO - [2023-01-07 21:15:55,463] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:15:55,470] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:15:55,494] {logging_mixin.py:115} INFO - [2023-01-07 21:15:55,494] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:15:55,515] {logging_mixin.py:115} INFO - [2023-01-07 21:15:55,515] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:15:55,525] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.184 seconds
[2023-01-07 21:16:25,594] {processor.py:153} INFO - Started process (PID=369) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:25,596] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:16:25,596] {logging_mixin.py:115} INFO - [2023-01-07 21:16:25,596] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:26,670] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:16:26,672] {logging_mixin.py:115} INFO - [2023-01-07 21:16:26,672] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:16:26,672] {logging_mixin.py:115} INFO - [2023-01-07 21:16:26,672] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:16:26,679] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:26,703] {logging_mixin.py:115} INFO - [2023-01-07 21:16:26,702] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:16:26,723] {logging_mixin.py:115} INFO - [2023-01-07 21:16:26,723] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:16:26,733] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.144 seconds
[2023-01-07 21:16:56,801] {processor.py:153} INFO - Started process (PID=388) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:56,802] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:16:56,802] {logging_mixin.py:115} INFO - [2023-01-07 21:16:56,802] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:57,797] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:16:57,798] {logging_mixin.py:115} INFO - [2023-01-07 21:16:57,798] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:16:57,799] {logging_mixin.py:115} INFO - [2023-01-07 21:16:57,798] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:16:57,806] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:16:57,829] {logging_mixin.py:115} INFO - [2023-01-07 21:16:57,829] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:16:57,851] {logging_mixin.py:115} INFO - [2023-01-07 21:16:57,850] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:16:57,860] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.064 seconds
[2023-01-07 21:17:27,928] {processor.py:153} INFO - Started process (PID=413) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:17:27,929] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:17:27,930] {logging_mixin.py:115} INFO - [2023-01-07 21:17:27,929] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:17:29,108] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:17:29,109] {logging_mixin.py:115} INFO - [2023-01-07 21:17:29,109] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:17:29,110] {logging_mixin.py:115} INFO - [2023-01-07 21:17:29,110] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:17:29,117] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:17:29,140] {logging_mixin.py:115} INFO - [2023-01-07 21:17:29,139] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:17:29,160] {logging_mixin.py:115} INFO - [2023-01-07 21:17:29,160] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:17:29,169] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.247 seconds
[2023-01-07 21:17:59,237] {processor.py:153} INFO - Started process (PID=439) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:17:59,238] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:17:59,239] {logging_mixin.py:115} INFO - [2023-01-07 21:17:59,239] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:18:00,126] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:18:00,127] {logging_mixin.py:115} INFO - [2023-01-07 21:18:00,127] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:18:00,127] {logging_mixin.py:115} INFO - [2023-01-07 21:18:00,127] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:18:00,134] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:18:00,157] {logging_mixin.py:115} INFO - [2023-01-07 21:18:00,157] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:18:00,177] {logging_mixin.py:115} INFO - [2023-01-07 21:18:00,177] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:18:00,186] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-07 21:18:30,255] {processor.py:153} INFO - Started process (PID=464) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:18:30,257] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:18:30,258] {logging_mixin.py:115} INFO - [2023-01-07 21:18:30,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:18:31,158] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:18:31,160] {logging_mixin.py:115} INFO - [2023-01-07 21:18:31,160] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:18:31,160] {logging_mixin.py:115} INFO - [2023-01-07 21:18:31,160] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:18:31,167] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:18:31,191] {logging_mixin.py:115} INFO - [2023-01-07 21:18:31,190] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:18:31,211] {logging_mixin.py:115} INFO - [2023-01-07 21:18:31,211] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:18:31,221] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 21:19:01,291] {processor.py:153} INFO - Started process (PID=482) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:01,292] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:19:01,293] {logging_mixin.py:115} INFO - [2023-01-07 21:19:01,293] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:02,489] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:19:02,490] {logging_mixin.py:115} INFO - [2023-01-07 21:19:02,490] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:19:02,491] {logging_mixin.py:115} INFO - [2023-01-07 21:19:02,491] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:19:02,503] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:02,535] {logging_mixin.py:115} INFO - [2023-01-07 21:19:02,534] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:19:02,565] {logging_mixin.py:115} INFO - [2023-01-07 21:19:02,565] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:19:02,577] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.291 seconds
[2023-01-07 21:19:32,666] {processor.py:153} INFO - Started process (PID=509) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:32,666] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:19:32,667] {logging_mixin.py:115} INFO - [2023-01-07 21:19:32,667] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:33,554] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:19:33,556] {logging_mixin.py:115} INFO - [2023-01-07 21:19:33,555] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:19:33,556] {logging_mixin.py:115} INFO - [2023-01-07 21:19:33,556] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:19:33,563] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:19:33,587] {logging_mixin.py:115} INFO - [2023-01-07 21:19:33,587] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:19:33,607] {logging_mixin.py:115} INFO - [2023-01-07 21:19:33,607] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:19:33,617] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 21:20:03,713] {processor.py:153} INFO - Started process (PID=534) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:03,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:20:03,715] {logging_mixin.py:115} INFO - [2023-01-07 21:20:03,715] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:04,593] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:20:04,594] {logging_mixin.py:115} INFO - [2023-01-07 21:20:04,594] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:20:04,595] {logging_mixin.py:115} INFO - [2023-01-07 21:20:04,595] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:20:04,602] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:04,625] {logging_mixin.py:115} INFO - [2023-01-07 21:20:04,625] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:20:04,646] {logging_mixin.py:115} INFO - [2023-01-07 21:20:04,645] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:20:04,655] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.946 seconds
[2023-01-07 21:20:34,755] {processor.py:153} INFO - Started process (PID=558) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:34,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:20:34,757] {logging_mixin.py:115} INFO - [2023-01-07 21:20:34,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:35,683] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:20:35,685] {logging_mixin.py:115} INFO - [2023-01-07 21:20:35,685] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:20:35,685] {logging_mixin.py:115} INFO - [2023-01-07 21:20:35,685] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:20:35,694] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:20:35,743] {logging_mixin.py:115} INFO - [2023-01-07 21:20:35,742] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:20:35,765] {logging_mixin.py:115} INFO - [2023-01-07 21:20:35,765] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:20:35,775] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.025 seconds
[2023-01-07 21:21:05,847] {processor.py:153} INFO - Started process (PID=576) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:05,848] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:21:05,848] {logging_mixin.py:115} INFO - [2023-01-07 21:21:05,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:06,773] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:21:06,776] {logging_mixin.py:115} INFO - [2023-01-07 21:21:06,776] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:21:06,777] {logging_mixin.py:115} INFO - [2023-01-07 21:21:06,776] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:21:06,784] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:06,823] {logging_mixin.py:115} INFO - [2023-01-07 21:21:06,822] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:21:06,852] {logging_mixin.py:115} INFO - [2023-01-07 21:21:06,852] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:21:06,865] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.023 seconds
[2023-01-07 21:21:36,932] {processor.py:153} INFO - Started process (PID=601) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:36,933] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:21:36,934] {logging_mixin.py:115} INFO - [2023-01-07 21:21:36,933] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:37,835] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:21:37,836] {logging_mixin.py:115} INFO - [2023-01-07 21:21:37,836] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:21:37,837] {logging_mixin.py:115} INFO - [2023-01-07 21:21:37,836] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:21:37,843] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:21:37,866] {logging_mixin.py:115} INFO - [2023-01-07 21:21:37,866] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:21:37,886] {logging_mixin.py:115} INFO - [2023-01-07 21:21:37,886] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:21:37,896] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-07 21:22:07,965] {processor.py:153} INFO - Started process (PID=626) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:07,967] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:22:07,967] {logging_mixin.py:115} INFO - [2023-01-07 21:22:07,967] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:08,863] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:22:08,864] {logging_mixin.py:115} INFO - [2023-01-07 21:22:08,864] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:22:08,864] {logging_mixin.py:115} INFO - [2023-01-07 21:22:08,864] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:22:08,871] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:08,895] {logging_mixin.py:115} INFO - [2023-01-07 21:22:08,895] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:22:08,917] {logging_mixin.py:115} INFO - [2023-01-07 21:22:08,916] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:22:08,926] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 21:22:38,998] {processor.py:153} INFO - Started process (PID=652) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:38,999] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:22:38,999] {logging_mixin.py:115} INFO - [2023-01-07 21:22:38,999] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:39,894] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:22:39,895] {logging_mixin.py:115} INFO - [2023-01-07 21:22:39,895] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:22:39,895] {logging_mixin.py:115} INFO - [2023-01-07 21:22:39,895] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:22:39,902] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:22:39,926] {logging_mixin.py:115} INFO - [2023-01-07 21:22:39,926] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:22:39,947] {logging_mixin.py:115} INFO - [2023-01-07 21:22:39,947] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:22:39,956] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.963 seconds
[2023-01-07 21:23:10,027] {processor.py:153} INFO - Started process (PID=670) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:10,027] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:23:10,028] {logging_mixin.py:115} INFO - [2023-01-07 21:23:10,028] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:10,913] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:23:10,914] {logging_mixin.py:115} INFO - [2023-01-07 21:23:10,914] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:23:10,915] {logging_mixin.py:115} INFO - [2023-01-07 21:23:10,914] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:23:10,921] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:10,944] {logging_mixin.py:115} INFO - [2023-01-07 21:23:10,944] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:23:10,964] {logging_mixin.py:115} INFO - [2023-01-07 21:23:10,964] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:23:10,974] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 21:23:41,042] {processor.py:153} INFO - Started process (PID=695) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:41,043] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:23:41,044] {logging_mixin.py:115} INFO - [2023-01-07 21:23:41,044] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:41,932] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:23:41,933] {logging_mixin.py:115} INFO - [2023-01-07 21:23:41,933] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:23:41,934] {logging_mixin.py:115} INFO - [2023-01-07 21:23:41,933] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:23:41,941] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:23:41,963] {logging_mixin.py:115} INFO - [2023-01-07 21:23:41,963] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:23:41,984] {logging_mixin.py:115} INFO - [2023-01-07 21:23:41,984] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:23:41,994] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 21:24:12,080] {processor.py:153} INFO - Started process (PID=719) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:12,081] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:24:12,081] {logging_mixin.py:115} INFO - [2023-01-07 21:24:12,081] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:12,958] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:24:12,959] {logging_mixin.py:115} INFO - [2023-01-07 21:24:12,959] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:24:12,960] {logging_mixin.py:115} INFO - [2023-01-07 21:24:12,959] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:24:12,967] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:12,988] {logging_mixin.py:115} INFO - [2023-01-07 21:24:12,988] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:24:13,008] {logging_mixin.py:115} INFO - [2023-01-07 21:24:13,008] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:24:13,018] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-07 21:24:43,771] {processor.py:153} INFO - Started process (PID=744) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:43,771] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:24:43,772] {logging_mixin.py:115} INFO - [2023-01-07 21:24:43,772] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:44,666] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:24:44,667] {logging_mixin.py:115} INFO - [2023-01-07 21:24:44,667] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:24:44,668] {logging_mixin.py:115} INFO - [2023-01-07 21:24:44,667] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:24:44,674] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:24:44,696] {logging_mixin.py:115} INFO - [2023-01-07 21:24:44,696] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:24:44,716] {logging_mixin.py:115} INFO - [2023-01-07 21:24:44,716] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:24:44,725] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.959 seconds
[2023-01-07 21:25:14,801] {processor.py:153} INFO - Started process (PID=762) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:14,802] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:25:14,803] {logging_mixin.py:115} INFO - [2023-01-07 21:25:14,803] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:15,750] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:25:15,751] {logging_mixin.py:115} INFO - [2023-01-07 21:25:15,751] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:25:15,752] {logging_mixin.py:115} INFO - [2023-01-07 21:25:15,752] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:25:15,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:15,781] {logging_mixin.py:115} INFO - [2023-01-07 21:25:15,781] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:25:15,803] {logging_mixin.py:115} INFO - [2023-01-07 21:25:15,803] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:25:15,812] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.016 seconds
[2023-01-07 21:25:45,882] {processor.py:153} INFO - Started process (PID=787) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:45,883] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:25:45,884] {logging_mixin.py:115} INFO - [2023-01-07 21:25:45,884] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:46,775] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:25:46,776] {logging_mixin.py:115} INFO - [2023-01-07 21:25:46,776] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:25:46,777] {logging_mixin.py:115} INFO - [2023-01-07 21:25:46,776] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:25:46,783] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:25:46,805] {logging_mixin.py:115} INFO - [2023-01-07 21:25:46,805] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:25:46,827] {logging_mixin.py:115} INFO - [2023-01-07 21:25:46,827] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:25:46,837] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.959 seconds
[2023-01-07 21:26:16,903] {processor.py:153} INFO - Started process (PID=812) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:16,905] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:26:16,905] {logging_mixin.py:115} INFO - [2023-01-07 21:26:16,905] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:17,806] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:26:17,807] {logging_mixin.py:115} INFO - [2023-01-07 21:26:17,807] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:26:17,808] {logging_mixin.py:115} INFO - [2023-01-07 21:26:17,807] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:26:17,814] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:17,836] {logging_mixin.py:115} INFO - [2023-01-07 21:26:17,836] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:26:17,857] {logging_mixin.py:115} INFO - [2023-01-07 21:26:17,857] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:26:17,866] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-07 21:26:47,936] {processor.py:153} INFO - Started process (PID=838) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:47,937] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:26:47,938] {logging_mixin.py:115} INFO - [2023-01-07 21:26:47,938] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:48,882] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:26:48,883] {logging_mixin.py:115} INFO - [2023-01-07 21:26:48,883] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:26:48,884] {logging_mixin.py:115} INFO - [2023-01-07 21:26:48,883] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:26:48,891] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:26:48,915] {logging_mixin.py:115} INFO - [2023-01-07 21:26:48,915] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:26:48,941] {logging_mixin.py:115} INFO - [2023-01-07 21:26:48,940] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:26:48,950] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.020 seconds
[2023-01-07 21:27:19,018] {processor.py:153} INFO - Started process (PID=856) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:19,019] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:27:19,020] {logging_mixin.py:115} INFO - [2023-01-07 21:27:19,020] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:19,891] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:27:19,893] {logging_mixin.py:115} INFO - [2023-01-07 21:27:19,893] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:27:19,893] {logging_mixin.py:115} INFO - [2023-01-07 21:27:19,893] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:27:19,900] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:19,923] {logging_mixin.py:115} INFO - [2023-01-07 21:27:19,923] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:27:19,944] {logging_mixin.py:115} INFO - [2023-01-07 21:27:19,943] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:27:19,953] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.939 seconds
[2023-01-07 21:27:50,019] {processor.py:153} INFO - Started process (PID=882) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:50,020] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:27:50,021] {logging_mixin.py:115} INFO - [2023-01-07 21:27:50,021] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:50,914] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:27:50,915] {logging_mixin.py:115} INFO - [2023-01-07 21:27:50,915] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:27:50,916] {logging_mixin.py:115} INFO - [2023-01-07 21:27:50,916] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:27:50,923] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:27:50,945] {logging_mixin.py:115} INFO - [2023-01-07 21:27:50,944] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:27:50,965] {logging_mixin.py:115} INFO - [2023-01-07 21:27:50,965] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:27:50,974] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 21:28:21,046] {processor.py:153} INFO - Started process (PID=910) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:21,047] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:28:21,048] {logging_mixin.py:115} INFO - [2023-01-07 21:28:21,047] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:21,943] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:28:21,944] {logging_mixin.py:115} INFO - [2023-01-07 21:28:21,944] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:28:21,945] {logging_mixin.py:115} INFO - [2023-01-07 21:28:21,944] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:28:21,951] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:21,973] {logging_mixin.py:115} INFO - [2023-01-07 21:28:21,973] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:28:21,994] {logging_mixin.py:115} INFO - [2023-01-07 21:28:21,993] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:28:22,003] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 21:28:52,069] {processor.py:153} INFO - Started process (PID=935) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:52,070] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:28:52,070] {logging_mixin.py:115} INFO - [2023-01-07 21:28:52,070] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:53,022] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:28:53,023] {logging_mixin.py:115} INFO - [2023-01-07 21:28:53,023] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:28:53,024] {logging_mixin.py:115} INFO - [2023-01-07 21:28:53,023] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:28:53,030] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:28:53,052] {logging_mixin.py:115} INFO - [2023-01-07 21:28:53,052] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:28:53,073] {logging_mixin.py:115} INFO - [2023-01-07 21:28:53,073] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:28:53,082] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.018 seconds
[2023-01-07 21:29:23,158] {processor.py:153} INFO - Started process (PID=953) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:29:23,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:29:23,160] {logging_mixin.py:115} INFO - [2023-01-07 21:29:23,160] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:29:24,081] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:29:24,082] {logging_mixin.py:115} INFO - [2023-01-07 21:29:24,082] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:29:24,083] {logging_mixin.py:115} INFO - [2023-01-07 21:29:24,083] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:29:24,090] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:29:24,112] {logging_mixin.py:115} INFO - [2023-01-07 21:29:24,112] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:29:24,132] {logging_mixin.py:115} INFO - [2023-01-07 21:29:24,132] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:29:24,143] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.989 seconds
[2023-01-07 21:32:54,691] {processor.py:153} INFO - Started process (PID=33) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:32:54,702] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:32:54,703] {logging_mixin.py:115} INFO - [2023-01-07 21:32:54,702] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:32:58,242] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:32:58,244] {logging_mixin.py:115} INFO - [2023-01-07 21:32:58,244] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:32:58,244] {logging_mixin.py:115} INFO - [2023-01-07 21:32:58,244] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:32:58,256] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:32:58,289] {logging_mixin.py:115} INFO - [2023-01-07 21:32:58,288] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:32:58,317] {logging_mixin.py:115} INFO - [2023-01-07 21:32:58,317] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:32:58,331] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 3.645 seconds
[2023-01-07 21:33:28,431] {processor.py:153} INFO - Started process (PID=58) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:33:28,431] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:33:28,432] {logging_mixin.py:115} INFO - [2023-01-07 21:33:28,432] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:33:29,658] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:33:29,659] {logging_mixin.py:115} INFO - [2023-01-07 21:33:29,659] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:33:29,660] {logging_mixin.py:115} INFO - [2023-01-07 21:33:29,660] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:33:29,667] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:33:29,689] {logging_mixin.py:115} INFO - [2023-01-07 21:33:29,689] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:33:29,710] {logging_mixin.py:115} INFO - [2023-01-07 21:33:29,710] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:33:29,719] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.293 seconds
[2023-01-07 21:33:59,801] {processor.py:153} INFO - Started process (PID=86) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:33:59,802] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:33:59,802] {logging_mixin.py:115} INFO - [2023-01-07 21:33:59,802] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:34:01,011] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:34:01,012] {logging_mixin.py:115} INFO - [2023-01-07 21:34:01,012] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:34:01,013] {logging_mixin.py:115} INFO - [2023-01-07 21:34:01,012] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:34:01,020] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:34:01,043] {logging_mixin.py:115} INFO - [2023-01-07 21:34:01,043] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:34:01,064] {logging_mixin.py:115} INFO - [2023-01-07 21:34:01,064] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:34:01,075] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.280 seconds
[2023-01-07 21:34:31,150] {processor.py:153} INFO - Started process (PID=103) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:34:31,152] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:34:31,152] {logging_mixin.py:115} INFO - [2023-01-07 21:34:31,152] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:34:32,049] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:34:32,050] {logging_mixin.py:115} INFO - [2023-01-07 21:34:32,050] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:34:32,051] {logging_mixin.py:115} INFO - [2023-01-07 21:34:32,051] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:34:32,058] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:34:32,081] {logging_mixin.py:115} INFO - [2023-01-07 21:34:32,081] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:34:32,102] {logging_mixin.py:115} INFO - [2023-01-07 21:34:32,102] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:34:32,111] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 21:35:02,181] {processor.py:153} INFO - Started process (PID=128) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:02,185] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:35:02,186] {logging_mixin.py:115} INFO - [2023-01-07 21:35:02,185] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:03,279] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:35:03,281] {logging_mixin.py:115} INFO - [2023-01-07 21:35:03,281] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:35:03,281] {logging_mixin.py:115} INFO - [2023-01-07 21:35:03,281] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:35:03,288] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:03,312] {logging_mixin.py:115} INFO - [2023-01-07 21:35:03,311] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:35:03,332] {logging_mixin.py:115} INFO - [2023-01-07 21:35:03,332] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:35:03,341] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.165 seconds
[2023-01-07 21:35:33,413] {processor.py:153} INFO - Started process (PID=153) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:33,414] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:35:33,415] {logging_mixin.py:115} INFO - [2023-01-07 21:35:33,415] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:34,335] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:35:34,336] {logging_mixin.py:115} INFO - [2023-01-07 21:35:34,336] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:35:34,337] {logging_mixin.py:115} INFO - [2023-01-07 21:35:34,336] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:35:34,343] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:35:34,366] {logging_mixin.py:115} INFO - [2023-01-07 21:35:34,366] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:35:34,387] {logging_mixin.py:115} INFO - [2023-01-07 21:35:34,386] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:35:34,396] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.987 seconds
[2023-01-07 21:36:04,487] {processor.py:153} INFO - Started process (PID=176) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:04,492] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:36:04,492] {logging_mixin.py:115} INFO - [2023-01-07 21:36:04,492] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:05,832] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:36:05,833] {logging_mixin.py:115} INFO - [2023-01-07 21:36:05,833] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:36:05,834] {logging_mixin.py:115} INFO - [2023-01-07 21:36:05,833] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:36:05,841] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:05,865] {logging_mixin.py:115} INFO - [2023-01-07 21:36:05,864] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:36:05,886] {logging_mixin.py:115} INFO - [2023-01-07 21:36:05,886] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:36:05,900] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.419 seconds
[2023-01-07 21:36:35,975] {processor.py:153} INFO - Started process (PID=196) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:35,977] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:36:35,978] {logging_mixin.py:115} INFO - [2023-01-07 21:36:35,978] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:37,195] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:36:37,196] {logging_mixin.py:115} INFO - [2023-01-07 21:36:37,196] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:36:37,197] {logging_mixin.py:115} INFO - [2023-01-07 21:36:37,196] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:36:37,203] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:36:37,228] {logging_mixin.py:115} INFO - [2023-01-07 21:36:37,227] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:36:37,251] {logging_mixin.py:115} INFO - [2023-01-07 21:36:37,251] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:36:37,260] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.291 seconds
[2023-01-07 21:37:07,329] {processor.py:153} INFO - Started process (PID=221) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:07,330] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:37:07,331] {logging_mixin.py:115} INFO - [2023-01-07 21:37:07,331] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:08,248] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:37:08,249] {logging_mixin.py:115} INFO - [2023-01-07 21:37:08,249] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:37:08,249] {logging_mixin.py:115} INFO - [2023-01-07 21:37:08,249] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:37:08,256] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:08,279] {logging_mixin.py:115} INFO - [2023-01-07 21:37:08,279] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:37:08,301] {logging_mixin.py:115} INFO - [2023-01-07 21:37:08,301] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:37:08,310] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 21:37:38,397] {processor.py:153} INFO - Started process (PID=243) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:38,398] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:37:38,399] {logging_mixin.py:115} INFO - [2023-01-07 21:37:38,399] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:39,299] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:37:39,301] {logging_mixin.py:115} INFO - [2023-01-07 21:37:39,300] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:37:39,301] {logging_mixin.py:115} INFO - [2023-01-07 21:37:39,301] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:37:39,308] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:37:39,333] {logging_mixin.py:115} INFO - [2023-01-07 21:37:39,332] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:37:39,356] {logging_mixin.py:115} INFO - [2023-01-07 21:37:39,356] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:37:39,366] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.973 seconds
[2023-01-07 21:38:09,435] {processor.py:153} INFO - Started process (PID=264) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:09,436] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:38:09,437] {logging_mixin.py:115} INFO - [2023-01-07 21:38:09,437] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:10,971] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:38:10,973] {logging_mixin.py:115} INFO - [2023-01-07 21:38:10,973] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:38:10,974] {logging_mixin.py:115} INFO - [2023-01-07 21:38:10,973] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:38:10,985] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:11,021] {logging_mixin.py:115} INFO - [2023-01-07 21:38:11,021] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:38:11,052] {logging_mixin.py:115} INFO - [2023-01-07 21:38:11,051] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:38:11,064] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.634 seconds
[2023-01-07 21:38:13,866] {processor.py:153} INFO - Started process (PID=275) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:13,866] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:38:13,867] {logging_mixin.py:115} INFO - [2023-01-07 21:38:13,867] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:14,757] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:38:14,758] {logging_mixin.py:115} INFO - [2023-01-07 21:38:14,758] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:38:14,759] {logging_mixin.py:115} INFO - [2023-01-07 21:38:14,758] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:38:14,766] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:14,821] {logging_mixin.py:115} INFO - [2023-01-07 21:38:14,821] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:38:14,840] {logging_mixin.py:115} INFO - [2023-01-07 21:38:14,840] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:38:14,853] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.992 seconds
[2023-01-07 21:38:44,936] {processor.py:153} INFO - Started process (PID=294) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:44,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:38:44,938] {logging_mixin.py:115} INFO - [2023-01-07 21:38:44,938] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:46,092] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:38:46,093] {logging_mixin.py:115} INFO - [2023-01-07 21:38:46,093] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:38:46,094] {logging_mixin.py:115} INFO - [2023-01-07 21:38:46,093] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:38:46,100] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:38:46,131] {logging_mixin.py:115} INFO - [2023-01-07 21:38:46,131] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:38:46,162] {logging_mixin.py:115} INFO - [2023-01-07 21:38:46,161] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:38:46,174] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.243 seconds
[2023-01-07 21:39:16,252] {processor.py:153} INFO - Started process (PID=319) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:16,252] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:39:16,253] {logging_mixin.py:115} INFO - [2023-01-07 21:39:16,253] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:17,138] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:39:17,139] {logging_mixin.py:115} INFO - [2023-01-07 21:39:17,139] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:39:17,140] {logging_mixin.py:115} INFO - [2023-01-07 21:39:17,139] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:39:17,148] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:17,178] {logging_mixin.py:115} INFO - [2023-01-07 21:39:17,178] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:39:17,207] {logging_mixin.py:115} INFO - [2023-01-07 21:39:17,207] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:39:17,220] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.973 seconds
[2023-01-07 21:39:48,004] {processor.py:153} INFO - Started process (PID=344) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:48,005] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:39:48,006] {logging_mixin.py:115} INFO - [2023-01-07 21:39:48,006] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:48,917] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:39:48,919] {logging_mixin.py:115} INFO - [2023-01-07 21:39:48,919] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:39:48,920] {logging_mixin.py:115} INFO - [2023-01-07 21:39:48,919] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:39:48,931] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:39:48,957] {logging_mixin.py:115} INFO - [2023-01-07 21:39:48,957] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:39:48,978] {logging_mixin.py:115} INFO - [2023-01-07 21:39:48,978] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:39:48,987] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.988 seconds
[2023-01-07 21:40:19,064] {processor.py:153} INFO - Started process (PID=368) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:19,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:40:19,066] {logging_mixin.py:115} INFO - [2023-01-07 21:40:19,066] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:19,980] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:40:19,982] {logging_mixin.py:115} INFO - [2023-01-07 21:40:19,982] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:40:19,982] {logging_mixin.py:115} INFO - [2023-01-07 21:40:19,982] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:40:19,989] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:20,013] {logging_mixin.py:115} INFO - [2023-01-07 21:40:20,013] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:40:20,035] {logging_mixin.py:115} INFO - [2023-01-07 21:40:20,035] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:40:20,045] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 21:40:21,073] {processor.py:153} INFO - Started process (PID=370) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:21,074] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:40:21,074] {logging_mixin.py:115} INFO - [2023-01-07 21:40:21,074] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:21,999] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:40:22,000] {logging_mixin.py:115} INFO - [2023-01-07 21:40:22,000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:40:22,001] {logging_mixin.py:115} INFO - [2023-01-07 21:40:22,000] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:40:22,008] {logging_mixin.py:115} INFO - [2023-01-07 21:40:22,007] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:40:22,008] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:22,026] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 21:40:32,099] {processor.py:153} INFO - Started process (PID=381) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:32,100] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:40:32,101] {logging_mixin.py:115} INFO - [2023-01-07 21:40:32,100] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:33,078] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:40:33,079] {logging_mixin.py:115} INFO - [2023-01-07 21:40:33,079] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:40:33,080] {logging_mixin.py:115} INFO - [2023-01-07 21:40:33,079] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:40:33,087] {logging_mixin.py:115} INFO - [2023-01-07 21:40:33,087] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:40:33,088] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:40:33,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.014 seconds
[2023-01-07 21:41:03,185] {processor.py:153} INFO - Started process (PID=406) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:03,185] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:41:03,186] {logging_mixin.py:115} INFO - [2023-01-07 21:41:03,186] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:04,080] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:41:04,082] {logging_mixin.py:115} INFO - [2023-01-07 21:41:04,082] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:41:04,082] {logging_mixin.py:115} INFO - [2023-01-07 21:41:04,082] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:41:04,089] {logging_mixin.py:115} INFO - [2023-01-07 21:41:04,088] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:41:04,089] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:04,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-07 21:41:34,152] {processor.py:153} INFO - Started process (PID=425) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:34,154] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:41:34,154] {logging_mixin.py:115} INFO - [2023-01-07 21:41:34,154] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:35,050] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:41:35,051] {logging_mixin.py:115} INFO - [2023-01-07 21:41:35,051] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:41:35,051] {logging_mixin.py:115} INFO - [2023-01-07 21:41:35,051] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:41:35,058] {logging_mixin.py:115} INFO - [2023-01-07 21:41:35,058] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:41:35,059] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:41:35,076] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-07 21:42:05,159] {processor.py:153} INFO - Started process (PID=450) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:05,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:42:05,161] {logging_mixin.py:115} INFO - [2023-01-07 21:42:05,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:06,108] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:42:06,109] {logging_mixin.py:115} INFO - [2023-01-07 21:42:06,109] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:42:06,110] {logging_mixin.py:115} INFO - [2023-01-07 21:42:06,110] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:42:06,117] {logging_mixin.py:115} INFO - [2023-01-07 21:42:06,117] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:42:06,118] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:06,141] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.989 seconds
[2023-01-07 21:42:36,239] {processor.py:153} INFO - Started process (PID=475) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:36,239] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:42:36,240] {logging_mixin.py:115} INFO - [2023-01-07 21:42:36,240] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:37,174] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:42:37,175] {logging_mixin.py:115} INFO - [2023-01-07 21:42:37,175] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:42:37,175] {logging_mixin.py:115} INFO - [2023-01-07 21:42:37,175] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:42:37,182] {logging_mixin.py:115} INFO - [2023-01-07 21:42:37,182] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 84, in <module>
    trigger_dbt_cloud_job_run = DbtCloudRunJobOperator(
NameError: name 'DbtCloudRunJobOperator' is not defined
[2023-01-07 21:42:37,183] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:37,200] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 21:42:55,294] {processor.py:153} INFO - Started process (PID=491) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:55,295] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:42:55,296] {logging_mixin.py:115} INFO - [2023-01-07 21:42:55,296] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:56,323] {logging_mixin.py:115} INFO - [2023-01-07 21:42:56,322] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:42:56,324] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:42:56,346] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.058 seconds
[2023-01-07 21:43:26,422] {processor.py:153} INFO - Started process (PID=509) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:26,423] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:43:26,423] {logging_mixin.py:115} INFO - [2023-01-07 21:43:26,423] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:27,346] {logging_mixin.py:115} INFO - [2023-01-07 21:43:27,346] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:43:27,347] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:27,365] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.949 seconds
[2023-01-07 21:43:57,441] {processor.py:153} INFO - Started process (PID=534) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:57,442] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:43:57,443] {logging_mixin.py:115} INFO - [2023-01-07 21:43:57,443] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:58,328] {logging_mixin.py:115} INFO - [2023-01-07 21:43:58,327] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:43:58,328] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:43:58,347] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-07 21:44:28,417] {processor.py:153} INFO - Started process (PID=559) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:44:28,418] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:44:28,418] {logging_mixin.py:115} INFO - [2023-01-07 21:44:28,418] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:44:29,312] {logging_mixin.py:115} INFO - [2023-01-07 21:44:29,312] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:44:29,313] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:44:29,342] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 21:44:59,419] {processor.py:153} INFO - Started process (PID=577) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:44:59,420] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:44:59,420] {logging_mixin.py:115} INFO - [2023-01-07 21:44:59,420] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:45:00,346] {logging_mixin.py:115} INFO - [2023-01-07 21:45:00,344] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:45:00,346] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:45:00,369] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.955 seconds
[2023-01-07 21:45:30,439] {processor.py:153} INFO - Started process (PID=601) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:45:30,440] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:45:30,440] {logging_mixin.py:115} INFO - [2023-01-07 21:45:30,440] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:45:31,336] {logging_mixin.py:115} INFO - [2023-01-07 21:45:31,336] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:45:31,337] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:45:31,354] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-07 21:46:01,431] {processor.py:153} INFO - Started process (PID=626) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:01,434] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:46:01,435] {logging_mixin.py:115} INFO - [2023-01-07 21:46:01,435] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:02,336] {logging_mixin.py:115} INFO - [2023-01-07 21:46:02,335] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:46:02,336] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:02,353] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.927 seconds
[2023-01-07 21:46:32,426] {processor.py:153} INFO - Started process (PID=651) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:32,427] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:46:32,427] {logging_mixin.py:115} INFO - [2023-01-07 21:46:32,427] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:33,320] {logging_mixin.py:115} INFO - [2023-01-07 21:46:33,320] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:46:33,321] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:46:33,339] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 21:47:03,426] {processor.py:153} INFO - Started process (PID=669) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:03,428] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:47:03,428] {logging_mixin.py:115} INFO - [2023-01-07 21:47:03,428] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:04,308] {logging_mixin.py:115} INFO - [2023-01-07 21:47:04,307] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:47:04,309] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:04,326] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.905 seconds
[2023-01-07 21:47:34,415] {processor.py:153} INFO - Started process (PID=694) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:34,416] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:47:34,416] {logging_mixin.py:115} INFO - [2023-01-07 21:47:34,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:35,311] {logging_mixin.py:115} INFO - [2023-01-07 21:47:35,310] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:47:35,311] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:47:35,328] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-07 21:48:05,438] {processor.py:153} INFO - Started process (PID=719) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:05,440] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:48:05,440] {logging_mixin.py:115} INFO - [2023-01-07 21:48:05,440] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:06,330] {logging_mixin.py:115} INFO - [2023-01-07 21:48:06,329] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:48:06,330] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:06,348] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-07 21:48:36,446] {processor.py:153} INFO - Started process (PID=746) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:36,447] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:48:36,447] {logging_mixin.py:115} INFO - [2023-01-07 21:48:36,447] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:37,601] {logging_mixin.py:115} INFO - [2023-01-07 21:48:37,600] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:48:37,601] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:48:37,622] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.182 seconds
[2023-01-07 21:49:07,696] {processor.py:153} INFO - Started process (PID=764) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:07,698] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:49:07,699] {logging_mixin.py:115} INFO - [2023-01-07 21:49:07,699] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:08,584] {logging_mixin.py:115} INFO - [2023-01-07 21:49:08,583] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:49:08,584] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:08,601] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.910 seconds
[2023-01-07 21:49:38,669] {processor.py:153} INFO - Started process (PID=789) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:38,671] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:49:38,671] {logging_mixin.py:115} INFO - [2023-01-07 21:49:38,671] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:39,556] {logging_mixin.py:115} INFO - [2023-01-07 21:49:39,555] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:49:39,556] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:49:39,574] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.909 seconds
[2023-01-07 21:50:09,643] {processor.py:153} INFO - Started process (PID=815) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:09,644] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:50:09,645] {logging_mixin.py:115} INFO - [2023-01-07 21:50:09,645] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:10,538] {logging_mixin.py:115} INFO - [2023-01-07 21:50:10,537] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:50:10,538] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:10,555] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.917 seconds
[2023-01-07 21:50:40,626] {processor.py:153} INFO - Started process (PID=840) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:40,627] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:50:40,628] {logging_mixin.py:115} INFO - [2023-01-07 21:50:40,628] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:41,782] {logging_mixin.py:115} INFO - [2023-01-07 21:50:41,775] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:50:41,785] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:50:41,820] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.198 seconds
[2023-01-07 21:51:11,899] {processor.py:153} INFO - Started process (PID=859) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:11,899] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:51:11,900] {logging_mixin.py:115} INFO - [2023-01-07 21:51:11,900] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:12,766] {logging_mixin.py:115} INFO - [2023-01-07 21:51:12,765] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:51:12,766] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:12,784] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-07 21:51:42,855] {processor.py:153} INFO - Started process (PID=885) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:42,856] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:51:42,857] {logging_mixin.py:115} INFO - [2023-01-07 21:51:42,857] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:43,734] {logging_mixin.py:115} INFO - [2023-01-07 21:51:43,733] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:51:43,734] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:51:43,752] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.902 seconds
[2023-01-07 21:52:13,828] {processor.py:153} INFO - Started process (PID=910) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:13,828] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:52:13,829] {logging_mixin.py:115} INFO - [2023-01-07 21:52:13,829] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:14,708] {logging_mixin.py:115} INFO - [2023-01-07 21:52:14,707] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:52:14,708] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:14,727] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.904 seconds
[2023-01-07 21:52:44,807] {processor.py:153} INFO - Started process (PID=929) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:44,809] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:52:44,809] {logging_mixin.py:115} INFO - [2023-01-07 21:52:44,809] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:45,706] {logging_mixin.py:115} INFO - [2023-01-07 21:52:45,705] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:52:45,707] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:52:45,732] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.930 seconds
[2023-01-07 21:53:15,829] {processor.py:153} INFO - Started process (PID=954) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:15,830] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:53:15,830] {logging_mixin.py:115} INFO - [2023-01-07 21:53:15,830] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:16,744] {logging_mixin.py:115} INFO - [2023-01-07 21:53:16,743] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:53:16,744] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:16,763] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-07 21:53:46,854] {processor.py:153} INFO - Started process (PID=979) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:46,855] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:53:46,855] {logging_mixin.py:115} INFO - [2023-01-07 21:53:46,855] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:47,752] {logging_mixin.py:115} INFO - [2023-01-07 21:53:47,751] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:53:47,753] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:53:47,770] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-07 21:54:17,870] {processor.py:153} INFO - Started process (PID=1004) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:17,871] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:54:17,872] {logging_mixin.py:115} INFO - [2023-01-07 21:54:17,872] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:18,813] {logging_mixin.py:115} INFO - [2023-01-07 21:54:18,812] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:54:18,813] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:18,831] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 21:54:48,935] {processor.py:153} INFO - Started process (PID=1022) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:48,936] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:54:48,936] {logging_mixin.py:115} INFO - [2023-01-07 21:54:48,936] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:49,878] {logging_mixin.py:115} INFO - [2023-01-07 21:54:49,877] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:54:49,878] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:54:49,896] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 21:55:19,947] {processor.py:153} INFO - Started process (PID=1047) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:19,951] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:55:19,952] {logging_mixin.py:115} INFO - [2023-01-07 21:55:19,952] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:20,856] {logging_mixin.py:115} INFO - [2023-01-07 21:55:20,855] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:55:20,856] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:20,874] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-07 21:55:50,980] {processor.py:153} INFO - Started process (PID=1072) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:50,981] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:55:50,982] {logging_mixin.py:115} INFO - [2023-01-07 21:55:50,982] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:51,890] {logging_mixin.py:115} INFO - [2023-01-07 21:55:51,889] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 12, in <module>
    from airflow.providers.dbt.cloud import DbtCloudRunJobOperator
ImportError: cannot import name 'DbtCloudRunJobOperator' from 'airflow.providers.dbt.cloud' (/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/dbt/cloud/__init__.py)
[2023-01-07 21:55:51,890] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:51,908] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-07 21:55:59,822] {processor.py:153} INFO - Started process (PID=1083) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:55:59,823] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:55:59,823] {logging_mixin.py:115} INFO - [2023-01-07 21:55:59,823] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:56:00,790] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:56:00,792] {logging_mixin.py:115} INFO - [2023-01-07 21:56:00,792] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:56:00,792] {logging_mixin.py:115} INFO - [2023-01-07 21:56:00,792] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:56:00,800] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:56:00,857] {logging_mixin.py:115} INFO - [2023-01-07 21:56:00,856] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:56:00,878] {logging_mixin.py:115} INFO - [2023-01-07 21:56:00,878] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:56:00,892] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.075 seconds
[2023-01-07 21:56:30,960] {processor.py:153} INFO - Started process (PID=1100) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:56:30,961] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:56:30,962] {logging_mixin.py:115} INFO - [2023-01-07 21:56:30,962] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:56:31,928] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:56:31,929] {logging_mixin.py:115} INFO - [2023-01-07 21:56:31,929] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:56:31,930] {logging_mixin.py:115} INFO - [2023-01-07 21:56:31,929] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:56:31,937] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:56:31,961] {logging_mixin.py:115} INFO - [2023-01-07 21:56:31,961] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:56:31,982] {logging_mixin.py:115} INFO - [2023-01-07 21:56:31,982] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:56:31,993] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.038 seconds
[2023-01-07 21:57:02,060] {processor.py:153} INFO - Started process (PID=1125) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:02,061] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:57:02,061] {logging_mixin.py:115} INFO - [2023-01-07 21:57:02,061] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:02,964] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:57:02,966] {logging_mixin.py:115} INFO - [2023-01-07 21:57:02,965] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:57:02,966] {logging_mixin.py:115} INFO - [2023-01-07 21:57:02,966] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:57:02,973] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:02,996] {logging_mixin.py:115} INFO - [2023-01-07 21:57:02,996] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:57:03,018] {logging_mixin.py:115} INFO - [2023-01-07 21:57:03,018] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:57:03,027] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 21:57:33,099] {processor.py:153} INFO - Started process (PID=1150) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:33,100] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:57:33,101] {logging_mixin.py:115} INFO - [2023-01-07 21:57:33,101] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:34,001] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:57:34,002] {logging_mixin.py:115} INFO - [2023-01-07 21:57:34,002] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:57:34,003] {logging_mixin.py:115} INFO - [2023-01-07 21:57:34,002] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:57:34,010] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:57:34,034] {logging_mixin.py:115} INFO - [2023-01-07 21:57:34,034] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:57:34,055] {logging_mixin.py:115} INFO - [2023-01-07 21:57:34,055] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:57:34,065] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 21:58:04,137] {processor.py:153} INFO - Started process (PID=1175) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:04,141] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:58:04,142] {logging_mixin.py:115} INFO - [2023-01-07 21:58:04,142] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:05,130] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:58:05,131] {logging_mixin.py:115} INFO - [2023-01-07 21:58:05,131] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:58:05,132] {logging_mixin.py:115} INFO - [2023-01-07 21:58:05,131] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:58:05,139] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:05,163] {logging_mixin.py:115} INFO - [2023-01-07 21:58:05,162] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:58:05,183] {logging_mixin.py:115} INFO - [2023-01-07 21:58:05,183] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:58:05,192] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.060 seconds
[2023-01-07 21:58:35,260] {processor.py:153} INFO - Started process (PID=1194) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:35,261] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:58:35,261] {logging_mixin.py:115} INFO - [2023-01-07 21:58:35,261] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:36,173] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:58:36,174] {logging_mixin.py:115} INFO - [2023-01-07 21:58:36,174] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:58:36,175] {logging_mixin.py:115} INFO - [2023-01-07 21:58:36,174] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:58:36,182] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:58:36,205] {logging_mixin.py:115} INFO - [2023-01-07 21:58:36,204] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:58:36,226] {logging_mixin.py:115} INFO - [2023-01-07 21:58:36,226] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:58:36,236] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.981 seconds
[2023-01-07 21:59:06,309] {processor.py:153} INFO - Started process (PID=1220) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:06,311] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:59:06,312] {logging_mixin.py:115} INFO - [2023-01-07 21:59:06,312] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:07,212] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:59:07,214] {logging_mixin.py:115} INFO - [2023-01-07 21:59:07,214] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:59:07,214] {logging_mixin.py:115} INFO - [2023-01-07 21:59:07,214] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:59:07,221] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:07,245] {logging_mixin.py:115} INFO - [2023-01-07 21:59:07,244] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:59:07,266] {logging_mixin.py:115} INFO - [2023-01-07 21:59:07,266] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:59:07,275] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 21:59:37,347] {processor.py:153} INFO - Started process (PID=1245) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:37,349] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 21:59:37,350] {logging_mixin.py:115} INFO - [2023-01-07 21:59:37,350] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:38,246] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 21:59:38,247] {logging_mixin.py:115} INFO - [2023-01-07 21:59:38,247] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 21:59:38,248] {logging_mixin.py:115} INFO - [2023-01-07 21:59:38,248] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 21:59:38,255] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 21:59:38,278] {logging_mixin.py:115} INFO - [2023-01-07 21:59:38,278] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 21:59:38,299] {logging_mixin.py:115} INFO - [2023-01-07 21:59:38,299] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 21:59:38,309] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 22:00:08,382] {processor.py:153} INFO - Started process (PID=1270) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:08,383] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:00:08,383] {logging_mixin.py:115} INFO - [2023-01-07 22:00:08,383] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:09,407] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:00:09,408] {logging_mixin.py:115} INFO - [2023-01-07 22:00:09,408] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:00:09,409] {logging_mixin.py:115} INFO - [2023-01-07 22:00:09,409] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:00:09,421] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:09,454] {logging_mixin.py:115} INFO - [2023-01-07 22:00:09,453] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:00:09,479] {logging_mixin.py:115} INFO - [2023-01-07 22:00:09,479] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:00:09,489] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.112 seconds
[2023-01-07 22:00:39,569] {processor.py:153} INFO - Started process (PID=1288) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:39,570] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:00:39,571] {logging_mixin.py:115} INFO - [2023-01-07 22:00:39,571] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:40,465] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:00:40,467] {logging_mixin.py:115} INFO - [2023-01-07 22:00:40,467] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:00:40,467] {logging_mixin.py:115} INFO - [2023-01-07 22:00:40,467] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:00:40,474] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:00:40,500] {logging_mixin.py:115} INFO - [2023-01-07 22:00:40,499] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:00:40,521] {logging_mixin.py:115} INFO - [2023-01-07 22:00:40,521] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:00:40,532] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-07 22:01:11,429] {processor.py:153} INFO - Started process (PID=1314) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:11,429] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:01:11,430] {logging_mixin.py:115} INFO - [2023-01-07 22:01:11,430] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:12,338] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:01:12,339] {logging_mixin.py:115} INFO - [2023-01-07 22:01:12,339] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:01:12,340] {logging_mixin.py:115} INFO - [2023-01-07 22:01:12,339] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:01:12,347] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:12,370] {logging_mixin.py:115} INFO - [2023-01-07 22:01:12,369] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:01:12,391] {logging_mixin.py:115} INFO - [2023-01-07 22:01:12,391] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:01:12,400] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.976 seconds
[2023-01-07 22:01:42,485] {processor.py:153} INFO - Started process (PID=1340) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:42,487] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:01:42,487] {logging_mixin.py:115} INFO - [2023-01-07 22:01:42,487] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:43,400] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:01:43,401] {logging_mixin.py:115} INFO - [2023-01-07 22:01:43,401] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:01:43,402] {logging_mixin.py:115} INFO - [2023-01-07 22:01:43,402] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:01:43,409] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:01:43,433] {logging_mixin.py:115} INFO - [2023-01-07 22:01:43,432] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:01:43,454] {logging_mixin.py:115} INFO - [2023-01-07 22:01:43,454] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:01:43,463] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.983 seconds
[2023-01-07 22:02:13,549] {processor.py:153} INFO - Started process (PID=1364) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:13,550] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:02:13,550] {logging_mixin.py:115} INFO - [2023-01-07 22:02:13,550] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:14,753] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:02:14,755] {logging_mixin.py:115} INFO - [2023-01-07 22:02:14,755] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:02:14,756] {logging_mixin.py:115} INFO - [2023-01-07 22:02:14,755] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:02:14,763] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:14,787] {logging_mixin.py:115} INFO - [2023-01-07 22:02:14,786] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:02:14,807] {logging_mixin.py:115} INFO - [2023-01-07 22:02:14,807] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:02:14,817] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.273 seconds
[2023-01-07 22:02:44,898] {processor.py:153} INFO - Started process (PID=1383) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:44,898] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:02:44,899] {logging_mixin.py:115} INFO - [2023-01-07 22:02:44,899] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:45,784] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:02:45,785] {logging_mixin.py:115} INFO - [2023-01-07 22:02:45,785] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:02:45,786] {logging_mixin.py:115} INFO - [2023-01-07 22:02:45,785] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:02:45,793] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:02:45,815] {logging_mixin.py:115} INFO - [2023-01-07 22:02:45,815] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:02:45,836] {logging_mixin.py:115} INFO - [2023-01-07 22:02:45,836] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:02:45,846] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.953 seconds
[2023-01-07 22:03:15,940] {processor.py:153} INFO - Started process (PID=1410) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:15,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:03:15,941] {logging_mixin.py:115} INFO - [2023-01-07 22:03:15,941] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:16,831] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:03:16,832] {logging_mixin.py:115} INFO - [2023-01-07 22:03:16,832] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:03:16,833] {logging_mixin.py:115} INFO - [2023-01-07 22:03:16,833] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:03:16,840] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:16,864] {logging_mixin.py:115} INFO - [2023-01-07 22:03:16,863] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:03:16,884] {logging_mixin.py:115} INFO - [2023-01-07 22:03:16,884] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:03:16,894] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.959 seconds
[2023-01-07 22:03:46,975] {processor.py:153} INFO - Started process (PID=1434) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:46,975] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:03:46,976] {logging_mixin.py:115} INFO - [2023-01-07 22:03:46,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:47,945] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:03:47,946] {logging_mixin.py:115} INFO - [2023-01-07 22:03:47,946] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:03:47,947] {logging_mixin.py:115} INFO - [2023-01-07 22:03:47,947] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:03:47,954] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:03:47,982] {logging_mixin.py:115} INFO - [2023-01-07 22:03:47,981] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:03:48,008] {logging_mixin.py:115} INFO - [2023-01-07 22:03:48,007] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:03:48,017] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.050 seconds
[2023-01-07 22:04:18,088] {processor.py:153} INFO - Started process (PID=1452) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:18,089] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:04:18,090] {logging_mixin.py:115} INFO - [2023-01-07 22:04:18,090] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:18,994] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:04:18,995] {logging_mixin.py:115} INFO - [2023-01-07 22:04:18,995] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:04:18,995] {logging_mixin.py:115} INFO - [2023-01-07 22:04:18,995] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:04:19,003] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:19,026] {logging_mixin.py:115} INFO - [2023-01-07 22:04:19,026] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:04:19,047] {logging_mixin.py:115} INFO - [2023-01-07 22:04:19,047] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:04:19,056] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-07 22:04:49,149] {processor.py:153} INFO - Started process (PID=1477) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:49,151] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:04:49,152] {logging_mixin.py:115} INFO - [2023-01-07 22:04:49,152] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:50,044] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:04:50,046] {logging_mixin.py:115} INFO - [2023-01-07 22:04:50,046] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:04:50,046] {logging_mixin.py:115} INFO - [2023-01-07 22:04:50,046] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:04:50,054] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:04:50,077] {logging_mixin.py:115} INFO - [2023-01-07 22:04:50,076] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:04:50,097] {logging_mixin.py:115} INFO - [2023-01-07 22:04:50,097] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:04:50,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.963 seconds
[2023-01-07 22:05:20,201] {processor.py:153} INFO - Started process (PID=1503) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:20,202] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:05:20,203] {logging_mixin.py:115} INFO - [2023-01-07 22:05:20,203] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:21,099] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:05:21,101] {logging_mixin.py:115} INFO - [2023-01-07 22:05:21,101] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:05:21,101] {logging_mixin.py:115} INFO - [2023-01-07 22:05:21,101] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:05:21,109] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:21,132] {logging_mixin.py:115} INFO - [2023-01-07 22:05:21,132] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:05:21,153] {logging_mixin.py:115} INFO - [2023-01-07 22:05:21,153] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:05:21,163] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 22:05:51,316] {processor.py:153} INFO - Started process (PID=1529) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:51,318] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:05:51,319] {logging_mixin.py:115} INFO - [2023-01-07 22:05:51,319] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:52,330] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:05:52,331] {logging_mixin.py:115} INFO - [2023-01-07 22:05:52,331] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:05:52,332] {logging_mixin.py:115} INFO - [2023-01-07 22:05:52,332] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:05:52,339] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:05:52,362] {logging_mixin.py:115} INFO - [2023-01-07 22:05:52,362] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:05:52,383] {logging_mixin.py:115} INFO - [2023-01-07 22:05:52,383] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:05:52,392] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.082 seconds
[2023-01-07 22:06:22,461] {processor.py:153} INFO - Started process (PID=1548) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:22,462] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:06:22,463] {logging_mixin.py:115} INFO - [2023-01-07 22:06:22,463] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:23,365] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:06:23,367] {logging_mixin.py:115} INFO - [2023-01-07 22:06:23,367] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:06:23,367] {logging_mixin.py:115} INFO - [2023-01-07 22:06:23,367] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:06:23,374] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:23,398] {logging_mixin.py:115} INFO - [2023-01-07 22:06:23,398] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:06:23,419] {logging_mixin.py:115} INFO - [2023-01-07 22:06:23,419] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:06:23,429] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.973 seconds
[2023-01-07 22:06:53,497] {processor.py:153} INFO - Started process (PID=1573) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:53,498] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:06:53,499] {logging_mixin.py:115} INFO - [2023-01-07 22:06:53,499] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:54,461] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:06:54,462] {logging_mixin.py:115} INFO - [2023-01-07 22:06:54,462] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:06:54,462] {logging_mixin.py:115} INFO - [2023-01-07 22:06:54,462] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:06:54,470] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:06:54,493] {logging_mixin.py:115} INFO - [2023-01-07 22:06:54,493] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:06:54,513] {logging_mixin.py:115} INFO - [2023-01-07 22:06:54,513] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:06:54,523] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.032 seconds
[2023-01-07 22:07:24,599] {processor.py:153} INFO - Started process (PID=1599) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:24,601] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:07:24,601] {logging_mixin.py:115} INFO - [2023-01-07 22:07:24,601] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:25,501] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:07:25,502] {logging_mixin.py:115} INFO - [2023-01-07 22:07:25,502] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:07:25,503] {logging_mixin.py:115} INFO - [2023-01-07 22:07:25,503] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:07:25,510] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:25,534] {logging_mixin.py:115} INFO - [2023-01-07 22:07:25,534] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:07:25,555] {logging_mixin.py:115} INFO - [2023-01-07 22:07:25,555] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:07:25,565] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 22:07:55,640] {processor.py:153} INFO - Started process (PID=1623) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:55,641] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:07:55,642] {logging_mixin.py:115} INFO - [2023-01-07 22:07:55,641] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:56,780] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:07:56,782] {logging_mixin.py:115} INFO - [2023-01-07 22:07:56,782] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:07:56,783] {logging_mixin.py:115} INFO - [2023-01-07 22:07:56,782] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:07:56,790] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:07:56,815] {logging_mixin.py:115} INFO - [2023-01-07 22:07:56,814] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:07:56,836] {logging_mixin.py:115} INFO - [2023-01-07 22:07:56,836] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:07:56,845] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.212 seconds
[2023-01-07 22:08:26,922] {processor.py:153} INFO - Started process (PID=1641) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:26,924] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:08:26,924] {logging_mixin.py:115} INFO - [2023-01-07 22:08:26,924] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:27,853] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:08:27,854] {logging_mixin.py:115} INFO - [2023-01-07 22:08:27,854] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:08:27,854] {logging_mixin.py:115} INFO - [2023-01-07 22:08:27,854] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:08:27,862] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:27,884] {logging_mixin.py:115} INFO - [2023-01-07 22:08:27,884] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:08:27,920] {logging_mixin.py:115} INFO - [2023-01-07 22:08:27,920] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:08:27,931] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.013 seconds
[2023-01-07 22:08:57,994] {processor.py:153} INFO - Started process (PID=1667) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:57,994] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:08:57,995] {logging_mixin.py:115} INFO - [2023-01-07 22:08:57,995] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:58,904] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:08:58,905] {logging_mixin.py:115} INFO - [2023-01-07 22:08:58,905] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:08:58,906] {logging_mixin.py:115} INFO - [2023-01-07 22:08:58,905] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:08:58,913] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:08:58,942] {logging_mixin.py:115} INFO - [2023-01-07 22:08:58,942] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:08:58,964] {logging_mixin.py:115} INFO - [2023-01-07 22:08:58,964] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:08:58,974] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 22:09:29,048] {processor.py:153} INFO - Started process (PID=1693) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:09:29,049] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:09:29,050] {logging_mixin.py:115} INFO - [2023-01-07 22:09:29,050] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:09:29,933] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:09:29,935] {logging_mixin.py:115} INFO - [2023-01-07 22:09:29,935] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:09:29,935] {logging_mixin.py:115} INFO - [2023-01-07 22:09:29,935] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:09:29,942] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:09:29,965] {logging_mixin.py:115} INFO - [2023-01-07 22:09:29,965] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:09:29,986] {logging_mixin.py:115} INFO - [2023-01-07 22:09:29,986] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:09:29,995] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 22:10:00,067] {processor.py:153} INFO - Started process (PID=1712) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:00,067] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:10:00,068] {logging_mixin.py:115} INFO - [2023-01-07 22:10:00,068] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:01,121] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:10:01,123] {logging_mixin.py:115} INFO - [2023-01-07 22:10:01,123] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:10:01,124] {logging_mixin.py:115} INFO - [2023-01-07 22:10:01,123] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:10:01,137] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:01,193] {logging_mixin.py:115} INFO - [2023-01-07 22:10:01,193] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:10:01,226] {logging_mixin.py:115} INFO - [2023-01-07 22:10:01,225] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:10:01,239] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.176 seconds
[2023-01-07 22:10:32,152] {processor.py:153} INFO - Started process (PID=1738) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:32,153] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:10:32,154] {logging_mixin.py:115} INFO - [2023-01-07 22:10:32,154] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:33,061] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:10:33,062] {logging_mixin.py:115} INFO - [2023-01-07 22:10:33,062] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:10:33,063] {logging_mixin.py:115} INFO - [2023-01-07 22:10:33,062] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:10:33,070] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:10:33,093] {logging_mixin.py:115} INFO - [2023-01-07 22:10:33,092] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:10:33,113] {logging_mixin.py:115} INFO - [2023-01-07 22:10:33,113] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:10:33,123] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.976 seconds
[2023-01-07 22:11:03,162] {processor.py:153} INFO - Started process (PID=1762) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:03,163] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:11:03,164] {logging_mixin.py:115} INFO - [2023-01-07 22:11:03,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:04,062] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:11:04,063] {logging_mixin.py:115} INFO - [2023-01-07 22:11:04,063] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:11:04,064] {logging_mixin.py:115} INFO - [2023-01-07 22:11:04,064] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:11:04,071] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:04,094] {logging_mixin.py:115} INFO - [2023-01-07 22:11:04,093] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:11:04,114] {logging_mixin.py:115} INFO - [2023-01-07 22:11:04,114] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:11:04,124] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 22:11:34,289] {processor.py:153} INFO - Started process (PID=1787) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:34,290] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:11:34,291] {logging_mixin.py:115} INFO - [2023-01-07 22:11:34,291] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:35,233] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:11:35,235] {logging_mixin.py:115} INFO - [2023-01-07 22:11:35,235] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:11:35,236] {logging_mixin.py:115} INFO - [2023-01-07 22:11:35,236] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:11:35,248] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:11:35,279] {logging_mixin.py:115} INFO - [2023-01-07 22:11:35,278] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:11:35,309] {logging_mixin.py:115} INFO - [2023-01-07 22:11:35,309] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:11:35,321] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.037 seconds
[2023-01-07 22:12:05,401] {processor.py:153} INFO - Started process (PID=1811) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:05,403] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:12:05,403] {logging_mixin.py:115} INFO - [2023-01-07 22:12:05,403] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:06,320] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:12:06,321] {logging_mixin.py:115} INFO - [2023-01-07 22:12:06,321] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:12:06,322] {logging_mixin.py:115} INFO - [2023-01-07 22:12:06,322] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:12:06,329] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:06,352] {logging_mixin.py:115} INFO - [2023-01-07 22:12:06,351] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:12:06,372] {logging_mixin.py:115} INFO - [2023-01-07 22:12:06,372] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:12:06,381] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 22:12:36,457] {processor.py:153} INFO - Started process (PID=1829) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:36,459] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:12:36,459] {logging_mixin.py:115} INFO - [2023-01-07 22:12:36,459] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:37,374] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:12:37,376] {logging_mixin.py:115} INFO - [2023-01-07 22:12:37,375] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:12:37,376] {logging_mixin.py:115} INFO - [2023-01-07 22:12:37,376] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:12:37,383] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:12:37,406] {logging_mixin.py:115} INFO - [2023-01-07 22:12:37,406] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:12:37,427] {logging_mixin.py:115} INFO - [2023-01-07 22:12:37,427] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:12:37,437] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 22:13:07,509] {processor.py:153} INFO - Started process (PID=1854) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:07,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:13:07,510] {logging_mixin.py:115} INFO - [2023-01-07 22:13:07,510] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:08,424] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:13:08,425] {logging_mixin.py:115} INFO - [2023-01-07 22:13:08,425] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:13:08,426] {logging_mixin.py:115} INFO - [2023-01-07 22:13:08,426] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:13:08,433] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:08,456] {logging_mixin.py:115} INFO - [2023-01-07 22:13:08,456] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:13:08,476] {logging_mixin.py:115} INFO - [2023-01-07 22:13:08,476] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:13:08,486] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.982 seconds
[2023-01-07 22:13:39,234] {processor.py:153} INFO - Started process (PID=1881) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:39,235] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:13:39,236] {logging_mixin.py:115} INFO - [2023-01-07 22:13:39,236] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:40,169] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:13:40,170] {logging_mixin.py:115} INFO - [2023-01-07 22:13:40,170] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:13:40,171] {logging_mixin.py:115} INFO - [2023-01-07 22:13:40,171] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:13:40,183] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:13:40,211] {logging_mixin.py:115} INFO - [2023-01-07 22:13:40,210] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:13:40,238] {logging_mixin.py:115} INFO - [2023-01-07 22:13:40,237] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:13:40,249] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.019 seconds
[2023-01-07 22:14:10,338] {processor.py:153} INFO - Started process (PID=1907) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:10,339] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:14:10,340] {logging_mixin.py:115} INFO - [2023-01-07 22:14:10,340] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:11,325] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:14:11,327] {logging_mixin.py:115} INFO - [2023-01-07 22:14:11,327] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:14:11,328] {logging_mixin.py:115} INFO - [2023-01-07 22:14:11,327] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:14:11,337] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:11,361] {logging_mixin.py:115} INFO - [2023-01-07 22:14:11,361] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:14:11,382] {logging_mixin.py:115} INFO - [2023-01-07 22:14:11,382] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:14:11,392] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.059 seconds
[2023-01-07 22:14:41,486] {processor.py:153} INFO - Started process (PID=1925) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:41,487] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:14:41,488] {logging_mixin.py:115} INFO - [2023-01-07 22:14:41,488] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:42,493] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:14:42,495] {logging_mixin.py:115} INFO - [2023-01-07 22:14:42,495] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:14:42,496] {logging_mixin.py:115} INFO - [2023-01-07 22:14:42,495] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:14:42,507] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:14:42,541] {logging_mixin.py:115} INFO - [2023-01-07 22:14:42,540] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:14:42,572] {logging_mixin.py:115} INFO - [2023-01-07 22:14:42,572] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:14:42,584] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.103 seconds
[2023-01-07 22:15:12,692] {processor.py:153} INFO - Started process (PID=1951) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:12,693] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:15:12,694] {logging_mixin.py:115} INFO - [2023-01-07 22:15:12,694] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:13,603] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:15:13,605] {logging_mixin.py:115} INFO - [2023-01-07 22:15:13,604] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:15:13,605] {logging_mixin.py:115} INFO - [2023-01-07 22:15:13,605] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:15:13,612] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:13,636] {logging_mixin.py:115} INFO - [2023-01-07 22:15:13,635] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:15:13,656] {logging_mixin.py:115} INFO - [2023-01-07 22:15:13,656] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:15:13,666] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 22:15:43,768] {processor.py:153} INFO - Started process (PID=1978) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:43,772] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:15:43,772] {logging_mixin.py:115} INFO - [2023-01-07 22:15:43,772] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:44,663] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:15:44,664] {logging_mixin.py:115} INFO - [2023-01-07 22:15:44,664] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:15:44,665] {logging_mixin.py:115} INFO - [2023-01-07 22:15:44,664] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:15:44,672] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:15:44,694] {logging_mixin.py:115} INFO - [2023-01-07 22:15:44,694] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:15:44,714] {logging_mixin.py:115} INFO - [2023-01-07 22:15:44,714] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:15:44,724] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 22:16:14,794] {processor.py:153} INFO - Started process (PID=2002) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:14,797] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:16:14,798] {logging_mixin.py:115} INFO - [2023-01-07 22:16:14,798] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:16,087] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:16:16,089] {logging_mixin.py:115} INFO - [2023-01-07 22:16:16,089] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:16:16,090] {logging_mixin.py:115} INFO - [2023-01-07 22:16:16,089] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:16:16,102] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:16,134] {logging_mixin.py:115} INFO - [2023-01-07 22:16:16,134] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:16:16,165] {logging_mixin.py:115} INFO - [2023-01-07 22:16:16,165] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:16:16,177] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.389 seconds
[2023-01-07 22:16:46,254] {processor.py:153} INFO - Started process (PID=2021) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:46,254] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:16:46,255] {logging_mixin.py:115} INFO - [2023-01-07 22:16:46,255] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:47,219] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:16:47,221] {logging_mixin.py:115} INFO - [2023-01-07 22:16:47,221] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:16:47,221] {logging_mixin.py:115} INFO - [2023-01-07 22:16:47,221] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:16:47,230] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:16:47,261] {logging_mixin.py:115} INFO - [2023-01-07 22:16:47,260] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:16:47,286] {logging_mixin.py:115} INFO - [2023-01-07 22:16:47,286] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:16:47,296] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.047 seconds
[2023-01-07 22:17:17,367] {processor.py:153} INFO - Started process (PID=2045) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:17,368] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:17:17,369] {logging_mixin.py:115} INFO - [2023-01-07 22:17:17,369] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:18,409] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:17:18,410] {logging_mixin.py:115} INFO - [2023-01-07 22:17:18,410] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:17:18,410] {logging_mixin.py:115} INFO - [2023-01-07 22:17:18,410] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:17:18,418] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:18,441] {logging_mixin.py:115} INFO - [2023-01-07 22:17:18,440] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:17:18,461] {logging_mixin.py:115} INFO - [2023-01-07 22:17:18,461] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:17:18,471] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.108 seconds
[2023-01-07 22:17:48,539] {processor.py:153} INFO - Started process (PID=2072) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:48,541] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:17:48,541] {logging_mixin.py:115} INFO - [2023-01-07 22:17:48,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:49,535] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:17:49,537] {logging_mixin.py:115} INFO - [2023-01-07 22:17:49,537] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:17:49,538] {logging_mixin.py:115} INFO - [2023-01-07 22:17:49,537] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:17:49,550] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:17:49,575] {logging_mixin.py:115} INFO - [2023-01-07 22:17:49,575] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:17:49,596] {logging_mixin.py:115} INFO - [2023-01-07 22:17:49,596] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:17:49,605] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.071 seconds
[2023-01-07 22:18:19,682] {processor.py:153} INFO - Started process (PID=2097) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:19,683] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:18:19,684] {logging_mixin.py:115} INFO - [2023-01-07 22:18:19,684] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:20,739] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:18:20,740] {logging_mixin.py:115} INFO - [2023-01-07 22:18:20,740] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:18:20,741] {logging_mixin.py:115} INFO - [2023-01-07 22:18:20,741] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:18:20,748] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:20,771] {logging_mixin.py:115} INFO - [2023-01-07 22:18:20,771] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:18:20,792] {logging_mixin.py:115} INFO - [2023-01-07 22:18:20,792] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:18:20,802] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.126 seconds
[2023-01-07 22:18:50,868] {processor.py:153} INFO - Started process (PID=2114) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:50,869] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:18:50,870] {logging_mixin.py:115} INFO - [2023-01-07 22:18:50,870] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:51,914] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:18:51,915] {logging_mixin.py:115} INFO - [2023-01-07 22:18:51,915] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:18:51,916] {logging_mixin.py:115} INFO - [2023-01-07 22:18:51,915] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:18:51,923] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:18:51,947] {logging_mixin.py:115} INFO - [2023-01-07 22:18:51,947] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:18:51,968] {logging_mixin.py:115} INFO - [2023-01-07 22:18:51,968] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:18:51,978] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.114 seconds
[2023-01-07 22:19:22,052] {processor.py:153} INFO - Started process (PID=2140) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:22,053] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:19:22,054] {logging_mixin.py:115} INFO - [2023-01-07 22:19:22,054] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:23,053] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:19:23,054] {logging_mixin.py:115} INFO - [2023-01-07 22:19:23,054] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:19:23,054] {logging_mixin.py:115} INFO - [2023-01-07 22:19:23,054] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:19:23,062] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:23,085] {logging_mixin.py:115} INFO - [2023-01-07 22:19:23,085] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:19:23,107] {logging_mixin.py:115} INFO - [2023-01-07 22:19:23,107] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:19:23,117] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.069 seconds
[2023-01-07 22:19:53,187] {processor.py:153} INFO - Started process (PID=2167) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:53,189] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:19:53,190] {logging_mixin.py:115} INFO - [2023-01-07 22:19:53,190] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:54,104] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:19:54,106] {logging_mixin.py:115} INFO - [2023-01-07 22:19:54,105] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:19:54,106] {logging_mixin.py:115} INFO - [2023-01-07 22:19:54,106] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:19:54,113] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:19:54,136] {logging_mixin.py:115} INFO - [2023-01-07 22:19:54,135] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:19:54,157] {logging_mixin.py:115} INFO - [2023-01-07 22:19:54,157] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:19:54,166] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 22:20:24,269] {processor.py:153} INFO - Started process (PID=2191) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:24,270] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:20:24,271] {logging_mixin.py:115} INFO - [2023-01-07 22:20:24,270] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:25,207] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:20:25,208] {logging_mixin.py:115} INFO - [2023-01-07 22:20:25,208] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:20:25,208] {logging_mixin.py:115} INFO - [2023-01-07 22:20:25,208] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:20:25,216] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:25,238] {logging_mixin.py:115} INFO - [2023-01-07 22:20:25,237] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:20:25,258] {logging_mixin.py:115} INFO - [2023-01-07 22:20:25,258] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:20:25,268] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.004 seconds
[2023-01-07 22:20:55,341] {processor.py:153} INFO - Started process (PID=2209) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:55,342] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:20:55,343] {logging_mixin.py:115} INFO - [2023-01-07 22:20:55,343] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:56,576] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:20:56,578] {logging_mixin.py:115} INFO - [2023-01-07 22:20:56,577] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:20:56,578] {logging_mixin.py:115} INFO - [2023-01-07 22:20:56,578] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:20:56,590] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:20:56,622] {logging_mixin.py:115} INFO - [2023-01-07 22:20:56,621] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:20:56,652] {logging_mixin.py:115} INFO - [2023-01-07 22:20:56,652] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:20:56,665] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.328 seconds
[2023-01-07 22:21:26,738] {processor.py:153} INFO - Started process (PID=2236) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:26,741] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:21:26,742] {logging_mixin.py:115} INFO - [2023-01-07 22:21:26,741] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:27,623] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:21:27,624] {logging_mixin.py:115} INFO - [2023-01-07 22:21:27,624] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:21:27,625] {logging_mixin.py:115} INFO - [2023-01-07 22:21:27,624] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:21:27,632] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:27,655] {logging_mixin.py:115} INFO - [2023-01-07 22:21:27,655] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:21:27,676] {logging_mixin.py:115} INFO - [2023-01-07 22:21:27,676] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:21:27,685] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.951 seconds
[2023-01-07 22:21:57,756] {processor.py:153} INFO - Started process (PID=2261) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:57,757] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:21:57,758] {logging_mixin.py:115} INFO - [2023-01-07 22:21:57,758] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:58,728] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:21:58,729] {logging_mixin.py:115} INFO - [2023-01-07 22:21:58,729] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:21:58,730] {logging_mixin.py:115} INFO - [2023-01-07 22:21:58,730] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:21:58,737] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:21:58,760] {logging_mixin.py:115} INFO - [2023-01-07 22:21:58,760] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:21:58,781] {logging_mixin.py:115} INFO - [2023-01-07 22:21:58,781] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:21:58,790] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.039 seconds
[2023-01-07 22:22:28,839] {processor.py:153} INFO - Started process (PID=2285) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:22:28,840] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:22:28,840] {logging_mixin.py:115} INFO - [2023-01-07 22:22:28,840] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:22:29,740] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:22:29,741] {logging_mixin.py:115} INFO - [2023-01-07 22:22:29,741] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:22:29,742] {logging_mixin.py:115} INFO - [2023-01-07 22:22:29,742] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:22:29,749] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:22:29,772] {logging_mixin.py:115} INFO - [2023-01-07 22:22:29,772] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:22:29,794] {logging_mixin.py:115} INFO - [2023-01-07 22:22:29,794] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:22:29,805] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-07 22:22:59,876] {processor.py:153} INFO - Started process (PID=2302) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:22:59,878] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:22:59,879] {logging_mixin.py:115} INFO - [2023-01-07 22:22:59,878] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:23:00,990] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:23:00,991] {logging_mixin.py:115} INFO - [2023-01-07 22:23:00,991] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:23:00,992] {logging_mixin.py:115} INFO - [2023-01-07 22:23:00,991] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:23:00,999] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:23:01,022] {logging_mixin.py:115} INFO - [2023-01-07 22:23:01,021] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:23:01,043] {logging_mixin.py:115} INFO - [2023-01-07 22:23:01,042] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:23:01,052] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.181 seconds
[2023-01-07 22:23:31,120] {processor.py:153} INFO - Started process (PID=2328) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:23:31,122] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:23:31,123] {logging_mixin.py:115} INFO - [2023-01-07 22:23:31,123] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:23:32,030] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:23:32,031] {logging_mixin.py:115} INFO - [2023-01-07 22:23:32,031] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:23:32,031] {logging_mixin.py:115} INFO - [2023-01-07 22:23:32,031] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:23:32,039] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:23:32,061] {logging_mixin.py:115} INFO - [2023-01-07 22:23:32,061] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:23:32,081] {logging_mixin.py:115} INFO - [2023-01-07 22:23:32,081] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:23:32,091] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.975 seconds
[2023-01-07 22:24:02,159] {processor.py:153} INFO - Started process (PID=2352) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:02,161] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:24:02,162] {logging_mixin.py:115} INFO - [2023-01-07 22:24:02,162] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:03,116] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:24:03,117] {logging_mixin.py:115} INFO - [2023-01-07 22:24:03,117] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:24:03,118] {logging_mixin.py:115} INFO - [2023-01-07 22:24:03,118] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:24:03,125] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:03,147] {logging_mixin.py:115} INFO - [2023-01-07 22:24:03,147] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:24:03,168] {logging_mixin.py:115} INFO - [2023-01-07 22:24:03,168] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:24:03,177] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.023 seconds
[2023-01-07 22:24:33,239] {processor.py:153} INFO - Started process (PID=2375) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:33,239] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:24:33,240] {logging_mixin.py:115} INFO - [2023-01-07 22:24:33,240] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:34,144] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:24:34,145] {logging_mixin.py:115} INFO - [2023-01-07 22:24:34,145] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:24:34,145] {logging_mixin.py:115} INFO - [2023-01-07 22:24:34,145] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:24:34,152] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:24:34,175] {logging_mixin.py:115} INFO - [2023-01-07 22:24:34,175] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:24:34,196] {logging_mixin.py:115} INFO - [2023-01-07 22:24:34,195] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:24:34,205] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-07 22:25:04,273] {processor.py:153} INFO - Started process (PID=2394) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:04,274] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:25:04,275] {logging_mixin.py:115} INFO - [2023-01-07 22:25:04,274] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:05,443] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:25:05,444] {logging_mixin.py:115} INFO - [2023-01-07 22:25:05,444] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:25:05,444] {logging_mixin.py:115} INFO - [2023-01-07 22:25:05,444] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:25:05,452] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:05,475] {logging_mixin.py:115} INFO - [2023-01-07 22:25:05,475] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:25:05,496] {logging_mixin.py:115} INFO - [2023-01-07 22:25:05,496] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:25:05,506] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.238 seconds
[2023-01-07 22:25:35,574] {processor.py:153} INFO - Started process (PID=2419) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:35,576] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:25:35,576] {logging_mixin.py:115} INFO - [2023-01-07 22:25:35,576] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:36,578] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:25:36,580] {logging_mixin.py:115} INFO - [2023-01-07 22:25:36,580] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:25:36,580] {logging_mixin.py:115} INFO - [2023-01-07 22:25:36,580] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:25:36,588] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:25:36,611] {logging_mixin.py:115} INFO - [2023-01-07 22:25:36,611] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:25:36,632] {logging_mixin.py:115} INFO - [2023-01-07 22:25:36,632] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:25:36,642] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.072 seconds
[2023-01-07 22:26:06,717] {processor.py:153} INFO - Started process (PID=2442) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:06,718] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:26:06,719] {logging_mixin.py:115} INFO - [2023-01-07 22:26:06,719] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:07,648] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:26:07,649] {logging_mixin.py:115} INFO - [2023-01-07 22:26:07,649] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:26:07,651] {logging_mixin.py:115} INFO - [2023-01-07 22:26:07,650] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:26:07,658] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:07,681] {logging_mixin.py:115} INFO - [2023-01-07 22:26:07,680] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:26:07,702] {logging_mixin.py:115} INFO - [2023-01-07 22:26:07,702] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:26:07,711] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.000 seconds
[2023-01-07 22:26:37,795] {processor.py:153} INFO - Started process (PID=2468) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:37,796] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:26:37,797] {logging_mixin.py:115} INFO - [2023-01-07 22:26:37,796] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:38,684] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:26:38,685] {logging_mixin.py:115} INFO - [2023-01-07 22:26:38,685] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:26:38,686] {logging_mixin.py:115} INFO - [2023-01-07 22:26:38,686] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:26:38,693] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:26:38,715] {logging_mixin.py:115} INFO - [2023-01-07 22:26:38,715] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:26:38,735] {logging_mixin.py:115} INFO - [2023-01-07 22:26:38,735] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:26:38,745] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.955 seconds
[2023-01-07 22:27:08,812] {processor.py:153} INFO - Started process (PID=2487) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:08,814] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:27:08,814] {logging_mixin.py:115} INFO - [2023-01-07 22:27:08,814] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:09,883] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:27:09,884] {logging_mixin.py:115} INFO - [2023-01-07 22:27:09,884] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:27:09,885] {logging_mixin.py:115} INFO - [2023-01-07 22:27:09,884] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:27:09,892] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:09,915] {logging_mixin.py:115} INFO - [2023-01-07 22:27:09,915] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:27:09,936] {logging_mixin.py:115} INFO - [2023-01-07 22:27:09,936] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:27:09,946] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.138 seconds
[2023-01-07 22:27:40,017] {processor.py:153} INFO - Started process (PID=2514) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:40,018] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:27:40,019] {logging_mixin.py:115} INFO - [2023-01-07 22:27:40,019] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:40,916] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:27:40,918] {logging_mixin.py:115} INFO - [2023-01-07 22:27:40,918] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:27:40,918] {logging_mixin.py:115} INFO - [2023-01-07 22:27:40,918] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:27:40,925] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:27:40,948] {logging_mixin.py:115} INFO - [2023-01-07 22:27:40,948] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:27:40,970] {logging_mixin.py:115} INFO - [2023-01-07 22:27:40,969] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:27:40,979] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 22:28:11,045] {processor.py:153} INFO - Started process (PID=2540) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:11,046] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:28:11,047] {logging_mixin.py:115} INFO - [2023-01-07 22:28:11,047] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:11,954] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:28:11,955] {logging_mixin.py:115} INFO - [2023-01-07 22:28:11,955] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:28:11,956] {logging_mixin.py:115} INFO - [2023-01-07 22:28:11,956] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:28:11,963] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:11,985] {logging_mixin.py:115} INFO - [2023-01-07 22:28:11,985] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:28:12,006] {logging_mixin.py:115} INFO - [2023-01-07 22:28:12,005] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:28:12,015] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-07 22:28:42,091] {processor.py:153} INFO - Started process (PID=2565) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:42,092] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:28:42,093] {logging_mixin.py:115} INFO - [2023-01-07 22:28:42,093] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:43,026] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:28:43,027] {logging_mixin.py:115} INFO - [2023-01-07 22:28:43,027] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:28:43,027] {logging_mixin.py:115} INFO - [2023-01-07 22:28:43,027] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:28:43,035] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:28:43,057] {logging_mixin.py:115} INFO - [2023-01-07 22:28:43,056] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:28:43,077] {logging_mixin.py:115} INFO - [2023-01-07 22:28:43,077] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:28:43,087] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.001 seconds
[2023-01-07 22:29:13,165] {processor.py:153} INFO - Started process (PID=2581) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:13,165] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:29:13,166] {logging_mixin.py:115} INFO - [2023-01-07 22:29:13,166] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:14,100] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:29:14,101] {logging_mixin.py:115} INFO - [2023-01-07 22:29:14,101] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:29:14,102] {logging_mixin.py:115} INFO - [2023-01-07 22:29:14,101] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:29:14,109] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:14,131] {logging_mixin.py:115} INFO - [2023-01-07 22:29:14,131] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:29:14,155] {logging_mixin.py:115} INFO - [2023-01-07 22:29:14,155] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:29:14,167] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.007 seconds
[2023-01-07 22:29:44,273] {processor.py:153} INFO - Started process (PID=2606) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:44,274] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:29:44,274] {logging_mixin.py:115} INFO - [2023-01-07 22:29:44,274] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:45,170] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:29:45,171] {logging_mixin.py:115} INFO - [2023-01-07 22:29:45,171] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:29:45,171] {logging_mixin.py:115} INFO - [2023-01-07 22:29:45,171] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:29:45,178] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:29:45,201] {logging_mixin.py:115} INFO - [2023-01-07 22:29:45,201] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:29:45,221] {logging_mixin.py:115} INFO - [2023-01-07 22:29:45,221] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:29:45,231] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 22:30:15,296] {processor.py:153} INFO - Started process (PID=2631) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:15,297] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:30:15,298] {logging_mixin.py:115} INFO - [2023-01-07 22:30:15,298] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:16,186] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:30:16,188] {logging_mixin.py:115} INFO - [2023-01-07 22:30:16,188] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:30:16,188] {logging_mixin.py:115} INFO - [2023-01-07 22:30:16,188] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:30:16,196] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:16,217] {logging_mixin.py:115} INFO - [2023-01-07 22:30:16,217] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:30:16,238] {logging_mixin.py:115} INFO - [2023-01-07 22:30:16,238] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:30:16,247] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.955 seconds
[2023-01-07 22:30:46,315] {processor.py:153} INFO - Started process (PID=2658) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:46,316] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:30:46,317] {logging_mixin.py:115} INFO - [2023-01-07 22:30:46,317] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:47,257] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:30:47,258] {logging_mixin.py:115} INFO - [2023-01-07 22:30:47,258] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:30:47,259] {logging_mixin.py:115} INFO - [2023-01-07 22:30:47,258] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:30:47,266] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:30:47,288] {logging_mixin.py:115} INFO - [2023-01-07 22:30:47,288] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:30:47,309] {logging_mixin.py:115} INFO - [2023-01-07 22:30:47,309] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:30:47,318] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.009 seconds
[2023-01-07 22:31:17,391] {processor.py:153} INFO - Started process (PID=2676) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:17,392] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:31:17,392] {logging_mixin.py:115} INFO - [2023-01-07 22:31:17,392] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:18,397] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:31:18,399] {logging_mixin.py:115} INFO - [2023-01-07 22:31:18,399] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:31:18,399] {logging_mixin.py:115} INFO - [2023-01-07 22:31:18,399] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:31:18,407] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:18,431] {logging_mixin.py:115} INFO - [2023-01-07 22:31:18,430] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:31:18,452] {logging_mixin.py:115} INFO - [2023-01-07 22:31:18,451] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:31:18,461] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.074 seconds
[2023-01-07 22:31:48,529] {processor.py:153} INFO - Started process (PID=2701) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:48,530] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:31:48,531] {logging_mixin.py:115} INFO - [2023-01-07 22:31:48,531] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:49,443] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:31:49,444] {logging_mixin.py:115} INFO - [2023-01-07 22:31:49,444] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:31:49,445] {logging_mixin.py:115} INFO - [2023-01-07 22:31:49,445] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:31:49,452] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:31:49,475] {logging_mixin.py:115} INFO - [2023-01-07 22:31:49,475] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:31:49,496] {logging_mixin.py:115} INFO - [2023-01-07 22:31:49,496] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:31:49,506] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.981 seconds
[2023-01-07 22:32:19,576] {processor.py:153} INFO - Started process (PID=2727) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:19,577] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:32:19,578] {logging_mixin.py:115} INFO - [2023-01-07 22:32:19,578] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:20,512] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:32:20,513] {logging_mixin.py:115} INFO - [2023-01-07 22:32:20,513] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:32:20,514] {logging_mixin.py:115} INFO - [2023-01-07 22:32:20,514] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:32:20,522] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:20,550] {logging_mixin.py:115} INFO - [2023-01-07 22:32:20,549] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:32:20,578] {logging_mixin.py:115} INFO - [2023-01-07 22:32:20,578] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:32:20,588] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.017 seconds
[2023-01-07 22:32:50,658] {processor.py:153} INFO - Started process (PID=2752) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:50,659] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:32:50,660] {logging_mixin.py:115} INFO - [2023-01-07 22:32:50,660] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:51,650] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:32:51,652] {logging_mixin.py:115} INFO - [2023-01-07 22:32:51,651] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:32:51,652] {logging_mixin.py:115} INFO - [2023-01-07 22:32:51,652] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:32:51,659] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:32:51,683] {logging_mixin.py:115} INFO - [2023-01-07 22:32:51,683] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:32:51,704] {logging_mixin.py:115} INFO - [2023-01-07 22:32:51,704] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:32:51,713] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.061 seconds
[2023-01-07 22:33:21,781] {processor.py:153} INFO - Started process (PID=2770) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:21,781] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:33:21,782] {logging_mixin.py:115} INFO - [2023-01-07 22:33:21,782] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:22,716] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:33:22,717] {logging_mixin.py:115} INFO - [2023-01-07 22:33:22,717] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:33:22,718] {logging_mixin.py:115} INFO - [2023-01-07 22:33:22,717] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:33:22,725] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:22,747] {logging_mixin.py:115} INFO - [2023-01-07 22:33:22,747] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:33:22,767] {logging_mixin.py:115} INFO - [2023-01-07 22:33:22,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:33:22,777] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.001 seconds
[2023-01-07 22:33:52,856] {processor.py:153} INFO - Started process (PID=2796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:52,857] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:33:52,858] {logging_mixin.py:115} INFO - [2023-01-07 22:33:52,858] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:53,748] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:33:53,749] {logging_mixin.py:115} INFO - [2023-01-07 22:33:53,749] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:33:53,750] {logging_mixin.py:115} INFO - [2023-01-07 22:33:53,750] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:33:53,757] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:33:53,780] {logging_mixin.py:115} INFO - [2023-01-07 22:33:53,780] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:33:53,804] {logging_mixin.py:115} INFO - [2023-01-07 22:33:53,804] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:33:53,813] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-07 22:34:23,883] {processor.py:153} INFO - Started process (PID=2821) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:23,884] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:34:23,885] {logging_mixin.py:115} INFO - [2023-01-07 22:34:23,884] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:24,874] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:34:24,876] {logging_mixin.py:115} INFO - [2023-01-07 22:34:24,876] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:34:24,876] {logging_mixin.py:115} INFO - [2023-01-07 22:34:24,876] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:34:24,884] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:24,907] {logging_mixin.py:115} INFO - [2023-01-07 22:34:24,906] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:34:24,927] {logging_mixin.py:115} INFO - [2023-01-07 22:34:24,927] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:34:24,937] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.060 seconds
[2023-01-07 22:34:55,042] {processor.py:153} INFO - Started process (PID=2839) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:55,045] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:34:55,045] {logging_mixin.py:115} INFO - [2023-01-07 22:34:55,045] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:56,309] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:34:56,311] {logging_mixin.py:115} INFO - [2023-01-07 22:34:56,311] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:34:56,312] {logging_mixin.py:115} INFO - [2023-01-07 22:34:56,311] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:34:56,324] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:34:56,355] {logging_mixin.py:115} INFO - [2023-01-07 22:34:56,354] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:34:56,384] {logging_mixin.py:115} INFO - [2023-01-07 22:34:56,384] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:34:56,397] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.360 seconds
[2023-01-07 22:35:26,469] {processor.py:153} INFO - Started process (PID=2865) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:26,470] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:35:26,471] {logging_mixin.py:115} INFO - [2023-01-07 22:35:26,470] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:27,365] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:35:27,366] {logging_mixin.py:115} INFO - [2023-01-07 22:35:27,366] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:35:27,367] {logging_mixin.py:115} INFO - [2023-01-07 22:35:27,367] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:35:27,374] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:27,397] {logging_mixin.py:115} INFO - [2023-01-07 22:35:27,396] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:35:27,417] {logging_mixin.py:115} INFO - [2023-01-07 22:35:27,417] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:35:27,426] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 22:35:57,495] {processor.py:153} INFO - Started process (PID=2890) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:57,497] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:35:57,498] {logging_mixin.py:115} INFO - [2023-01-07 22:35:57,498] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:58,634] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:35:58,635] {logging_mixin.py:115} INFO - [2023-01-07 22:35:58,635] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:35:58,636] {logging_mixin.py:115} INFO - [2023-01-07 22:35:58,635] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:35:58,643] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:35:58,665] {logging_mixin.py:115} INFO - [2023-01-07 22:35:58,665] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:35:58,686] {logging_mixin.py:115} INFO - [2023-01-07 22:35:58,686] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:35:58,695] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.205 seconds
[2023-01-07 22:36:28,763] {processor.py:153} INFO - Started process (PID=2915) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:36:28,765] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:36:28,765] {logging_mixin.py:115} INFO - [2023-01-07 22:36:28,765] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:36:29,653] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:36:29,654] {logging_mixin.py:115} INFO - [2023-01-07 22:36:29,654] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:36:29,654] {logging_mixin.py:115} INFO - [2023-01-07 22:36:29,654] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:36:29,662] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:36:29,684] {logging_mixin.py:115} INFO - [2023-01-07 22:36:29,684] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:36:29,706] {logging_mixin.py:115} INFO - [2023-01-07 22:36:29,706] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:36:29,716] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 22:36:59,787] {processor.py:153} INFO - Started process (PID=2933) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:36:59,788] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:36:59,789] {logging_mixin.py:115} INFO - [2023-01-07 22:36:59,789] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:37:00,921] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:37:00,923] {logging_mixin.py:115} INFO - [2023-01-07 22:37:00,923] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:37:00,924] {logging_mixin.py:115} INFO - [2023-01-07 22:37:00,923] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:37:00,936] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:37:00,967] {logging_mixin.py:115} INFO - [2023-01-07 22:37:00,967] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:37:00,998] {logging_mixin.py:115} INFO - [2023-01-07 22:37:00,997] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:37:01,010] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.228 seconds
[2023-01-07 22:37:31,082] {processor.py:153} INFO - Started process (PID=2958) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:37:31,083] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:37:31,084] {logging_mixin.py:115} INFO - [2023-01-07 22:37:31,084] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:37:32,130] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:37:32,131] {logging_mixin.py:115} INFO - [2023-01-07 22:37:32,131] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:37:32,131] {logging_mixin.py:115} INFO - [2023-01-07 22:37:32,131] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:37:32,139] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:37:32,162] {logging_mixin.py:115} INFO - [2023-01-07 22:37:32,161] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:37:32,182] {logging_mixin.py:115} INFO - [2023-01-07 22:37:32,182] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:37:32,191] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.115 seconds
[2023-01-07 22:38:02,257] {processor.py:153} INFO - Started process (PID=2984) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:02,258] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:38:02,259] {logging_mixin.py:115} INFO - [2023-01-07 22:38:02,259] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:03,160] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:38:03,161] {logging_mixin.py:115} INFO - [2023-01-07 22:38:03,161] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:38:03,162] {logging_mixin.py:115} INFO - [2023-01-07 22:38:03,161] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:38:03,174] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:03,207] {logging_mixin.py:115} INFO - [2023-01-07 22:38:03,207] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:38:03,236] {logging_mixin.py:115} INFO - [2023-01-07 22:38:03,236] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:38:03,248] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.995 seconds
[2023-01-07 22:38:33,335] {processor.py:153} INFO - Started process (PID=3007) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:33,336] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:38:33,337] {logging_mixin.py:115} INFO - [2023-01-07 22:38:33,337] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:34,299] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:38:34,300] {logging_mixin.py:115} INFO - [2023-01-07 22:38:34,300] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:38:34,300] {logging_mixin.py:115} INFO - [2023-01-07 22:38:34,300] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:38:34,308] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:38:34,330] {logging_mixin.py:115} INFO - [2023-01-07 22:38:34,330] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:38:34,351] {logging_mixin.py:115} INFO - [2023-01-07 22:38:34,351] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:38:34,360] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.029 seconds
[2023-01-07 22:39:04,419] {processor.py:153} INFO - Started process (PID=3024) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:04,420] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:39:04,421] {logging_mixin.py:115} INFO - [2023-01-07 22:39:04,420] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:05,787] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:39:05,788] {logging_mixin.py:115} INFO - [2023-01-07 22:39:05,788] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:39:05,789] {logging_mixin.py:115} INFO - [2023-01-07 22:39:05,789] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:39:05,801] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:05,833] {logging_mixin.py:115} INFO - [2023-01-07 22:39:05,833] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:39:05,862] {logging_mixin.py:115} INFO - [2023-01-07 22:39:05,862] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:39:05,875] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.463 seconds
[2023-01-07 22:39:35,949] {processor.py:153} INFO - Started process (PID=3049) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:35,950] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:39:35,951] {logging_mixin.py:115} INFO - [2023-01-07 22:39:35,950] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:36,844] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:39:36,845] {logging_mixin.py:115} INFO - [2023-01-07 22:39:36,845] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:39:36,846] {logging_mixin.py:115} INFO - [2023-01-07 22:39:36,845] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:39:36,853] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:39:36,875] {logging_mixin.py:115} INFO - [2023-01-07 22:39:36,875] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:39:36,896] {logging_mixin.py:115} INFO - [2023-01-07 22:39:36,896] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:39:36,905] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-07 22:40:06,974] {processor.py:153} INFO - Started process (PID=3074) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:06,975] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:40:06,976] {logging_mixin.py:115} INFO - [2023-01-07 22:40:06,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:07,879] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:40:07,880] {logging_mixin.py:115} INFO - [2023-01-07 22:40:07,880] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:40:07,881] {logging_mixin.py:115} INFO - [2023-01-07 22:40:07,880] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:40:07,888] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:07,911] {logging_mixin.py:115} INFO - [2023-01-07 22:40:07,911] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:40:07,932] {logging_mixin.py:115} INFO - [2023-01-07 22:40:07,931] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:40:07,941] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 22:40:38,026] {processor.py:153} INFO - Started process (PID=3099) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:38,028] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:40:38,029] {logging_mixin.py:115} INFO - [2023-01-07 22:40:38,029] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:39,200] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:40:39,202] {logging_mixin.py:115} INFO - [2023-01-07 22:40:39,202] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:40:39,202] {logging_mixin.py:115} INFO - [2023-01-07 22:40:39,202] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:40:39,209] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:40:39,232] {logging_mixin.py:115} INFO - [2023-01-07 22:40:39,231] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:40:39,252] {logging_mixin.py:115} INFO - [2023-01-07 22:40:39,252] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:40:39,261] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.241 seconds
[2023-01-07 22:41:09,348] {processor.py:153} INFO - Started process (PID=3119) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:09,349] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:41:09,350] {logging_mixin.py:115} INFO - [2023-01-07 22:41:09,350] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:10,250] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:41:10,251] {logging_mixin.py:115} INFO - [2023-01-07 22:41:10,251] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:41:10,251] {logging_mixin.py:115} INFO - [2023-01-07 22:41:10,251] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:41:10,259] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:10,282] {logging_mixin.py:115} INFO - [2023-01-07 22:41:10,282] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:41:10,303] {logging_mixin.py:115} INFO - [2023-01-07 22:41:10,302] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:41:10,312] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-07 22:41:40,409] {processor.py:153} INFO - Started process (PID=3145) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:40,410] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:41:40,411] {logging_mixin.py:115} INFO - [2023-01-07 22:41:40,411] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:41,306] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:41:41,307] {logging_mixin.py:115} INFO - [2023-01-07 22:41:41,307] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:41:41,308] {logging_mixin.py:115} INFO - [2023-01-07 22:41:41,307] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:41:41,315] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:41:41,337] {logging_mixin.py:115} INFO - [2023-01-07 22:41:41,337] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:41:41,359] {logging_mixin.py:115} INFO - [2023-01-07 22:41:41,359] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:41:41,369] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.965 seconds
[2023-01-07 22:42:11,469] {processor.py:153} INFO - Started process (PID=3171) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:11,470] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:42:11,471] {logging_mixin.py:115} INFO - [2023-01-07 22:42:11,471] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:12,377] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:42:12,378] {logging_mixin.py:115} INFO - [2023-01-07 22:42:12,378] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:42:12,379] {logging_mixin.py:115} INFO - [2023-01-07 22:42:12,379] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:42:12,386] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:12,409] {logging_mixin.py:115} INFO - [2023-01-07 22:42:12,409] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:42:12,431] {logging_mixin.py:115} INFO - [2023-01-07 22:42:12,431] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:42:12,442] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.977 seconds
[2023-01-07 22:42:42,565] {processor.py:153} INFO - Started process (PID=3198) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:42,567] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:42:42,567] {logging_mixin.py:115} INFO - [2023-01-07 22:42:42,567] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:43,492] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:42:43,493] {logging_mixin.py:115} INFO - [2023-01-07 22:42:43,493] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:42:43,494] {logging_mixin.py:115} INFO - [2023-01-07 22:42:43,493] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:42:43,501] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:42:43,523] {logging_mixin.py:115} INFO - [2023-01-07 22:42:43,523] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:42:43,544] {logging_mixin.py:115} INFO - [2023-01-07 22:42:43,544] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:42:43,553] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.994 seconds
[2023-01-07 22:43:13,650] {processor.py:153} INFO - Started process (PID=3216) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:13,654] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:43:13,655] {logging_mixin.py:115} INFO - [2023-01-07 22:43:13,654] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:14,541] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:43:14,543] {logging_mixin.py:115} INFO - [2023-01-07 22:43:14,542] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:43:14,543] {logging_mixin.py:115} INFO - [2023-01-07 22:43:14,543] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:43:14,550] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:14,573] {logging_mixin.py:115} INFO - [2023-01-07 22:43:14,573] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:43:14,595] {logging_mixin.py:115} INFO - [2023-01-07 22:43:14,595] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:43:14,605] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
[2023-01-07 22:43:44,671] {processor.py:153} INFO - Started process (PID=3243) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:44,673] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:43:44,673] {logging_mixin.py:115} INFO - [2023-01-07 22:43:44,673] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:45,578] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:43:45,579] {logging_mixin.py:115} INFO - [2023-01-07 22:43:45,579] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:43:45,580] {logging_mixin.py:115} INFO - [2023-01-07 22:43:45,579] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:43:45,587] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:43:45,610] {logging_mixin.py:115} INFO - [2023-01-07 22:43:45,610] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:43:45,631] {logging_mixin.py:115} INFO - [2023-01-07 22:43:45,631] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:43:45,640] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-07 22:44:15,722] {processor.py:153} INFO - Started process (PID=3268) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:15,722] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:44:15,723] {logging_mixin.py:115} INFO - [2023-01-07 22:44:15,723] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:16,595] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:44:16,596] {logging_mixin.py:115} INFO - [2023-01-07 22:44:16,596] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:44:16,597] {logging_mixin.py:115} INFO - [2023-01-07 22:44:16,597] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:44:16,604] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:16,627] {logging_mixin.py:115} INFO - [2023-01-07 22:44:16,626] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:44:16,647] {logging_mixin.py:115} INFO - [2023-01-07 22:44:16,647] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:44:16,657] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.940 seconds
[2023-01-07 22:44:46,726] {processor.py:153} INFO - Started process (PID=3294) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:46,728] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:44:46,728] {logging_mixin.py:115} INFO - [2023-01-07 22:44:46,728] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:47,745] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:44:47,746] {logging_mixin.py:115} INFO - [2023-01-07 22:44:47,746] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:44:47,746] {logging_mixin.py:115} INFO - [2023-01-07 22:44:47,746] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:44:47,753] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:44:47,776] {logging_mixin.py:115} INFO - [2023-01-07 22:44:47,776] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:44:47,797] {logging_mixin.py:115} INFO - [2023-01-07 22:44:47,797] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:44:47,807] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.085 seconds
[2023-01-07 22:45:17,872] {processor.py:153} INFO - Started process (PID=3312) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:17,873] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:45:17,874] {logging_mixin.py:115} INFO - [2023-01-07 22:45:17,874] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:18,812] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:45:18,813] {logging_mixin.py:115} INFO - [2023-01-07 22:45:18,813] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:45:18,814] {logging_mixin.py:115} INFO - [2023-01-07 22:45:18,814] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:45:18,821] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:18,844] {logging_mixin.py:115} INFO - [2023-01-07 22:45:18,844] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:45:18,865] {logging_mixin.py:115} INFO - [2023-01-07 22:45:18,865] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:45:18,875] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.008 seconds
[2023-01-07 22:45:48,949] {processor.py:153} INFO - Started process (PID=3339) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:48,950] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:45:48,951] {logging_mixin.py:115} INFO - [2023-01-07 22:45:48,951] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:49,844] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:45:49,845] {logging_mixin.py:115} INFO - [2023-01-07 22:45:49,845] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:45:49,846] {logging_mixin.py:115} INFO - [2023-01-07 22:45:49,845] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:45:49,853] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:45:49,875] {logging_mixin.py:115} INFO - [2023-01-07 22:45:49,875] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:45:49,897] {logging_mixin.py:115} INFO - [2023-01-07 22:45:49,896] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:45:49,906] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-07 22:46:19,976] {processor.py:153} INFO - Started process (PID=3364) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:19,977] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:46:19,978] {logging_mixin.py:115} INFO - [2023-01-07 22:46:19,978] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:20,878] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:46:20,880] {logging_mixin.py:115} INFO - [2023-01-07 22:46:20,880] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:46:20,880] {logging_mixin.py:115} INFO - [2023-01-07 22:46:20,880] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:46:20,888] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:20,910] {logging_mixin.py:115} INFO - [2023-01-07 22:46:20,910] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:46:20,931] {logging_mixin.py:115} INFO - [2023-01-07 22:46:20,931] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:46:20,941] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.969 seconds
[2023-01-07 22:46:51,010] {processor.py:153} INFO - Started process (PID=3382) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:51,010] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:46:51,011] {logging_mixin.py:115} INFO - [2023-01-07 22:46:51,011] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:51,934] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:46:51,935] {logging_mixin.py:115} INFO - [2023-01-07 22:46:51,935] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:46:51,936] {logging_mixin.py:115} INFO - [2023-01-07 22:46:51,935] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:46:51,945] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:46:51,973] {logging_mixin.py:115} INFO - [2023-01-07 22:46:51,973] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:46:51,994] {logging_mixin.py:115} INFO - [2023-01-07 22:46:51,994] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:46:52,005] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.000 seconds
[2023-01-07 22:47:22,090] {processor.py:153} INFO - Started process (PID=3408) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:22,092] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:47:22,092] {logging_mixin.py:115} INFO - [2023-01-07 22:47:22,092] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:22,970] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:47:22,971] {logging_mixin.py:115} INFO - [2023-01-07 22:47:22,971] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:47:22,972] {logging_mixin.py:115} INFO - [2023-01-07 22:47:22,972] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:47:22,980] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:23,002] {logging_mixin.py:115} INFO - [2023-01-07 22:47:23,001] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:47:23,022] {logging_mixin.py:115} INFO - [2023-01-07 22:47:23,021] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:47:23,031] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.945 seconds
[2023-01-07 22:47:53,097] {processor.py:153} INFO - Started process (PID=3433) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:53,097] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:47:53,098] {logging_mixin.py:115} INFO - [2023-01-07 22:47:53,098] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:53,993] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:47:53,994] {logging_mixin.py:115} INFO - [2023-01-07 22:47:53,994] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:47:53,994] {logging_mixin.py:115} INFO - [2023-01-07 22:47:53,994] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:47:54,002] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:47:54,024] {logging_mixin.py:115} INFO - [2023-01-07 22:47:54,024] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:47:54,045] {logging_mixin.py:115} INFO - [2023-01-07 22:47:54,045] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:47:54,054] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 22:48:24,126] {processor.py:153} INFO - Started process (PID=3458) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:24,127] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:48:24,128] {logging_mixin.py:115} INFO - [2023-01-07 22:48:24,128] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:25,018] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:48:25,019] {logging_mixin.py:115} INFO - [2023-01-07 22:48:25,019] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:48:25,020] {logging_mixin.py:115} INFO - [2023-01-07 22:48:25,020] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:48:25,027] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:25,050] {logging_mixin.py:115} INFO - [2023-01-07 22:48:25,050] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:48:25,071] {logging_mixin.py:115} INFO - [2023-01-07 22:48:25,071] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:48:25,081] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.959 seconds
[2023-01-07 22:48:55,150] {processor.py:153} INFO - Started process (PID=3478) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:55,150] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:48:55,151] {logging_mixin.py:115} INFO - [2023-01-07 22:48:55,151] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:56,095] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:48:56,097] {logging_mixin.py:115} INFO - [2023-01-07 22:48:56,097] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:48:56,097] {logging_mixin.py:115} INFO - [2023-01-07 22:48:56,097] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:48:56,104] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:48:56,127] {logging_mixin.py:115} INFO - [2023-01-07 22:48:56,127] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:48:56,148] {logging_mixin.py:115} INFO - [2023-01-07 22:48:56,147] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:48:56,157] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.012 seconds
[2023-01-07 22:49:26,233] {processor.py:153} INFO - Started process (PID=3505) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:26,234] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:49:26,234] {logging_mixin.py:115} INFO - [2023-01-07 22:49:26,234] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:27,120] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:49:27,121] {logging_mixin.py:115} INFO - [2023-01-07 22:49:27,121] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:49:27,122] {logging_mixin.py:115} INFO - [2023-01-07 22:49:27,121] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:49:27,129] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:27,151] {logging_mixin.py:115} INFO - [2023-01-07 22:49:27,151] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:49:27,172] {logging_mixin.py:115} INFO - [2023-01-07 22:49:27,172] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:49:27,181] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.953 seconds
[2023-01-07 22:49:57,248] {processor.py:153} INFO - Started process (PID=3530) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:57,248] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:49:57,249] {logging_mixin.py:115} INFO - [2023-01-07 22:49:57,249] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:58,138] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:49:58,139] {logging_mixin.py:115} INFO - [2023-01-07 22:49:58,139] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:49:58,140] {logging_mixin.py:115} INFO - [2023-01-07 22:49:58,140] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:49:58,147] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:49:58,170] {logging_mixin.py:115} INFO - [2023-01-07 22:49:58,169] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:49:58,190] {logging_mixin.py:115} INFO - [2023-01-07 22:49:58,190] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:49:58,199] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 22:50:28,270] {processor.py:153} INFO - Started process (PID=3556) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:50:28,271] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:50:28,271] {logging_mixin.py:115} INFO - [2023-01-07 22:50:28,271] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:50:29,154] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:50:29,155] {logging_mixin.py:115} INFO - [2023-01-07 22:50:29,155] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:50:29,156] {logging_mixin.py:115} INFO - [2023-01-07 22:50:29,155] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:50:29,163] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:50:29,185] {logging_mixin.py:115} INFO - [2023-01-07 22:50:29,185] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:50:29,205] {logging_mixin.py:115} INFO - [2023-01-07 22:50:29,205] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:50:29,214] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.949 seconds
[2023-01-07 22:50:59,284] {processor.py:153} INFO - Started process (PID=3574) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:50:59,287] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:50:59,287] {logging_mixin.py:115} INFO - [2023-01-07 22:50:59,287] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:51:00,241] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:51:00,242] {logging_mixin.py:115} INFO - [2023-01-07 22:51:00,242] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:51:00,243] {logging_mixin.py:115} INFO - [2023-01-07 22:51:00,243] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:51:00,251] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:51:00,273] {logging_mixin.py:115} INFO - [2023-01-07 22:51:00,273] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:51:00,293] {logging_mixin.py:115} INFO - [2023-01-07 22:51:00,293] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:51:00,303] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.024 seconds
[2023-01-07 22:51:30,369] {processor.py:153} INFO - Started process (PID=3599) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:51:30,370] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:51:30,371] {logging_mixin.py:115} INFO - [2023-01-07 22:51:30,371] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:51:31,273] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:51:31,275] {logging_mixin.py:115} INFO - [2023-01-07 22:51:31,274] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:51:31,275] {logging_mixin.py:115} INFO - [2023-01-07 22:51:31,275] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:51:31,282] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:51:31,306] {logging_mixin.py:115} INFO - [2023-01-07 22:51:31,305] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:51:31,326] {logging_mixin.py:115} INFO - [2023-01-07 22:51:31,326] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:51:31,336] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 22:52:01,403] {processor.py:153} INFO - Started process (PID=3623) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:01,405] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:52:01,406] {logging_mixin.py:115} INFO - [2023-01-07 22:52:01,406] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:02,305] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:52:02,306] {logging_mixin.py:115} INFO - [2023-01-07 22:52:02,306] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:52:02,307] {logging_mixin.py:115} INFO - [2023-01-07 22:52:02,307] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:52:02,314] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:02,337] {logging_mixin.py:115} INFO - [2023-01-07 22:52:02,336] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:52:02,359] {logging_mixin.py:115} INFO - [2023-01-07 22:52:02,359] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:52:02,370] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 22:52:32,449] {processor.py:153} INFO - Started process (PID=3646) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:32,449] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:52:32,450] {logging_mixin.py:115} INFO - [2023-01-07 22:52:32,450] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:33,385] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:52:33,387] {logging_mixin.py:115} INFO - [2023-01-07 22:52:33,386] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:52:33,387] {logging_mixin.py:115} INFO - [2023-01-07 22:52:33,387] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:52:33,394] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:52:33,421] {logging_mixin.py:115} INFO - [2023-01-07 22:52:33,421] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:52:33,442] {logging_mixin.py:115} INFO - [2023-01-07 22:52:33,442] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:52:33,451] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.007 seconds
[2023-01-07 22:53:03,537] {processor.py:153} INFO - Started process (PID=3664) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:03,538] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:53:03,539] {logging_mixin.py:115} INFO - [2023-01-07 22:53:03,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:04,493] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:53:04,494] {logging_mixin.py:115} INFO - [2023-01-07 22:53:04,494] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:53:04,494] {logging_mixin.py:115} INFO - [2023-01-07 22:53:04,494] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:53:04,502] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:04,524] {logging_mixin.py:115} INFO - [2023-01-07 22:53:04,524] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:53:04,545] {logging_mixin.py:115} INFO - [2023-01-07 22:53:04,545] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:53:04,554] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.022 seconds
[2023-01-07 22:53:34,620] {processor.py:153} INFO - Started process (PID=3689) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:34,621] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:53:34,621] {logging_mixin.py:115} INFO - [2023-01-07 22:53:34,621] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:35,516] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:53:35,518] {logging_mixin.py:115} INFO - [2023-01-07 22:53:35,517] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:53:35,518] {logging_mixin.py:115} INFO - [2023-01-07 22:53:35,518] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:53:35,525] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:53:35,548] {logging_mixin.py:115} INFO - [2023-01-07 22:53:35,548] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:53:35,569] {logging_mixin.py:115} INFO - [2023-01-07 22:53:35,569] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:53:35,579] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.963 seconds
[2023-01-07 22:54:05,647] {processor.py:153} INFO - Started process (PID=3714) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:05,649] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:54:05,649] {logging_mixin.py:115} INFO - [2023-01-07 22:54:05,649] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:06,568] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:54:06,569] {logging_mixin.py:115} INFO - [2023-01-07 22:54:06,569] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:54:06,570] {logging_mixin.py:115} INFO - [2023-01-07 22:54:06,570] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:54:06,577] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:06,603] {logging_mixin.py:115} INFO - [2023-01-07 22:54:06,603] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:54:06,630] {logging_mixin.py:115} INFO - [2023-01-07 22:54:06,630] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:54:06,644] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.001 seconds
[2023-01-07 22:54:36,689] {processor.py:153} INFO - Started process (PID=3739) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:36,689] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:54:36,690] {logging_mixin.py:115} INFO - [2023-01-07 22:54:36,690] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:37,577] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:54:37,578] {logging_mixin.py:115} INFO - [2023-01-07 22:54:37,578] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:54:37,579] {logging_mixin.py:115} INFO - [2023-01-07 22:54:37,579] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:54:37,586] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:54:37,609] {logging_mixin.py:115} INFO - [2023-01-07 22:54:37,608] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:54:37,629] {logging_mixin.py:115} INFO - [2023-01-07 22:54:37,629] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:54:37,638] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-07 22:55:07,699] {processor.py:153} INFO - Started process (PID=3756) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:07,700] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:55:07,701] {logging_mixin.py:115} INFO - [2023-01-07 22:55:07,701] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:08,599] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:55:08,600] {logging_mixin.py:115} INFO - [2023-01-07 22:55:08,600] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:55:08,601] {logging_mixin.py:115} INFO - [2023-01-07 22:55:08,600] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:55:08,608] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:08,631] {logging_mixin.py:115} INFO - [2023-01-07 22:55:08,631] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:55:08,652] {logging_mixin.py:115} INFO - [2023-01-07 22:55:08,651] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:55:08,662] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 22:55:38,808] {processor.py:153} INFO - Started process (PID=3781) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:38,808] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:55:38,809] {logging_mixin.py:115} INFO - [2023-01-07 22:55:38,809] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:39,732] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:55:39,733] {logging_mixin.py:115} INFO - [2023-01-07 22:55:39,733] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:55:39,733] {logging_mixin.py:115} INFO - [2023-01-07 22:55:39,733] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:55:39,741] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:55:39,764] {logging_mixin.py:115} INFO - [2023-01-07 22:55:39,764] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:55:39,785] {logging_mixin.py:115} INFO - [2023-01-07 22:55:39,785] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:55:39,794] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.991 seconds
[2023-01-07 22:56:09,879] {processor.py:153} INFO - Started process (PID=3806) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:09,880] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:56:09,881] {logging_mixin.py:115} INFO - [2023-01-07 22:56:09,881] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:10,765] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:56:10,766] {logging_mixin.py:115} INFO - [2023-01-07 22:56:10,766] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:56:10,767] {logging_mixin.py:115} INFO - [2023-01-07 22:56:10,766] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:56:10,774] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:10,806] {logging_mixin.py:115} INFO - [2023-01-07 22:56:10,805] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:56:10,827] {logging_mixin.py:115} INFO - [2023-01-07 22:56:10,827] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:56:10,836] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-07 22:56:41,841] {processor.py:153} INFO - Started process (PID=3831) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:41,842] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:56:41,842] {logging_mixin.py:115} INFO - [2023-01-07 22:56:41,842] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:42,715] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:56:42,717] {logging_mixin.py:115} INFO - [2023-01-07 22:56:42,717] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:56:42,717] {logging_mixin.py:115} INFO - [2023-01-07 22:56:42,717] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:56:42,724] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:56:42,747] {logging_mixin.py:115} INFO - [2023-01-07 22:56:42,746] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:56:42,767] {logging_mixin.py:115} INFO - [2023-01-07 22:56:42,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:56:42,777] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.940 seconds
[2023-01-07 22:57:12,898] {processor.py:153} INFO - Started process (PID=3849) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:12,898] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:57:12,899] {logging_mixin.py:115} INFO - [2023-01-07 22:57:12,899] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:13,875] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:57:13,877] {logging_mixin.py:115} INFO - [2023-01-07 22:57:13,877] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:57:13,878] {logging_mixin.py:115} INFO - [2023-01-07 22:57:13,878] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:57:13,891] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:13,914] {logging_mixin.py:115} INFO - [2023-01-07 22:57:13,914] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:57:13,935] {logging_mixin.py:115} INFO - [2023-01-07 22:57:13,934] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:57:13,944] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.050 seconds
[2023-01-07 22:57:44,014] {processor.py:153} INFO - Started process (PID=3874) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:44,016] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:57:44,016] {logging_mixin.py:115} INFO - [2023-01-07 22:57:44,016] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:44,930] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:57:44,931] {logging_mixin.py:115} INFO - [2023-01-07 22:57:44,931] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:57:44,932] {logging_mixin.py:115} INFO - [2023-01-07 22:57:44,931] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:57:44,939] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:57:44,963] {logging_mixin.py:115} INFO - [2023-01-07 22:57:44,963] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:57:44,983] {logging_mixin.py:115} INFO - [2023-01-07 22:57:44,983] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:57:44,993] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.983 seconds
[2023-01-07 22:58:15,099] {processor.py:153} INFO - Started process (PID=3899) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:15,100] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:58:15,101] {logging_mixin.py:115} INFO - [2023-01-07 22:58:15,101] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:15,997] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:58:15,998] {logging_mixin.py:115} INFO - [2023-01-07 22:58:15,998] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:58:15,999] {logging_mixin.py:115} INFO - [2023-01-07 22:58:15,998] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:58:16,006] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:16,028] {logging_mixin.py:115} INFO - [2023-01-07 22:58:16,028] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:58:16,049] {logging_mixin.py:115} INFO - [2023-01-07 22:58:16,048] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:58:16,058] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.964 seconds
[2023-01-07 22:58:46,128] {processor.py:153} INFO - Started process (PID=3924) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:46,129] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:58:46,130] {logging_mixin.py:115} INFO - [2023-01-07 22:58:46,130] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:47,051] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:58:47,052] {logging_mixin.py:115} INFO - [2023-01-07 22:58:47,052] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:58:47,053] {logging_mixin.py:115} INFO - [2023-01-07 22:58:47,052] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:58:47,060] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:58:47,082] {logging_mixin.py:115} INFO - [2023-01-07 22:58:47,082] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:58:47,103] {logging_mixin.py:115} INFO - [2023-01-07 22:58:47,102] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:58:47,112] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.989 seconds
[2023-01-07 22:59:17,208] {processor.py:153} INFO - Started process (PID=3943) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:17,210] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:59:17,210] {logging_mixin.py:115} INFO - [2023-01-07 22:59:17,210] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:18,462] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:59:18,466] {logging_mixin.py:115} INFO - [2023-01-07 22:59:18,466] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:59:18,467] {logging_mixin.py:115} INFO - [2023-01-07 22:59:18,466] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:59:18,474] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:18,502] {logging_mixin.py:115} INFO - [2023-01-07 22:59:18,501] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:59:18,523] {logging_mixin.py:115} INFO - [2023-01-07 22:59:18,523] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:59:18,532] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.330 seconds
[2023-01-07 22:59:48,608] {processor.py:153} INFO - Started process (PID=3969) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:48,610] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 22:59:48,610] {logging_mixin.py:115} INFO - [2023-01-07 22:59:48,610] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:49,494] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 22:59:49,495] {logging_mixin.py:115} INFO - [2023-01-07 22:59:49,495] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 22:59:49,496] {logging_mixin.py:115} INFO - [2023-01-07 22:59:49,496] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 22:59:49,503] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 22:59:49,526] {logging_mixin.py:115} INFO - [2023-01-07 22:59:49,526] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 22:59:49,546] {logging_mixin.py:115} INFO - [2023-01-07 22:59:49,546] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 22:59:49,555] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-07 23:00:19,626] {processor.py:153} INFO - Started process (PID=3993) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:19,629] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:00:19,629] {logging_mixin.py:115} INFO - [2023-01-07 23:00:19,629] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:20,586] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:00:20,587] {logging_mixin.py:115} INFO - [2023-01-07 23:00:20,587] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:00:20,587] {logging_mixin.py:115} INFO - [2023-01-07 23:00:20,587] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:00:20,595] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:20,618] {logging_mixin.py:115} INFO - [2023-01-07 23:00:20,618] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:00:20,639] {logging_mixin.py:115} INFO - [2023-01-07 23:00:20,639] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:00:20,651] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.030 seconds
[2023-01-07 23:00:50,720] {processor.py:153} INFO - Started process (PID=4018) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:50,720] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:00:50,721] {logging_mixin.py:115} INFO - [2023-01-07 23:00:50,721] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:51,608] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:00:51,609] {logging_mixin.py:115} INFO - [2023-01-07 23:00:51,609] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:00:51,609] {logging_mixin.py:115} INFO - [2023-01-07 23:00:51,609] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:00:51,617] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:00:51,640] {logging_mixin.py:115} INFO - [2023-01-07 23:00:51,639] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:00:51,661] {logging_mixin.py:115} INFO - [2023-01-07 23:00:51,661] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:00:51,670] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 23:01:21,799] {processor.py:153} INFO - Started process (PID=4035) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:21,800] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:01:21,801] {logging_mixin.py:115} INFO - [2023-01-07 23:01:21,801] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:22,724] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:01:22,725] {logging_mixin.py:115} INFO - [2023-01-07 23:01:22,725] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:01:22,726] {logging_mixin.py:115} INFO - [2023-01-07 23:01:22,726] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:01:22,733] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:22,756] {logging_mixin.py:115} INFO - [2023-01-07 23:01:22,755] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:01:22,777] {logging_mixin.py:115} INFO - [2023-01-07 23:01:22,776] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:01:22,786] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.992 seconds
[2023-01-07 23:01:52,875] {processor.py:153} INFO - Started process (PID=4059) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:52,876] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:01:52,877] {logging_mixin.py:115} INFO - [2023-01-07 23:01:52,877] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:53,833] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:01:53,835] {logging_mixin.py:115} INFO - [2023-01-07 23:01:53,835] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:01:53,835] {logging_mixin.py:115} INFO - [2023-01-07 23:01:53,835] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:01:53,842] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:01:53,866] {logging_mixin.py:115} INFO - [2023-01-07 23:01:53,865] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:01:53,887] {logging_mixin.py:115} INFO - [2023-01-07 23:01:53,886] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:01:53,896] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.026 seconds
[2023-01-07 23:02:23,964] {processor.py:153} INFO - Started process (PID=4084) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:23,966] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:02:23,966] {logging_mixin.py:115} INFO - [2023-01-07 23:02:23,966] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:24,868] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:02:24,869] {logging_mixin.py:115} INFO - [2023-01-07 23:02:24,869] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:02:24,870] {logging_mixin.py:115} INFO - [2023-01-07 23:02:24,869] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:02:24,877] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:24,900] {logging_mixin.py:115} INFO - [2023-01-07 23:02:24,899] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:02:24,921] {logging_mixin.py:115} INFO - [2023-01-07 23:02:24,920] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:02:24,930] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 23:02:55,000] {processor.py:153} INFO - Started process (PID=4108) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:55,001] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:02:55,001] {logging_mixin.py:115} INFO - [2023-01-07 23:02:55,001] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:55,905] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:02:55,907] {logging_mixin.py:115} INFO - [2023-01-07 23:02:55,907] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:02:55,907] {logging_mixin.py:115} INFO - [2023-01-07 23:02:55,907] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:02:55,915] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:02:55,938] {logging_mixin.py:115} INFO - [2023-01-07 23:02:55,938] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:02:55,963] {logging_mixin.py:115} INFO - [2023-01-07 23:02:55,963] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:02:55,972] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 23:03:26,050] {processor.py:153} INFO - Started process (PID=4127) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:26,053] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:03:26,054] {logging_mixin.py:115} INFO - [2023-01-07 23:03:26,054] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:27,178] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:03:27,180] {logging_mixin.py:115} INFO - [2023-01-07 23:03:27,179] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:03:27,180] {logging_mixin.py:115} INFO - [2023-01-07 23:03:27,180] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:03:27,193] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:27,223] {logging_mixin.py:115} INFO - [2023-01-07 23:03:27,223] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:03:27,252] {logging_mixin.py:115} INFO - [2023-01-07 23:03:27,252] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:03:27,262] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.218 seconds
[2023-01-07 23:03:57,341] {processor.py:153} INFO - Started process (PID=4154) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:57,341] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:03:57,342] {logging_mixin.py:115} INFO - [2023-01-07 23:03:57,342] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:58,249] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:03:58,250] {logging_mixin.py:115} INFO - [2023-01-07 23:03:58,250] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:03:58,250] {logging_mixin.py:115} INFO - [2023-01-07 23:03:58,250] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:03:58,258] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:03:58,280] {logging_mixin.py:115} INFO - [2023-01-07 23:03:58,280] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:03:58,301] {logging_mixin.py:115} INFO - [2023-01-07 23:03:58,301] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:03:58,311] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.975 seconds
[2023-01-07 23:04:28,383] {processor.py:153} INFO - Started process (PID=4179) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:04:28,384] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:04:28,385] {logging_mixin.py:115} INFO - [2023-01-07 23:04:28,385] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:04:29,283] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:04:29,284] {logging_mixin.py:115} INFO - [2023-01-07 23:04:29,284] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:04:29,285] {logging_mixin.py:115} INFO - [2023-01-07 23:04:29,284] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:04:29,292] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:04:29,316] {logging_mixin.py:115} INFO - [2023-01-07 23:04:29,316] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:04:29,337] {logging_mixin.py:115} INFO - [2023-01-07 23:04:29,337] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:04:29,346] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-07 23:05:00,106] {processor.py:153} INFO - Started process (PID=4205) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:00,107] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:05:00,107] {logging_mixin.py:115} INFO - [2023-01-07 23:05:00,107] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:01,102] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:05:01,103] {logging_mixin.py:115} INFO - [2023-01-07 23:05:01,103] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:05:01,103] {logging_mixin.py:115} INFO - [2023-01-07 23:05:01,103] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:05:01,111] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:01,134] {logging_mixin.py:115} INFO - [2023-01-07 23:05:01,133] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:05:01,155] {logging_mixin.py:115} INFO - [2023-01-07 23:05:01,155] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:05:01,165] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.065 seconds
[2023-01-07 23:05:31,247] {processor.py:153} INFO - Started process (PID=4230) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:31,248] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:05:31,249] {logging_mixin.py:115} INFO - [2023-01-07 23:05:31,249] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:32,188] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:05:32,189] {logging_mixin.py:115} INFO - [2023-01-07 23:05:32,189] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:05:32,190] {logging_mixin.py:115} INFO - [2023-01-07 23:05:32,189] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:05:32,197] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:05:32,221] {logging_mixin.py:115} INFO - [2023-01-07 23:05:32,220] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:05:32,242] {logging_mixin.py:115} INFO - [2023-01-07 23:05:32,242] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:05:32,252] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.011 seconds
[2023-01-07 23:06:02,328] {processor.py:153} INFO - Started process (PID=4249) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:02,329] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:06:02,330] {logging_mixin.py:115} INFO - [2023-01-07 23:06:02,329] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:03,279] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:06:03,280] {logging_mixin.py:115} INFO - [2023-01-07 23:06:03,280] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:06:03,281] {logging_mixin.py:115} INFO - [2023-01-07 23:06:03,280] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:06:03,288] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:03,313] {logging_mixin.py:115} INFO - [2023-01-07 23:06:03,313] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:06:03,334] {logging_mixin.py:115} INFO - [2023-01-07 23:06:03,334] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:06:03,344] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.021 seconds
[2023-01-07 23:06:33,421] {processor.py:153} INFO - Started process (PID=4272) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:33,421] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:06:33,422] {logging_mixin.py:115} INFO - [2023-01-07 23:06:33,422] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:34,340] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:06:34,341] {logging_mixin.py:115} INFO - [2023-01-07 23:06:34,341] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:06:34,342] {logging_mixin.py:115} INFO - [2023-01-07 23:06:34,341] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:06:34,349] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:06:34,372] {logging_mixin.py:115} INFO - [2023-01-07 23:06:34,372] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:06:34,393] {logging_mixin.py:115} INFO - [2023-01-07 23:06:34,393] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:06:34,402] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.986 seconds
[2023-01-07 23:07:04,493] {processor.py:153} INFO - Started process (PID=4297) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:04,494] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:07:04,495] {logging_mixin.py:115} INFO - [2023-01-07 23:07:04,495] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:05,396] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:07:05,397] {logging_mixin.py:115} INFO - [2023-01-07 23:07:05,397] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:07:05,398] {logging_mixin.py:115} INFO - [2023-01-07 23:07:05,398] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:07:05,405] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:05,428] {logging_mixin.py:115} INFO - [2023-01-07 23:07:05,428] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:07:05,449] {logging_mixin.py:115} INFO - [2023-01-07 23:07:05,449] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:07:05,459] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 23:07:35,539] {processor.py:153} INFO - Started process (PID=4315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:35,540] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:07:35,541] {logging_mixin.py:115} INFO - [2023-01-07 23:07:35,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:36,488] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:07:36,489] {logging_mixin.py:115} INFO - [2023-01-07 23:07:36,489] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:07:36,490] {logging_mixin.py:115} INFO - [2023-01-07 23:07:36,489] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:07:36,497] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:07:36,522] {logging_mixin.py:115} INFO - [2023-01-07 23:07:36,521] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:07:36,544] {logging_mixin.py:115} INFO - [2023-01-07 23:07:36,543] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:07:36,553] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.019 seconds
[2023-01-07 23:08:06,646] {processor.py:153} INFO - Started process (PID=4340) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:06,647] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:08:06,647] {logging_mixin.py:115} INFO - [2023-01-07 23:08:06,647] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:07,658] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:08:07,659] {logging_mixin.py:115} INFO - [2023-01-07 23:08:07,659] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:08:07,660] {logging_mixin.py:115} INFO - [2023-01-07 23:08:07,660] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:08:07,672] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:07,705] {logging_mixin.py:115} INFO - [2023-01-07 23:08:07,704] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:08:07,735] {logging_mixin.py:115} INFO - [2023-01-07 23:08:07,735] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:08:07,748] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.106 seconds
[2023-01-07 23:08:37,822] {processor.py:153} INFO - Started process (PID=4366) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:37,824] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:08:37,824] {logging_mixin.py:115} INFO - [2023-01-07 23:08:37,824] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:38,736] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:08:38,737] {logging_mixin.py:115} INFO - [2023-01-07 23:08:38,737] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:08:38,738] {logging_mixin.py:115} INFO - [2023-01-07 23:08:38,738] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:08:38,745] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:08:38,768] {logging_mixin.py:115} INFO - [2023-01-07 23:08:38,767] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:08:38,790] {logging_mixin.py:115} INFO - [2023-01-07 23:08:38,789] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:08:38,799] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.982 seconds
[2023-01-07 23:09:09,722] {processor.py:153} INFO - Started process (PID=4391) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:09,722] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:09:09,723] {logging_mixin.py:115} INFO - [2023-01-07 23:09:09,723] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:10,658] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:09:10,659] {logging_mixin.py:115} INFO - [2023-01-07 23:09:10,659] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:09:10,660] {logging_mixin.py:115} INFO - [2023-01-07 23:09:10,660] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:09:10,667] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:10,690] {logging_mixin.py:115} INFO - [2023-01-07 23:09:10,689] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:09:10,710] {logging_mixin.py:115} INFO - [2023-01-07 23:09:10,710] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:09:10,720] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.003 seconds
[2023-01-07 23:09:40,799] {processor.py:153} INFO - Started process (PID=4416) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:40,800] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:09:40,801] {logging_mixin.py:115} INFO - [2023-01-07 23:09:40,801] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:41,968] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:09:41,970] {logging_mixin.py:115} INFO - [2023-01-07 23:09:41,970] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:09:41,971] {logging_mixin.py:115} INFO - [2023-01-07 23:09:41,970] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:09:41,983] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:09:42,018] {logging_mixin.py:115} INFO - [2023-01-07 23:09:42,017] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:09:42,047] {logging_mixin.py:115} INFO - [2023-01-07 23:09:42,047] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:09:42,057] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.263 seconds
[2023-01-07 23:10:12,131] {processor.py:153} INFO - Started process (PID=4434) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:12,131] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:10:12,132] {logging_mixin.py:115} INFO - [2023-01-07 23:10:12,132] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:13,083] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:10:13,085] {logging_mixin.py:115} INFO - [2023-01-07 23:10:13,085] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:10:13,085] {logging_mixin.py:115} INFO - [2023-01-07 23:10:13,085] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:10:13,092] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:13,117] {logging_mixin.py:115} INFO - [2023-01-07 23:10:13,116] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:10:13,138] {logging_mixin.py:115} INFO - [2023-01-07 23:10:13,138] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:10:13,148] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.022 seconds
[2023-01-07 23:10:43,220] {processor.py:153} INFO - Started process (PID=4458) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:43,221] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:10:43,221] {logging_mixin.py:115} INFO - [2023-01-07 23:10:43,221] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:44,119] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:10:44,120] {logging_mixin.py:115} INFO - [2023-01-07 23:10:44,120] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:10:44,121] {logging_mixin.py:115} INFO - [2023-01-07 23:10:44,120] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:10:44,128] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:10:44,150] {logging_mixin.py:115} INFO - [2023-01-07 23:10:44,150] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:10:44,171] {logging_mixin.py:115} INFO - [2023-01-07 23:10:44,171] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:10:44,180] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 23:11:14,250] {processor.py:153} INFO - Started process (PID=4483) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:14,253] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:11:14,253] {logging_mixin.py:115} INFO - [2023-01-07 23:11:14,253] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:15,163] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:11:15,164] {logging_mixin.py:115} INFO - [2023-01-07 23:11:15,164] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:11:15,165] {logging_mixin.py:115} INFO - [2023-01-07 23:11:15,164] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:11:15,172] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:15,196] {logging_mixin.py:115} INFO - [2023-01-07 23:11:15,195] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:11:15,219] {logging_mixin.py:115} INFO - [2023-01-07 23:11:15,219] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:11:15,230] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-07 23:11:45,326] {processor.py:153} INFO - Started process (PID=4508) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:45,327] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:11:45,328] {logging_mixin.py:115} INFO - [2023-01-07 23:11:45,328] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:46,245] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:11:46,247] {logging_mixin.py:115} INFO - [2023-01-07 23:11:46,246] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:11:46,247] {logging_mixin.py:115} INFO - [2023-01-07 23:11:46,247] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:11:46,259] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:11:46,307] {logging_mixin.py:115} INFO - [2023-01-07 23:11:46,307] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:11:46,332] {logging_mixin.py:115} INFO - [2023-01-07 23:11:46,332] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:11:46,342] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.022 seconds
[2023-01-07 23:12:16,414] {processor.py:153} INFO - Started process (PID=4527) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:16,415] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:12:16,416] {logging_mixin.py:115} INFO - [2023-01-07 23:12:16,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:17,323] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:12:17,324] {logging_mixin.py:115} INFO - [2023-01-07 23:12:17,324] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:12:17,325] {logging_mixin.py:115} INFO - [2023-01-07 23:12:17,324] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:12:17,332] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:17,356] {logging_mixin.py:115} INFO - [2023-01-07 23:12:17,355] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:12:17,377] {logging_mixin.py:115} INFO - [2023-01-07 23:12:17,376] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:12:17,386] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.978 seconds
[2023-01-07 23:12:47,487] {processor.py:153} INFO - Started process (PID=4552) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:47,488] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:12:47,489] {logging_mixin.py:115} INFO - [2023-01-07 23:12:47,489] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:48,445] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:12:48,446] {logging_mixin.py:115} INFO - [2023-01-07 23:12:48,446] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:12:48,446] {logging_mixin.py:115} INFO - [2023-01-07 23:12:48,446] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:12:48,454] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:12:48,477] {logging_mixin.py:115} INFO - [2023-01-07 23:12:48,477] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:12:48,498] {logging_mixin.py:115} INFO - [2023-01-07 23:12:48,498] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:12:48,508] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.026 seconds
[2023-01-07 23:13:18,580] {processor.py:153} INFO - Started process (PID=4577) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:18,581] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:13:18,581] {logging_mixin.py:115} INFO - [2023-01-07 23:13:18,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:19,466] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:13:19,467] {logging_mixin.py:115} INFO - [2023-01-07 23:13:19,467] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:13:19,468] {logging_mixin.py:115} INFO - [2023-01-07 23:13:19,468] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:13:19,475] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:19,499] {logging_mixin.py:115} INFO - [2023-01-07 23:13:19,498] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:13:19,519] {logging_mixin.py:115} INFO - [2023-01-07 23:13:19,519] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:13:19,529] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.953 seconds
[2023-01-07 23:13:49,599] {processor.py:153} INFO - Started process (PID=4601) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:49,601] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:13:49,601] {logging_mixin.py:115} INFO - [2023-01-07 23:13:49,601] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:50,523] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:13:50,524] {logging_mixin.py:115} INFO - [2023-01-07 23:13:50,524] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:13:50,525] {logging_mixin.py:115} INFO - [2023-01-07 23:13:50,525] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:13:50,532] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:13:50,555] {logging_mixin.py:115} INFO - [2023-01-07 23:13:50,555] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:13:50,578] {logging_mixin.py:115} INFO - [2023-01-07 23:13:50,578] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:13:50,588] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.994 seconds
[2023-01-07 23:14:20,674] {processor.py:153} INFO - Started process (PID=4619) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:20,675] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:14:20,676] {logging_mixin.py:115} INFO - [2023-01-07 23:14:20,676] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:21,636] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:14:21,638] {logging_mixin.py:115} INFO - [2023-01-07 23:14:21,638] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:14:21,639] {logging_mixin.py:115} INFO - [2023-01-07 23:14:21,638] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:14:21,651] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:21,682] {logging_mixin.py:115} INFO - [2023-01-07 23:14:21,682] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:14:21,712] {logging_mixin.py:115} INFO - [2023-01-07 23:14:21,712] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:14:21,725] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.056 seconds
[2023-01-07 23:14:51,797] {processor.py:153} INFO - Started process (PID=4644) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:51,798] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:14:51,799] {logging_mixin.py:115} INFO - [2023-01-07 23:14:51,799] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:52,699] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:14:52,701] {logging_mixin.py:115} INFO - [2023-01-07 23:14:52,701] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:14:52,701] {logging_mixin.py:115} INFO - [2023-01-07 23:14:52,701] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:14:52,708] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:14:52,731] {logging_mixin.py:115} INFO - [2023-01-07 23:14:52,731] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:14:52,752] {logging_mixin.py:115} INFO - [2023-01-07 23:14:52,752] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:14:52,762] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.969 seconds
[2023-01-07 23:15:22,833] {processor.py:153} INFO - Started process (PID=4669) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:22,833] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:15:22,834] {logging_mixin.py:115} INFO - [2023-01-07 23:15:22,834] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:23,713] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:15:23,715] {logging_mixin.py:115} INFO - [2023-01-07 23:15:23,715] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:15:23,715] {logging_mixin.py:115} INFO - [2023-01-07 23:15:23,715] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:15:23,723] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:23,750] {logging_mixin.py:115} INFO - [2023-01-07 23:15:23,749] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:15:23,772] {logging_mixin.py:115} INFO - [2023-01-07 23:15:23,772] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:15:23,784] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-07 23:15:53,858] {processor.py:153} INFO - Started process (PID=4692) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:53,860] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:15:53,861] {logging_mixin.py:115} INFO - [2023-01-07 23:15:53,861] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:54,802] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:15:54,804] {logging_mixin.py:115} INFO - [2023-01-07 23:15:54,804] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:15:54,804] {logging_mixin.py:115} INFO - [2023-01-07 23:15:54,804] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:15:54,811] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:15:54,835] {logging_mixin.py:115} INFO - [2023-01-07 23:15:54,834] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:15:54,856] {logging_mixin.py:115} INFO - [2023-01-07 23:15:54,856] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:15:54,865] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.013 seconds
[2023-01-07 23:16:24,937] {processor.py:153} INFO - Started process (PID=4712) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:24,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:16:24,939] {logging_mixin.py:115} INFO - [2023-01-07 23:16:24,939] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:25,862] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:16:25,863] {logging_mixin.py:115} INFO - [2023-01-07 23:16:25,863] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:16:25,864] {logging_mixin.py:115} INFO - [2023-01-07 23:16:25,863] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:16:25,872] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:25,901] {logging_mixin.py:115} INFO - [2023-01-07 23:16:25,901] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:16:25,926] {logging_mixin.py:115} INFO - [2023-01-07 23:16:25,925] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:16:25,943] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.010 seconds
[2023-01-07 23:16:56,017] {processor.py:153} INFO - Started process (PID=4737) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:56,019] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:16:56,019] {logging_mixin.py:115} INFO - [2023-01-07 23:16:56,019] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:56,957] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:16:56,959] {logging_mixin.py:115} INFO - [2023-01-07 23:16:56,959] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:16:56,959] {logging_mixin.py:115} INFO - [2023-01-07 23:16:56,959] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:16:56,966] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:16:56,990] {logging_mixin.py:115} INFO - [2023-01-07 23:16:56,990] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:16:57,018] {logging_mixin.py:115} INFO - [2023-01-07 23:16:57,018] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:16:57,031] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.018 seconds
[2023-01-07 23:17:27,105] {processor.py:153} INFO - Started process (PID=4761) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:27,106] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:17:27,106] {logging_mixin.py:115} INFO - [2023-01-07 23:17:27,106] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:28,025] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:17:28,027] {logging_mixin.py:115} INFO - [2023-01-07 23:17:28,027] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:17:28,027] {logging_mixin.py:115} INFO - [2023-01-07 23:17:28,027] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:17:28,034] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:28,057] {logging_mixin.py:115} INFO - [2023-01-07 23:17:28,057] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:17:28,078] {logging_mixin.py:115} INFO - [2023-01-07 23:17:28,078] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:17:28,087] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.987 seconds
[2023-01-07 23:17:58,183] {processor.py:153} INFO - Started process (PID=4785) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:58,184] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:17:58,185] {logging_mixin.py:115} INFO - [2023-01-07 23:17:58,184] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:59,084] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:17:59,085] {logging_mixin.py:115} INFO - [2023-01-07 23:17:59,085] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:17:59,086] {logging_mixin.py:115} INFO - [2023-01-07 23:17:59,086] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:17:59,093] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:17:59,116] {logging_mixin.py:115} INFO - [2023-01-07 23:17:59,116] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:17:59,138] {logging_mixin.py:115} INFO - [2023-01-07 23:17:59,138] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:17:59,148] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.971 seconds
[2023-01-07 23:18:29,215] {processor.py:153} INFO - Started process (PID=4802) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:18:29,217] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:18:29,217] {logging_mixin.py:115} INFO - [2023-01-07 23:18:29,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:18:30,145] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:18:30,146] {logging_mixin.py:115} INFO - [2023-01-07 23:18:30,146] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:18:30,147] {logging_mixin.py:115} INFO - [2023-01-07 23:18:30,147] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:18:30,154] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:18:30,178] {logging_mixin.py:115} INFO - [2023-01-07 23:18:30,177] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:18:30,198] {logging_mixin.py:115} INFO - [2023-01-07 23:18:30,198] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:18:30,208] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.997 seconds
[2023-01-07 23:19:00,289] {processor.py:153} INFO - Started process (PID=4827) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:00,291] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:19:00,291] {logging_mixin.py:115} INFO - [2023-01-07 23:19:00,291] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:01,193] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:19:01,194] {logging_mixin.py:115} INFO - [2023-01-07 23:19:01,194] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:19:01,195] {logging_mixin.py:115} INFO - [2023-01-07 23:19:01,195] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:19:01,202] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:01,226] {logging_mixin.py:115} INFO - [2023-01-07 23:19:01,225] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:19:01,247] {logging_mixin.py:115} INFO - [2023-01-07 23:19:01,246] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:19:01,256] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.972 seconds
[2023-01-07 23:19:31,325] {processor.py:153} INFO - Started process (PID=4851) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:31,328] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:19:31,329] {logging_mixin.py:115} INFO - [2023-01-07 23:19:31,329] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:32,221] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:19:32,223] {logging_mixin.py:115} INFO - [2023-01-07 23:19:32,223] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:19:32,223] {logging_mixin.py:115} INFO - [2023-01-07 23:19:32,223] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:19:32,230] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:19:32,253] {logging_mixin.py:115} INFO - [2023-01-07 23:19:32,253] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:19:32,275] {logging_mixin.py:115} INFO - [2023-01-07 23:19:32,274] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:19:32,284] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.964 seconds
[2023-01-07 23:20:02,355] {processor.py:153} INFO - Started process (PID=4878) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:02,356] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:20:02,357] {logging_mixin.py:115} INFO - [2023-01-07 23:20:02,356] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:03,303] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:20:03,304] {logging_mixin.py:115} INFO - [2023-01-07 23:20:03,304] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:20:03,305] {logging_mixin.py:115} INFO - [2023-01-07 23:20:03,304] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:20:03,312] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:03,336] {logging_mixin.py:115} INFO - [2023-01-07 23:20:03,336] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:20:03,358] {logging_mixin.py:115} INFO - [2023-01-07 23:20:03,358] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:20:03,368] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.017 seconds
[2023-01-07 23:20:33,442] {processor.py:153} INFO - Started process (PID=4897) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:33,443] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:20:33,444] {logging_mixin.py:115} INFO - [2023-01-07 23:20:33,444] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:34,390] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:20:34,391] {logging_mixin.py:115} INFO - [2023-01-07 23:20:34,391] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:20:34,392] {logging_mixin.py:115} INFO - [2023-01-07 23:20:34,392] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:20:34,404] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:20:34,436] {logging_mixin.py:115} INFO - [2023-01-07 23:20:34,436] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:20:34,466] {logging_mixin.py:115} INFO - [2023-01-07 23:20:34,466] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:20:34,478] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.041 seconds
[2023-01-07 23:21:04,560] {processor.py:153} INFO - Started process (PID=4923) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:04,560] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:21:04,561] {logging_mixin.py:115} INFO - [2023-01-07 23:21:04,561] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:05,451] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:21:05,453] {logging_mixin.py:115} INFO - [2023-01-07 23:21:05,452] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:21:05,453] {logging_mixin.py:115} INFO - [2023-01-07 23:21:05,453] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:21:05,460] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:05,487] {logging_mixin.py:115} INFO - [2023-01-07 23:21:05,486] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:21:05,515] {logging_mixin.py:115} INFO - [2023-01-07 23:21:05,515] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:21:05,525] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-07 23:21:35,603] {processor.py:153} INFO - Started process (PID=4948) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:35,605] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:21:35,606] {logging_mixin.py:115} INFO - [2023-01-07 23:21:35,606] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:36,572] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:21:36,573] {logging_mixin.py:115} INFO - [2023-01-07 23:21:36,573] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:21:36,573] {logging_mixin.py:115} INFO - [2023-01-07 23:21:36,573] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:21:36,581] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:21:36,604] {logging_mixin.py:115} INFO - [2023-01-07 23:21:36,604] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:21:36,625] {logging_mixin.py:115} INFO - [2023-01-07 23:21:36,625] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:21:36,635] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.036 seconds
[2023-01-07 23:22:06,705] {processor.py:153} INFO - Started process (PID=4975) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:06,706] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:22:06,707] {logging_mixin.py:115} INFO - [2023-01-07 23:22:06,707] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:07,725] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:22:07,726] {logging_mixin.py:115} INFO - [2023-01-07 23:22:07,726] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:22:07,727] {logging_mixin.py:115} INFO - [2023-01-07 23:22:07,727] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:22:07,734] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:07,759] {logging_mixin.py:115} INFO - [2023-01-07 23:22:07,759] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:22:07,781] {logging_mixin.py:115} INFO - [2023-01-07 23:22:07,781] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:22:07,791] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.092 seconds
[2023-01-07 23:22:37,860] {processor.py:153} INFO - Started process (PID=4994) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:37,861] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:22:37,862] {logging_mixin.py:115} INFO - [2023-01-07 23:22:37,862] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:38,776] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:22:38,777] {logging_mixin.py:115} INFO - [2023-01-07 23:22:38,777] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:22:38,778] {logging_mixin.py:115} INFO - [2023-01-07 23:22:38,777] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:22:38,785] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:22:38,808] {logging_mixin.py:115} INFO - [2023-01-07 23:22:38,807] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:22:38,829] {logging_mixin.py:115} INFO - [2023-01-07 23:22:38,829] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:22:38,839] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-07 23:23:08,933] {processor.py:153} INFO - Started process (PID=5019) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:08,934] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:23:08,934] {logging_mixin.py:115} INFO - [2023-01-07 23:23:08,934] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:09,830] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:23:09,832] {logging_mixin.py:115} INFO - [2023-01-07 23:23:09,832] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:23:09,832] {logging_mixin.py:115} INFO - [2023-01-07 23:23:09,832] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:23:09,841] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:09,864] {logging_mixin.py:115} INFO - [2023-01-07 23:23:09,863] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:23:09,884] {logging_mixin.py:115} INFO - [2023-01-07 23:23:09,884] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:23:09,894] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.966 seconds
[2023-01-07 23:23:39,964] {processor.py:153} INFO - Started process (PID=5043) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:39,965] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:23:39,966] {logging_mixin.py:115} INFO - [2023-01-07 23:23:39,966] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:40,871] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:23:40,872] {logging_mixin.py:115} INFO - [2023-01-07 23:23:40,872] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:23:40,873] {logging_mixin.py:115} INFO - [2023-01-07 23:23:40,872] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:23:40,880] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:23:40,907] {logging_mixin.py:115} INFO - [2023-01-07 23:23:40,907] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:23:40,931] {logging_mixin.py:115} INFO - [2023-01-07 23:23:40,931] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:23:40,942] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-07 23:24:11,036] {processor.py:153} INFO - Started process (PID=5068) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:11,037] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:24:11,037] {logging_mixin.py:115} INFO - [2023-01-07 23:24:11,037] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:11,957] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:24:11,958] {logging_mixin.py:115} INFO - [2023-01-07 23:24:11,958] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:24:11,958] {logging_mixin.py:115} INFO - [2023-01-07 23:24:11,958] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:24:11,966] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:11,988] {logging_mixin.py:115} INFO - [2023-01-07 23:24:11,988] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:24:12,010] {logging_mixin.py:115} INFO - [2023-01-07 23:24:12,009] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:24:12,019] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.988 seconds
[2023-01-07 23:24:42,112] {processor.py:153} INFO - Started process (PID=5086) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:42,113] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:24:42,114] {logging_mixin.py:115} INFO - [2023-01-07 23:24:42,113] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:43,225] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:24:43,226] {logging_mixin.py:115} INFO - [2023-01-07 23:24:43,226] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:24:43,227] {logging_mixin.py:115} INFO - [2023-01-07 23:24:43,227] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:24:43,234] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:24:43,257] {logging_mixin.py:115} INFO - [2023-01-07 23:24:43,257] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:24:43,278] {logging_mixin.py:115} INFO - [2023-01-07 23:24:43,277] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:24:43,287] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.180 seconds
[2023-01-07 23:25:13,357] {processor.py:153} INFO - Started process (PID=5111) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:13,357] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:25:13,358] {logging_mixin.py:115} INFO - [2023-01-07 23:25:13,358] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:14,282] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:25:14,284] {logging_mixin.py:115} INFO - [2023-01-07 23:25:14,284] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:25:14,284] {logging_mixin.py:115} INFO - [2023-01-07 23:25:14,284] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:25:14,294] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:14,317] {logging_mixin.py:115} INFO - [2023-01-07 23:25:14,317] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:25:14,338] {logging_mixin.py:115} INFO - [2023-01-07 23:25:14,338] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:25:14,348] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.996 seconds
[2023-01-07 23:25:44,433] {processor.py:153} INFO - Started process (PID=5138) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:44,434] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:25:44,435] {logging_mixin.py:115} INFO - [2023-01-07 23:25:44,435] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:45,336] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:25:45,338] {logging_mixin.py:115} INFO - [2023-01-07 23:25:45,337] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:25:45,338] {logging_mixin.py:115} INFO - [2023-01-07 23:25:45,338] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:25:45,345] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:25:45,368] {logging_mixin.py:115} INFO - [2023-01-07 23:25:45,368] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:25:45,388] {logging_mixin.py:115} INFO - [2023-01-07 23:25:45,388] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:25:45,398] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-07 23:26:15,471] {processor.py:153} INFO - Started process (PID=5163) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:15,472] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:26:15,473] {logging_mixin.py:115} INFO - [2023-01-07 23:26:15,473] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:16,699] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:26:16,700] {logging_mixin.py:115} INFO - [2023-01-07 23:26:16,700] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:26:16,701] {logging_mixin.py:115} INFO - [2023-01-07 23:26:16,701] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:26:16,708] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:16,738] {logging_mixin.py:115} INFO - [2023-01-07 23:26:16,737] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:26:16,767] {logging_mixin.py:115} INFO - [2023-01-07 23:26:16,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:26:16,779] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.314 seconds
[2023-01-07 23:26:46,851] {processor.py:153} INFO - Started process (PID=5181) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:46,853] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:26:46,854] {logging_mixin.py:115} INFO - [2023-01-07 23:26:46,854] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:47,757] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:26:47,758] {logging_mixin.py:115} INFO - [2023-01-07 23:26:47,758] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:26:47,759] {logging_mixin.py:115} INFO - [2023-01-07 23:26:47,759] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:26:47,766] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:26:47,789] {logging_mixin.py:115} INFO - [2023-01-07 23:26:47,789] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:26:47,810] {logging_mixin.py:115} INFO - [2023-01-07 23:26:47,810] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:26:47,820] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-07 23:27:17,890] {processor.py:153} INFO - Started process (PID=5207) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:17,893] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:27:17,894] {logging_mixin.py:115} INFO - [2023-01-07 23:27:17,894] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:18,779] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:27:18,780] {logging_mixin.py:115} INFO - [2023-01-07 23:27:18,780] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:27:18,780] {logging_mixin.py:115} INFO - [2023-01-07 23:27:18,780] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:27:18,787] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:18,812] {logging_mixin.py:115} INFO - [2023-01-07 23:27:18,811] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:27:18,833] {logging_mixin.py:115} INFO - [2023-01-07 23:27:18,832] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:27:18,842] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-07 23:27:48,915] {processor.py:153} INFO - Started process (PID=5233) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:48,916] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:27:48,917] {logging_mixin.py:115} INFO - [2023-01-07 23:27:48,917] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:49,859] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:27:49,860] {logging_mixin.py:115} INFO - [2023-01-07 23:27:49,860] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:27:49,861] {logging_mixin.py:115} INFO - [2023-01-07 23:27:49,860] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:27:49,868] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:27:49,891] {logging_mixin.py:115} INFO - [2023-01-07 23:27:49,891] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:27:49,915] {logging_mixin.py:115} INFO - [2023-01-07 23:27:49,915] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:27:49,924] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.014 seconds
[2023-01-07 23:28:20,003] {processor.py:153} INFO - Started process (PID=5258) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:20,004] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:28:20,005] {logging_mixin.py:115} INFO - [2023-01-07 23:28:20,005] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:20,921] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:28:20,922] {logging_mixin.py:115} INFO - [2023-01-07 23:28:20,922] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:28:20,922] {logging_mixin.py:115} INFO - [2023-01-07 23:28:20,922] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:28:20,930] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:20,953] {logging_mixin.py:115} INFO - [2023-01-07 23:28:20,953] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:28:20,974] {logging_mixin.py:115} INFO - [2023-01-07 23:28:20,973] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:28:20,984] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.985 seconds
[2023-01-07 23:28:51,058] {processor.py:153} INFO - Started process (PID=5274) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:51,059] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:28:51,061] {logging_mixin.py:115} INFO - [2023-01-07 23:28:51,060] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:51,969] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:28:51,970] {logging_mixin.py:115} INFO - [2023-01-07 23:28:51,970] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:28:51,971] {logging_mixin.py:115} INFO - [2023-01-07 23:28:51,970] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:28:51,978] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:28:52,008] {logging_mixin.py:115} INFO - [2023-01-07 23:28:52,007] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:28:52,037] {logging_mixin.py:115} INFO - [2023-01-07 23:28:52,037] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:28:52,050] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.998 seconds
[2023-01-07 23:29:22,144] {processor.py:153} INFO - Started process (PID=5300) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:22,145] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:29:22,146] {logging_mixin.py:115} INFO - [2023-01-07 23:29:22,146] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:23,066] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:29:23,068] {logging_mixin.py:115} INFO - [2023-01-07 23:29:23,068] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:29:23,069] {logging_mixin.py:115} INFO - [2023-01-07 23:29:23,068] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:29:23,079] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:23,109] {logging_mixin.py:115} INFO - [2023-01-07 23:29:23,108] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:29:23,129] {logging_mixin.py:115} INFO - [2023-01-07 23:29:23,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:29:23,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.999 seconds
[2023-01-07 23:29:53,219] {processor.py:153} INFO - Started process (PID=5326) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:53,221] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:29:53,222] {logging_mixin.py:115} INFO - [2023-01-07 23:29:53,222] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:54,119] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:29:54,120] {logging_mixin.py:115} INFO - [2023-01-07 23:29:54,120] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:29:54,120] {logging_mixin.py:115} INFO - [2023-01-07 23:29:54,120] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:29:54,128] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:29:54,151] {logging_mixin.py:115} INFO - [2023-01-07 23:29:54,151] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:29:54,172] {logging_mixin.py:115} INFO - [2023-01-07 23:29:54,172] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:29:54,181] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-07 23:30:24,249] {processor.py:153} INFO - Started process (PID=5351) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:24,250] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:30:24,252] {logging_mixin.py:115} INFO - [2023-01-07 23:30:24,252] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:25,204] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:30:25,205] {logging_mixin.py:115} INFO - [2023-01-07 23:30:25,205] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:30:25,206] {logging_mixin.py:115} INFO - [2023-01-07 23:30:25,206] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:30:25,213] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:25,236] {logging_mixin.py:115} INFO - [2023-01-07 23:30:25,236] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:30:25,257] {logging_mixin.py:115} INFO - [2023-01-07 23:30:25,257] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:30:25,266] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.022 seconds
[2023-01-07 23:30:55,335] {processor.py:153} INFO - Started process (PID=5368) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:55,335] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:30:55,337] {logging_mixin.py:115} INFO - [2023-01-07 23:30:55,337] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:56,368] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:30:56,369] {logging_mixin.py:115} INFO - [2023-01-07 23:30:56,369] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:30:56,370] {logging_mixin.py:115} INFO - [2023-01-07 23:30:56,370] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:30:56,377] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:30:56,400] {logging_mixin.py:115} INFO - [2023-01-07 23:30:56,399] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:30:56,423] {logging_mixin.py:115} INFO - [2023-01-07 23:30:56,423] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:30:56,436] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.106 seconds
[2023-01-07 23:31:26,520] {processor.py:153} INFO - Started process (PID=5393) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:31:26,521] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-07 23:31:26,523] {logging_mixin.py:115} INFO - [2023-01-07 23:31:26,523] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:31:27,409] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-07 23:31:27,411] {logging_mixin.py:115} INFO - [2023-01-07 23:31:27,410] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-07 23:31:27,411] {logging_mixin.py:115} INFO - [2023-01-07 23:31:27,411] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-07 23:31:27,421] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-07 23:31:27,445] {logging_mixin.py:115} INFO - [2023-01-07 23:31:27,445] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-07 23:31:27,466] {logging_mixin.py:115} INFO - [2023-01-07 23:31:27,466] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-06T23:15:00+00:00, run_after=2023-01-09T23:15:00+00:00
[2023-01-07 23:31:27,476] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.960 seconds
