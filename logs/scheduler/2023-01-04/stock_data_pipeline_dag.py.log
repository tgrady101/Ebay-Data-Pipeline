[2023-01-04 18:54:58,237] {processor.py:153} INFO - Started process (PID=782) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:54:58,237] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:54:58,238] {logging_mixin.py:115} INFO - [2023-01-04 18:54:58,238] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:54:59,018] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:54:59,084] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,084] {manager.py:508} INFO - Created Permission View: can delete on DAG:stock_data_pipeline_dag
[2023-01-04 18:54:59,094] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,094] {manager.py:508} INFO - Created Permission View: can read on DAG:stock_data_pipeline_dag
[2023-01-04 18:54:59,100] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,100] {manager.py:508} INFO - Created Permission View: can edit on DAG:stock_data_pipeline_dag
[2023-01-04 18:54:59,101] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,100] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:54:59,108] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,107] {dag.py:2398} INFO - Creating ORM DAG for stock_data_pipeline_dag
[2023-01-04 18:54:59,119] {logging_mixin.py:115} INFO - [2023-01-04 18:54:59,119] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:54:59,130] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.898 seconds
[2023-01-04 18:55:29,220] {processor.py:153} INFO - Started process (PID=798) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:55:29,220] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:55:29,221] {logging_mixin.py:115} INFO - [2023-01-04 18:55:29,221] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:55:30,115] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:55:30,137] {logging_mixin.py:115} INFO - [2023-01-04 18:55:30,137] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:55:30,156] {logging_mixin.py:115} INFO - [2023-01-04 18:55:30,156] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:55:30,165] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.950 seconds
[2023-01-04 18:56:00,256] {processor.py:153} INFO - Started process (PID=820) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:00,264] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:56:00,265] {logging_mixin.py:115} INFO - [2023-01-04 18:56:00,265] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:01,037] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:01,060] {logging_mixin.py:115} INFO - [2023-01-04 18:56:01,059] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:56:01,080] {logging_mixin.py:115} INFO - [2023-01-04 18:56:01,079] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:56:01,089] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 18:56:31,153] {processor.py:153} INFO - Started process (PID=844) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:31,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:56:31,161] {logging_mixin.py:115} INFO - [2023-01-04 18:56:31,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:31,920] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:56:31,942] {logging_mixin.py:115} INFO - [2023-01-04 18:56:31,941] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:56:31,961] {logging_mixin.py:115} INFO - [2023-01-04 18:56:31,961] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:56:31,970] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.821 seconds
[2023-01-04 18:57:02,033] {processor.py:153} INFO - Started process (PID=869) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:02,035] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:57:02,035] {logging_mixin.py:115} INFO - [2023-01-04 18:57:02,035] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:02,812] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:02,833] {logging_mixin.py:115} INFO - [2023-01-04 18:57:02,833] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:57:02,853] {logging_mixin.py:115} INFO - [2023-01-04 18:57:02,853] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:57:02,862] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.834 seconds
[2023-01-04 18:57:32,929] {processor.py:153} INFO - Started process (PID=886) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:32,930] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:57:32,930] {logging_mixin.py:115} INFO - [2023-01-04 18:57:32,930] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:33,763] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:57:33,785] {logging_mixin.py:115} INFO - [2023-01-04 18:57:33,784] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:57:33,804] {logging_mixin.py:115} INFO - [2023-01-04 18:57:33,804] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:57:33,813] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.889 seconds
[2023-01-04 18:58:03,852] {processor.py:153} INFO - Started process (PID=909) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:03,853] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:58:03,854] {logging_mixin.py:115} INFO - [2023-01-04 18:58:03,853] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:04,606] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:04,627] {logging_mixin.py:115} INFO - [2023-01-04 18:58:04,626] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:58:04,646] {logging_mixin.py:115} INFO - [2023-01-04 18:58:04,646] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:58:04,655] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.808 seconds
[2023-01-04 18:58:34,746] {processor.py:153} INFO - Started process (PID=932) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:34,747] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:58:34,747] {logging_mixin.py:115} INFO - [2023-01-04 18:58:34,747] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:35,500] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:58:35,521] {logging_mixin.py:115} INFO - [2023-01-04 18:58:35,521] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:58:35,541] {logging_mixin.py:115} INFO - [2023-01-04 18:58:35,541] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:58:35,550] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.808 seconds
[2023-01-04 18:59:05,638] {processor.py:153} INFO - Started process (PID=955) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:05,643] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:59:05,644] {logging_mixin.py:115} INFO - [2023-01-04 18:59:05,644] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:06,427] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:06,449] {logging_mixin.py:115} INFO - [2023-01-04 18:59:06,449] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:59:06,468] {logging_mixin.py:115} INFO - [2023-01-04 18:59:06,468] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:59:06,477] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 18:59:36,547] {processor.py:153} INFO - Started process (PID=973) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:36,547] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 18:59:36,548] {logging_mixin.py:115} INFO - [2023-01-04 18:59:36,548] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:37,306] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 18:59:37,327] {logging_mixin.py:115} INFO - [2023-01-04 18:59:37,326] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 18:59:37,347] {logging_mixin.py:115} INFO - [2023-01-04 18:59:37,347] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 18:59:37,356] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.814 seconds
[2023-01-04 19:00:07,436] {processor.py:153} INFO - Started process (PID=996) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:07,438] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:00:07,438] {logging_mixin.py:115} INFO - [2023-01-04 19:00:07,438] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:08,193] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:08,213] {logging_mixin.py:115} INFO - [2023-01-04 19:00:08,213] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:00:08,232] {logging_mixin.py:115} INFO - [2023-01-04 19:00:08,232] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:00:08,241] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 19:00:38,302] {processor.py:153} INFO - Started process (PID=1019) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:38,302] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:00:38,303] {logging_mixin.py:115} INFO - [2023-01-04 19:00:38,303] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:39,054] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:00:39,075] {logging_mixin.py:115} INFO - [2023-01-04 19:00:39,074] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:00:39,094] {logging_mixin.py:115} INFO - [2023-01-04 19:00:39,094] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:00:39,102] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 19:01:09,196] {processor.py:153} INFO - Started process (PID=1044) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:09,198] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:01:09,198] {logging_mixin.py:115} INFO - [2023-01-04 19:01:09,198] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:09,984] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:10,004] {logging_mixin.py:115} INFO - [2023-01-04 19:01:10,004] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:01:10,024] {logging_mixin.py:115} INFO - [2023-01-04 19:01:10,023] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:01:10,032] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 19:01:40,121] {processor.py:153} INFO - Started process (PID=1060) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:40,122] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:01:40,123] {logging_mixin.py:115} INFO - [2023-01-04 19:01:40,123] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:40,865] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:01:40,885] {logging_mixin.py:115} INFO - [2023-01-04 19:01:40,885] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:01:40,904] {logging_mixin.py:115} INFO - [2023-01-04 19:01:40,904] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:01:40,913] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.796 seconds
[2023-01-04 19:02:11,006] {processor.py:153} INFO - Started process (PID=1082) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:11,009] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:02:11,010] {logging_mixin.py:115} INFO - [2023-01-04 19:02:11,010] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:11,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:11,780] {logging_mixin.py:115} INFO - [2023-01-04 19:02:11,779] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:02:11,799] {logging_mixin.py:115} INFO - [2023-01-04 19:02:11,799] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:02:11,808] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.808 seconds
[2023-01-04 19:02:41,872] {processor.py:153} INFO - Started process (PID=1104) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:41,873] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:02:41,873] {logging_mixin.py:115} INFO - [2023-01-04 19:02:41,873] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:42,652] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:02:42,673] {logging_mixin.py:115} INFO - [2023-01-04 19:02:42,672] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:02:42,694] {logging_mixin.py:115} INFO - [2023-01-04 19:02:42,693] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:02:42,702] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 19:03:12,765] {processor.py:153} INFO - Started process (PID=1127) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:12,769] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:03:12,769] {logging_mixin.py:115} INFO - [2023-01-04 19:03:12,769] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:13,558] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:13,582] {logging_mixin.py:115} INFO - [2023-01-04 19:03:13,582] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:03:13,608] {logging_mixin.py:115} INFO - [2023-01-04 19:03:13,608] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:03:13,620] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 19:03:43,687] {processor.py:153} INFO - Started process (PID=1143) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:43,688] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:03:43,689] {logging_mixin.py:115} INFO - [2023-01-04 19:03:43,689] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:44,465] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:03:44,486] {logging_mixin.py:115} INFO - [2023-01-04 19:03:44,486] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:03:44,505] {logging_mixin.py:115} INFO - [2023-01-04 19:03:44,505] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:03:44,514] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.831 seconds
[2023-01-04 19:04:14,605] {processor.py:153} INFO - Started process (PID=1165) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:14,606] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:04:14,606] {logging_mixin.py:115} INFO - [2023-01-04 19:04:14,606] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:15,442] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:15,463] {logging_mixin.py:115} INFO - [2023-01-04 19:04:15,462] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:04:15,482] {logging_mixin.py:115} INFO - [2023-01-04 19:04:15,481] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:04:15,490] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-04 19:04:45,579] {processor.py:153} INFO - Started process (PID=1188) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:45,580] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:04:45,581] {logging_mixin.py:115} INFO - [2023-01-04 19:04:45,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:46,333] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:04:46,354] {logging_mixin.py:115} INFO - [2023-01-04 19:04:46,354] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:04:46,373] {logging_mixin.py:115} INFO - [2023-01-04 19:04:46,373] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:04:46,382] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.807 seconds
[2023-01-04 19:05:16,485] {processor.py:153} INFO - Started process (PID=1211) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:16,486] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:05:16,486] {logging_mixin.py:115} INFO - [2023-01-04 19:05:16,486] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:17,253] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:17,274] {logging_mixin.py:115} INFO - [2023-01-04 19:05:17,274] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:05:17,293] {logging_mixin.py:115} INFO - [2023-01-04 19:05:17,293] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:05:17,301] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.821 seconds
[2023-01-04 19:05:47,365] {processor.py:153} INFO - Started process (PID=1228) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:47,367] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:05:47,367] {logging_mixin.py:115} INFO - [2023-01-04 19:05:47,367] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:48,148] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:05:48,170] {logging_mixin.py:115} INFO - [2023-01-04 19:05:48,169] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:05:48,189] {logging_mixin.py:115} INFO - [2023-01-04 19:05:48,189] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:05:48,198] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 19:06:18,262] {processor.py:153} INFO - Started process (PID=1252) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:18,262] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:06:18,263] {logging_mixin.py:115} INFO - [2023-01-04 19:06:18,263] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:19,032] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:19,053] {logging_mixin.py:115} INFO - [2023-01-04 19:06:19,053] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:06:19,074] {logging_mixin.py:115} INFO - [2023-01-04 19:06:19,074] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:06:19,085] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.828 seconds
[2023-01-04 19:06:49,148] {processor.py:153} INFO - Started process (PID=1275) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:49,148] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:06:49,149] {logging_mixin.py:115} INFO - [2023-01-04 19:06:49,149] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:49,929] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:06:49,950] {logging_mixin.py:115} INFO - [2023-01-04 19:06:49,950] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:06:49,971] {logging_mixin.py:115} INFO - [2023-01-04 19:06:49,971] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:06:49,980] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 19:07:20,050] {processor.py:153} INFO - Started process (PID=1299) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:20,052] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:07:20,052] {logging_mixin.py:115} INFO - [2023-01-04 19:07:20,052] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:20,976] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:20,999] {logging_mixin.py:115} INFO - [2023-01-04 19:07:20,999] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:07:21,018] {logging_mixin.py:115} INFO - [2023-01-04 19:07:21,018] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:07:21,027] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.982 seconds
[2023-01-04 19:07:51,113] {processor.py:153} INFO - Started process (PID=1315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:51,114] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:07:51,115] {logging_mixin.py:115} INFO - [2023-01-04 19:07:51,115] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:51,887] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:07:51,907] {logging_mixin.py:115} INFO - [2023-01-04 19:07:51,907] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:07:51,927] {logging_mixin.py:115} INFO - [2023-01-04 19:07:51,926] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:07:51,936] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.827 seconds
[2023-01-04 19:08:22,026] {processor.py:153} INFO - Started process (PID=1340) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:22,027] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:08:22,028] {logging_mixin.py:115} INFO - [2023-01-04 19:08:22,028] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:22,788] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:22,809] {logging_mixin.py:115} INFO - [2023-01-04 19:08:22,809] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:08:22,828] {logging_mixin.py:115} INFO - [2023-01-04 19:08:22,828] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:08:22,836] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.815 seconds
[2023-01-04 19:08:52,926] {processor.py:153} INFO - Started process (PID=1364) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:52,927] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:08:52,927] {logging_mixin.py:115} INFO - [2023-01-04 19:08:52,927] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:53,735] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:08:53,756] {logging_mixin.py:115} INFO - [2023-01-04 19:08:53,756] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:08:53,775] {logging_mixin.py:115} INFO - [2023-01-04 19:08:53,775] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:08:53,786] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 19:09:23,888] {processor.py:153} INFO - Started process (PID=1387) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:23,889] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:09:23,890] {logging_mixin.py:115} INFO - [2023-01-04 19:09:23,890] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:24,705] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:24,726] {logging_mixin.py:115} INFO - [2023-01-04 19:09:24,726] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:09:24,745] {logging_mixin.py:115} INFO - [2023-01-04 19:09:24,745] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:09:24,754] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.872 seconds
[2023-01-04 19:09:54,826] {processor.py:153} INFO - Started process (PID=1402) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:54,827] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:09:54,828] {logging_mixin.py:115} INFO - [2023-01-04 19:09:54,828] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:55,578] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:09:55,599] {logging_mixin.py:115} INFO - [2023-01-04 19:09:55,598] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:09:55,618] {logging_mixin.py:115} INFO - [2023-01-04 19:09:55,618] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:09:55,627] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 19:10:25,712] {processor.py:153} INFO - Started process (PID=1426) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:25,713] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:10:25,714] {logging_mixin.py:115} INFO - [2023-01-04 19:10:25,714] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:26,464] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:26,489] {logging_mixin.py:115} INFO - [2023-01-04 19:10:26,488] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:10:26,511] {logging_mixin.py:115} INFO - [2023-01-04 19:10:26,511] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:10:26,519] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.812 seconds
[2023-01-04 19:10:56,581] {processor.py:153} INFO - Started process (PID=1449) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:56,582] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:10:56,583] {logging_mixin.py:115} INFO - [2023-01-04 19:10:56,583] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:57,335] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:10:57,359] {logging_mixin.py:115} INFO - [2023-01-04 19:10:57,359] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:10:57,378] {logging_mixin.py:115} INFO - [2023-01-04 19:10:57,378] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:10:57,387] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.811 seconds
[2023-01-04 19:11:27,483] {processor.py:153} INFO - Started process (PID=1470) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:27,486] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:11:27,487] {logging_mixin.py:115} INFO - [2023-01-04 19:11:27,487] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:28,319] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:28,339] {logging_mixin.py:115} INFO - [2023-01-04 19:11:28,339] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:11:28,359] {logging_mixin.py:115} INFO - [2023-01-04 19:11:28,358] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:11:28,367] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.889 seconds
[2023-01-04 19:11:58,456] {processor.py:153} INFO - Started process (PID=1486) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:58,457] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:11:58,458] {logging_mixin.py:115} INFO - [2023-01-04 19:11:58,458] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:59,248] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:11:59,269] {logging_mixin.py:115} INFO - [2023-01-04 19:11:59,268] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:11:59,288] {logging_mixin.py:115} INFO - [2023-01-04 19:11:59,288] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:11:59,297] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.845 seconds
[2023-01-04 19:12:29,388] {processor.py:153} INFO - Started process (PID=1509) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:12:29,389] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:12:29,390] {logging_mixin.py:115} INFO - [2023-01-04 19:12:29,390] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:12:30,140] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:12:30,161] {logging_mixin.py:115} INFO - [2023-01-04 19:12:30,161] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:12:30,180] {logging_mixin.py:115} INFO - [2023-01-04 19:12:30,180] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:12:30,188] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 19:13:00,251] {processor.py:153} INFO - Started process (PID=1532) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:00,252] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:13:00,253] {logging_mixin.py:115} INFO - [2023-01-04 19:13:00,253] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:01,009] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:01,029] {logging_mixin.py:115} INFO - [2023-01-04 19:13:01,029] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:13:01,048] {logging_mixin.py:115} INFO - [2023-01-04 19:13:01,048] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:13:01,057] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.810 seconds
[2023-01-04 19:13:31,122] {processor.py:153} INFO - Started process (PID=1555) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:31,123] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:13:31,124] {logging_mixin.py:115} INFO - [2023-01-04 19:13:31,124] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:32,084] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:13:32,106] {logging_mixin.py:115} INFO - [2023-01-04 19:13:32,106] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:13:32,126] {logging_mixin.py:115} INFO - [2023-01-04 19:13:32,126] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:13:32,135] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.018 seconds
[2023-01-04 19:14:02,193] {processor.py:153} INFO - Started process (PID=1572) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:02,194] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:14:02,195] {logging_mixin.py:115} INFO - [2023-01-04 19:14:02,195] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:02,951] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:02,980] {logging_mixin.py:115} INFO - [2023-01-04 19:14:02,979] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:14:03,008] {logging_mixin.py:115} INFO - [2023-01-04 19:14:03,008] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:14:03,018] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.829 seconds
[2023-01-04 19:14:33,115] {processor.py:153} INFO - Started process (PID=1594) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:33,116] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:14:33,116] {logging_mixin.py:115} INFO - [2023-01-04 19:14:33,116] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:33,877] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:14:33,899] {logging_mixin.py:115} INFO - [2023-01-04 19:14:33,899] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:14:33,919] {logging_mixin.py:115} INFO - [2023-01-04 19:14:33,918] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:14:33,927] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.817 seconds
[2023-01-04 19:15:04,028] {processor.py:153} INFO - Started process (PID=1616) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:04,029] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:15:04,030] {logging_mixin.py:115} INFO - [2023-01-04 19:15:04,030] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:04,833] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:04,860] {logging_mixin.py:115} INFO - [2023-01-04 19:15:04,860] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:15:04,880] {logging_mixin.py:115} INFO - [2023-01-04 19:15:04,880] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:15:04,889] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.866 seconds
[2023-01-04 19:15:34,979] {processor.py:153} INFO - Started process (PID=1638) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:34,979] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:15:34,980] {logging_mixin.py:115} INFO - [2023-01-04 19:15:34,980] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:35,742] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:15:35,763] {logging_mixin.py:115} INFO - [2023-01-04 19:15:35,763] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:15:35,782] {logging_mixin.py:115} INFO - [2023-01-04 19:15:35,782] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:15:35,790] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.817 seconds
[2023-01-04 19:16:05,855] {processor.py:153} INFO - Started process (PID=1654) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:05,859] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:16:05,859] {logging_mixin.py:115} INFO - [2023-01-04 19:16:05,859] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:06,633] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:06,659] {logging_mixin.py:115} INFO - [2023-01-04 19:16:06,659] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:16:06,687] {logging_mixin.py:115} INFO - [2023-01-04 19:16:06,687] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:16:06,699] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 19:16:36,762] {processor.py:153} INFO - Started process (PID=1678) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:36,763] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:16:36,763] {logging_mixin.py:115} INFO - [2023-01-04 19:16:36,763] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:37,588] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:16:37,616] {logging_mixin.py:115} INFO - [2023-01-04 19:16:37,616] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:16:37,638] {logging_mixin.py:115} INFO - [2023-01-04 19:16:37,637] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:16:37,647] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-04 19:17:07,708] {processor.py:153} INFO - Started process (PID=1701) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:07,709] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:17:07,710] {logging_mixin.py:115} INFO - [2023-01-04 19:17:07,710] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:08,476] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:08,498] {logging_mixin.py:115} INFO - [2023-01-04 19:17:08,498] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:17:08,517] {logging_mixin.py:115} INFO - [2023-01-04 19:17:08,517] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:17:08,527] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.825 seconds
[2023-01-04 19:17:38,594] {processor.py:153} INFO - Started process (PID=1723) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:38,595] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:17:38,596] {logging_mixin.py:115} INFO - [2023-01-04 19:17:38,596] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:39,364] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:17:39,386] {logging_mixin.py:115} INFO - [2023-01-04 19:17:39,386] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:17:39,405] {logging_mixin.py:115} INFO - [2023-01-04 19:17:39,405] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:17:39,414] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.825 seconds
[2023-01-04 19:18:09,508] {processor.py:153} INFO - Started process (PID=1739) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:09,508] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:18:09,509] {logging_mixin.py:115} INFO - [2023-01-04 19:18:09,509] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:10,483] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:10,512] {logging_mixin.py:115} INFO - [2023-01-04 19:18:10,512] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:18:10,540] {logging_mixin.py:115} INFO - [2023-01-04 19:18:10,540] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:18:10,551] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.049 seconds
[2023-01-04 19:18:40,620] {processor.py:153} INFO - Started process (PID=1763) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:40,623] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:18:40,624] {logging_mixin.py:115} INFO - [2023-01-04 19:18:40,624] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:41,372] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:18:41,393] {logging_mixin.py:115} INFO - [2023-01-04 19:18:41,392] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:18:41,412] {logging_mixin.py:115} INFO - [2023-01-04 19:18:41,412] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:18:41,420] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.807 seconds
[2023-01-04 19:19:11,509] {processor.py:153} INFO - Started process (PID=1786) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:11,510] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:19:11,511] {logging_mixin.py:115} INFO - [2023-01-04 19:19:11,511] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:12,255] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:12,276] {logging_mixin.py:115} INFO - [2023-01-04 19:19:12,276] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:19:12,295] {logging_mixin.py:115} INFO - [2023-01-04 19:19:12,295] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:19:12,304] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.800 seconds
[2023-01-04 19:19:42,394] {processor.py:153} INFO - Started process (PID=1808) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:42,395] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:19:42,396] {logging_mixin.py:115} INFO - [2023-01-04 19:19:42,396] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:43,183] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:19:43,204] {logging_mixin.py:115} INFO - [2023-01-04 19:19:43,204] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:19:43,223] {logging_mixin.py:115} INFO - [2023-01-04 19:19:43,223] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:19:43,232] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 19:20:13,333] {processor.py:153} INFO - Started process (PID=1823) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:13,334] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:20:13,335] {logging_mixin.py:115} INFO - [2023-01-04 19:20:13,335] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:14,105] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:14,126] {logging_mixin.py:115} INFO - [2023-01-04 19:20:14,126] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:20:14,147] {logging_mixin.py:115} INFO - [2023-01-04 19:20:14,147] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:20:14,158] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.830 seconds
[2023-01-04 19:20:44,221] {processor.py:153} INFO - Started process (PID=1846) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:44,222] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:20:44,223] {logging_mixin.py:115} INFO - [2023-01-04 19:20:44,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:44,961] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:20:44,982] {logging_mixin.py:115} INFO - [2023-01-04 19:20:44,982] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:20:45,002] {logging_mixin.py:115} INFO - [2023-01-04 19:20:45,002] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:20:45,011] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.794 seconds
[2023-01-04 19:21:15,074] {processor.py:153} INFO - Started process (PID=1869) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:15,075] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:21:15,076] {logging_mixin.py:115} INFO - [2023-01-04 19:21:15,076] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:15,867] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:15,889] {logging_mixin.py:115} INFO - [2023-01-04 19:21:15,888] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:21:15,908] {logging_mixin.py:115} INFO - [2023-01-04 19:21:15,908] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:21:15,917] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 19:21:45,979] {processor.py:153} INFO - Started process (PID=1892) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:45,979] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:21:45,980] {logging_mixin.py:115} INFO - [2023-01-04 19:21:45,980] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:46,738] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:21:46,759] {logging_mixin.py:115} INFO - [2023-01-04 19:21:46,758] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:21:46,778] {logging_mixin.py:115} INFO - [2023-01-04 19:21:46,778] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:21:46,786] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.812 seconds
[2023-01-04 19:22:16,883] {processor.py:153} INFO - Started process (PID=1907) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:16,885] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:22:16,885] {logging_mixin.py:115} INFO - [2023-01-04 19:22:16,885] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:17,657] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:17,679] {logging_mixin.py:115} INFO - [2023-01-04 19:22:17,679] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:22:17,699] {logging_mixin.py:115} INFO - [2023-01-04 19:22:17,699] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:22:17,707] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.829 seconds
[2023-01-04 19:22:47,797] {processor.py:153} INFO - Started process (PID=1930) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:47,798] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:22:47,799] {logging_mixin.py:115} INFO - [2023-01-04 19:22:47,799] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:48,647] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:22:48,668] {logging_mixin.py:115} INFO - [2023-01-04 19:22:48,668] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:22:48,687] {logging_mixin.py:115} INFO - [2023-01-04 19:22:48,687] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:22:48,696] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-04 19:23:18,790] {processor.py:153} INFO - Started process (PID=1952) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:18,791] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:23:18,791] {logging_mixin.py:115} INFO - [2023-01-04 19:23:18,791] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:19,540] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:19,561] {logging_mixin.py:115} INFO - [2023-01-04 19:23:19,560] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:23:19,580] {logging_mixin.py:115} INFO - [2023-01-04 19:23:19,580] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:23:19,589] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.804 seconds
[2023-01-04 19:23:49,652] {processor.py:153} INFO - Started process (PID=1975) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:49,653] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:23:49,653] {logging_mixin.py:115} INFO - [2023-01-04 19:23:49,653] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:50,392] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:23:50,413] {logging_mixin.py:115} INFO - [2023-01-04 19:23:50,412] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:23:50,432] {logging_mixin.py:115} INFO - [2023-01-04 19:23:50,432] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:23:50,440] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.793 seconds
[2023-01-04 19:24:20,502] {processor.py:153} INFO - Started process (PID=1991) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:20,503] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:24:20,503] {logging_mixin.py:115} INFO - [2023-01-04 19:24:20,503] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:21,279] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:21,308] {logging_mixin.py:115} INFO - [2023-01-04 19:24:21,308] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:24:21,329] {logging_mixin.py:115} INFO - [2023-01-04 19:24:21,329] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:24:21,338] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.841 seconds
[2023-01-04 19:24:51,399] {processor.py:153} INFO - Started process (PID=2014) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:51,400] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:24:51,401] {logging_mixin.py:115} INFO - [2023-01-04 19:24:51,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:52,152] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:24:52,173] {logging_mixin.py:115} INFO - [2023-01-04 19:24:52,173] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:24:52,192] {logging_mixin.py:115} INFO - [2023-01-04 19:24:52,192] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:24:52,204] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 19:25:22,297] {processor.py:153} INFO - Started process (PID=2037) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:22,299] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:25:22,299] {logging_mixin.py:115} INFO - [2023-01-04 19:25:22,299] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:23,077] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:23,098] {logging_mixin.py:115} INFO - [2023-01-04 19:25:23,098] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:25:23,117] {logging_mixin.py:115} INFO - [2023-01-04 19:25:23,117] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:25:23,125] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 19:25:53,221] {processor.py:153} INFO - Started process (PID=2059) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:53,223] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:25:53,223] {logging_mixin.py:115} INFO - [2023-01-04 19:25:53,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:53,995] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:25:54,016] {logging_mixin.py:115} INFO - [2023-01-04 19:25:54,016] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:25:54,035] {logging_mixin.py:115} INFO - [2023-01-04 19:25:54,035] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:25:54,044] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.828 seconds
[2023-01-04 19:26:24,126] {processor.py:153} INFO - Started process (PID=2076) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:24,127] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:26:24,127] {logging_mixin.py:115} INFO - [2023-01-04 19:26:24,127] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:24,894] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:24,915] {logging_mixin.py:115} INFO - [2023-01-04 19:26:24,915] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:26:24,935] {logging_mixin.py:115} INFO - [2023-01-04 19:26:24,935] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:26:24,944] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.823 seconds
[2023-01-04 19:26:55,013] {processor.py:153} INFO - Started process (PID=2100) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:55,014] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:26:55,014] {logging_mixin.py:115} INFO - [2023-01-04 19:26:55,014] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:55,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:26:55,781] {logging_mixin.py:115} INFO - [2023-01-04 19:26:55,781] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:26:55,801] {logging_mixin.py:115} INFO - [2023-01-04 19:26:55,800] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:26:55,810] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.802 seconds
[2023-01-04 19:27:25,875] {processor.py:153} INFO - Started process (PID=2124) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:25,875] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:27:25,876] {logging_mixin.py:115} INFO - [2023-01-04 19:27:25,876] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:26,628] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:26,650] {logging_mixin.py:115} INFO - [2023-01-04 19:27:26,649] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:27:26,669] {logging_mixin.py:115} INFO - [2023-01-04 19:27:26,669] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:27:26,677] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.807 seconds
[2023-01-04 19:27:56,722] {processor.py:153} INFO - Started process (PID=2147) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:56,723] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:27:56,724] {logging_mixin.py:115} INFO - [2023-01-04 19:27:56,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:57,489] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:27:57,511] {logging_mixin.py:115} INFO - [2023-01-04 19:27:57,510] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:27:57,530] {logging_mixin.py:115} INFO - [2023-01-04 19:27:57,530] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:27:57,539] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.821 seconds
[2023-01-04 19:28:27,629] {processor.py:153} INFO - Started process (PID=2164) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:27,631] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:28:27,631] {logging_mixin.py:115} INFO - [2023-01-04 19:28:27,631] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:28,383] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:28,404] {logging_mixin.py:115} INFO - [2023-01-04 19:28:28,404] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:28:28,424] {logging_mixin.py:115} INFO - [2023-01-04 19:28:28,424] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:28:28,433] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.808 seconds
[2023-01-04 19:28:58,521] {processor.py:153} INFO - Started process (PID=2187) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:58,522] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:28:58,523] {logging_mixin.py:115} INFO - [2023-01-04 19:28:58,523] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:59,287] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:28:59,309] {logging_mixin.py:115} INFO - [2023-01-04 19:28:59,309] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:28:59,330] {logging_mixin.py:115} INFO - [2023-01-04 19:28:59,330] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:28:59,338] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.821 seconds
[2023-01-04 19:29:29,403] {processor.py:153} INFO - Started process (PID=2211) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:29:29,403] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:29:29,404] {logging_mixin.py:115} INFO - [2023-01-04 19:29:29,404] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:29:30,162] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:29:30,183] {logging_mixin.py:115} INFO - [2023-01-04 19:29:30,183] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:29:30,202] {logging_mixin.py:115} INFO - [2023-01-04 19:29:30,202] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:29:30,211] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.813 seconds
[2023-01-04 19:30:00,272] {processor.py:153} INFO - Started process (PID=2234) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:00,274] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:30:00,274] {logging_mixin.py:115} INFO - [2023-01-04 19:30:00,274] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:01,082] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:01,103] {logging_mixin.py:115} INFO - [2023-01-04 19:30:01,102] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:30:01,122] {logging_mixin.py:115} INFO - [2023-01-04 19:30:01,122] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:30:01,130] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 19:30:31,194] {processor.py:153} INFO - Started process (PID=2250) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:31,196] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:30:31,197] {logging_mixin.py:115} INFO - [2023-01-04 19:30:31,196] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:31,983] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:30:32,005] {logging_mixin.py:115} INFO - [2023-01-04 19:30:32,004] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:30:32,024] {logging_mixin.py:115} INFO - [2023-01-04 19:30:32,023] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:30:32,032] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.842 seconds
[2023-01-04 19:31:02,077] {processor.py:153} INFO - Started process (PID=2275) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:02,078] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:31:02,079] {logging_mixin.py:115} INFO - [2023-01-04 19:31:02,079] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:02,863] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:02,884] {logging_mixin.py:115} INFO - [2023-01-04 19:31:02,883] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:31:02,903] {logging_mixin.py:115} INFO - [2023-01-04 19:31:02,903] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:31:02,912] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 19:31:33,004] {processor.py:153} INFO - Started process (PID=2299) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:33,005] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:31:33,005] {logging_mixin.py:115} INFO - [2023-01-04 19:31:33,005] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:33,798] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:31:33,819] {logging_mixin.py:115} INFO - [2023-01-04 19:31:33,818] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:31:33,838] {logging_mixin.py:115} INFO - [2023-01-04 19:31:33,838] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:31:33,847] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 19:32:03,939] {processor.py:153} INFO - Started process (PID=2315) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:03,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:32:03,940] {logging_mixin.py:115} INFO - [2023-01-04 19:32:03,940] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:04,803] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:04,828] {logging_mixin.py:115} INFO - [2023-01-04 19:32:04,827] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:32:04,848] {logging_mixin.py:115} INFO - [2023-01-04 19:32:04,848] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:32:04,857] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-04 19:32:34,947] {processor.py:153} INFO - Started process (PID=2339) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:34,951] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:32:34,952] {logging_mixin.py:115} INFO - [2023-01-04 19:32:34,952] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:35,699] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:32:35,720] {logging_mixin.py:115} INFO - [2023-01-04 19:32:35,720] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:32:35,739] {logging_mixin.py:115} INFO - [2023-01-04 19:32:35,739] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:32:35,748] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 19:33:05,828] {processor.py:153} INFO - Started process (PID=2363) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:05,829] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:33:05,830] {logging_mixin.py:115} INFO - [2023-01-04 19:33:05,829] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:06,619] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:06,640] {logging_mixin.py:115} INFO - [2023-01-04 19:33:06,640] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:33:06,660] {logging_mixin.py:115} INFO - [2023-01-04 19:33:06,660] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:33:06,668] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 19:33:36,734] {processor.py:153} INFO - Started process (PID=2387) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:36,735] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:33:36,736] {logging_mixin.py:115} INFO - [2023-01-04 19:33:36,736] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:37,525] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:33:37,549] {logging_mixin.py:115} INFO - [2023-01-04 19:33:37,548] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:33:37,570] {logging_mixin.py:115} INFO - [2023-01-04 19:33:37,570] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:33:37,580] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 19:34:07,649] {processor.py:153} INFO - Started process (PID=2403) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:07,650] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:34:07,651] {logging_mixin.py:115} INFO - [2023-01-04 19:34:07,651] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:08,451] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:08,472] {logging_mixin.py:115} INFO - [2023-01-04 19:34:08,471] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:34:08,491] {logging_mixin.py:115} INFO - [2023-01-04 19:34:08,491] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:34:08,500] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 19:34:38,568] {processor.py:153} INFO - Started process (PID=2426) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:38,569] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:34:38,569] {logging_mixin.py:115} INFO - [2023-01-04 19:34:38,569] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:39,346] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:34:39,367] {logging_mixin.py:115} INFO - [2023-01-04 19:34:39,367] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:34:39,386] {logging_mixin.py:115} INFO - [2023-01-04 19:34:39,386] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:34:39,394] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.831 seconds
[2023-01-04 19:35:09,512] {processor.py:153} INFO - Started process (PID=2449) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:09,513] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:35:09,514] {logging_mixin.py:115} INFO - [2023-01-04 19:35:09,514] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:10,266] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:10,287] {logging_mixin.py:115} INFO - [2023-01-04 19:35:10,286] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:35:10,306] {logging_mixin.py:115} INFO - [2023-01-04 19:35:10,305] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:35:10,315] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.807 seconds
[2023-01-04 19:35:40,410] {processor.py:153} INFO - Started process (PID=2473) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:40,410] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:35:40,411] {logging_mixin.py:115} INFO - [2023-01-04 19:35:40,411] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:41,162] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:35:41,184] {logging_mixin.py:115} INFO - [2023-01-04 19:35:41,183] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:35:41,203] {logging_mixin.py:115} INFO - [2023-01-04 19:35:41,202] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:35:41,211] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.806 seconds
[2023-01-04 19:36:11,300] {processor.py:153} INFO - Started process (PID=2489) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:11,304] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:36:11,305] {logging_mixin.py:115} INFO - [2023-01-04 19:36:11,305] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:12,181] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:12,202] {logging_mixin.py:115} INFO - [2023-01-04 19:36:12,202] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:36:12,222] {logging_mixin.py:115} INFO - [2023-01-04 19:36:12,221] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:36:12,230] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.936 seconds
[2023-01-04 19:36:42,316] {processor.py:153} INFO - Started process (PID=2512) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:42,316] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:36:42,317] {logging_mixin.py:115} INFO - [2023-01-04 19:36:42,317] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:43,083] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:36:43,103] {logging_mixin.py:115} INFO - [2023-01-04 19:36:43,103] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:36:43,122] {logging_mixin.py:115} INFO - [2023-01-04 19:36:43,122] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:36:43,131] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.820 seconds
[2023-01-04 19:37:13,195] {processor.py:153} INFO - Started process (PID=2536) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:13,196] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:37:13,197] {logging_mixin.py:115} INFO - [2023-01-04 19:37:13,197] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:13,974] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:13,995] {logging_mixin.py:115} INFO - [2023-01-04 19:37:13,995] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:37:14,015] {logging_mixin.py:115} INFO - [2023-01-04 19:37:14,014] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:37:14,023] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 19:37:44,086] {processor.py:153} INFO - Started process (PID=2558) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:44,087] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:37:44,087] {logging_mixin.py:115} INFO - [2023-01-04 19:37:44,087] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:44,856] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:37:44,876] {logging_mixin.py:115} INFO - [2023-01-04 19:37:44,876] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:37:44,895] {logging_mixin.py:115} INFO - [2023-01-04 19:37:44,895] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:37:44,904] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.823 seconds
[2023-01-04 19:38:14,966] {processor.py:153} INFO - Started process (PID=2574) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:14,967] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:38:14,968] {logging_mixin.py:115} INFO - [2023-01-04 19:38:14,968] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:15,719] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:15,742] {logging_mixin.py:115} INFO - [2023-01-04 19:38:15,741] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:38:15,761] {logging_mixin.py:115} INFO - [2023-01-04 19:38:15,761] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:38:15,770] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 19:38:45,869] {processor.py:153} INFO - Started process (PID=2599) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:45,869] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:38:45,870] {logging_mixin.py:115} INFO - [2023-01-04 19:38:45,870] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:46,613] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:38:46,635] {logging_mixin.py:115} INFO - [2023-01-04 19:38:46,634] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:38:46,654] {logging_mixin.py:115} INFO - [2023-01-04 19:38:46,654] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:38:46,662] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.799 seconds
[2023-01-04 19:39:16,753] {processor.py:153} INFO - Started process (PID=2623) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:16,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:39:16,757] {logging_mixin.py:115} INFO - [2023-01-04 19:39:16,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:17,534] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:17,555] {logging_mixin.py:115} INFO - [2023-01-04 19:39:17,554] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:39:17,574] {logging_mixin.py:115} INFO - [2023-01-04 19:39:17,574] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:39:17,583] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 19:39:47,660] {processor.py:153} INFO - Started process (PID=2647) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:47,660] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:39:47,661] {logging_mixin.py:115} INFO - [2023-01-04 19:39:47,661] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:48,414] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:39:48,436] {logging_mixin.py:115} INFO - [2023-01-04 19:39:48,435] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:39:48,455] {logging_mixin.py:115} INFO - [2023-01-04 19:39:48,454] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:39:48,463] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.808 seconds
[2023-01-04 19:40:18,537] {processor.py:153} INFO - Started process (PID=2663) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:18,541] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:40:18,541] {logging_mixin.py:115} INFO - [2023-01-04 19:40:18,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:19,332] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:19,360] {logging_mixin.py:115} INFO - [2023-01-04 19:40:19,360] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:40:19,385] {logging_mixin.py:115} INFO - [2023-01-04 19:40:19,385] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:40:19,395] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 19:40:49,460] {processor.py:153} INFO - Started process (PID=2687) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:49,461] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:40:49,461] {logging_mixin.py:115} INFO - [2023-01-04 19:40:49,461] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:50,260] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:40:50,290] {logging_mixin.py:115} INFO - [2023-01-04 19:40:50,290] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:40:50,320] {logging_mixin.py:115} INFO - [2023-01-04 19:40:50,319] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:40:50,332] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.876 seconds
[2023-01-04 19:41:20,394] {processor.py:153} INFO - Started process (PID=2710) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:20,396] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:41:20,396] {logging_mixin.py:115} INFO - [2023-01-04 19:41:20,396] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:21,204] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:21,225] {logging_mixin.py:115} INFO - [2023-01-04 19:41:21,225] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:41:21,245] {logging_mixin.py:115} INFO - [2023-01-04 19:41:21,244] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:41:21,253] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 19:41:51,344] {processor.py:153} INFO - Started process (PID=2735) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:51,346] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:41:51,346] {logging_mixin.py:115} INFO - [2023-01-04 19:41:51,346] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:52,162] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:41:52,183] {logging_mixin.py:115} INFO - [2023-01-04 19:41:52,183] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:41:52,202] {logging_mixin.py:115} INFO - [2023-01-04 19:41:52,202] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:41:52,211] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.871 seconds
[2023-01-04 19:42:22,308] {processor.py:153} INFO - Started process (PID=2750) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:22,309] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:42:22,310] {logging_mixin.py:115} INFO - [2023-01-04 19:42:22,310] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:23,180] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:23,201] {logging_mixin.py:115} INFO - [2023-01-04 19:42:23,201] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:42:23,221] {logging_mixin.py:115} INFO - [2023-01-04 19:42:23,221] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:42:23,230] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-04 19:42:53,323] {processor.py:153} INFO - Started process (PID=2773) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:53,324] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:42:53,325] {logging_mixin.py:115} INFO - [2023-01-04 19:42:53,324] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:54,123] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:42:54,144] {logging_mixin.py:115} INFO - [2023-01-04 19:42:54,144] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:42:54,164] {logging_mixin.py:115} INFO - [2023-01-04 19:42:54,164] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:42:54,173] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 19:43:24,261] {processor.py:153} INFO - Started process (PID=2796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:24,263] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:43:24,263] {logging_mixin.py:115} INFO - [2023-01-04 19:43:24,263] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:25,033] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:25,055] {logging_mixin.py:115} INFO - [2023-01-04 19:43:25,054] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:43:25,074] {logging_mixin.py:115} INFO - [2023-01-04 19:43:25,074] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:43:25,083] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.826 seconds
[2023-01-04 19:43:55,160] {processor.py:153} INFO - Started process (PID=2811) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:55,162] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:43:55,162] {logging_mixin.py:115} INFO - [2023-01-04 19:43:55,162] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:56,075] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:43:56,102] {logging_mixin.py:115} INFO - [2023-01-04 19:43:56,102] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:43:56,130] {logging_mixin.py:115} INFO - [2023-01-04 19:43:56,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:43:56,140] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-04 19:44:26,228] {processor.py:153} INFO - Started process (PID=2833) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:26,228] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:44:26,229] {logging_mixin.py:115} INFO - [2023-01-04 19:44:26,229] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:26,983] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:27,005] {logging_mixin.py:115} INFO - [2023-01-04 19:44:27,005] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:44:27,024] {logging_mixin.py:115} INFO - [2023-01-04 19:44:27,024] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:44:27,033] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 19:44:57,128] {processor.py:153} INFO - Started process (PID=2857) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:57,129] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:44:57,130] {logging_mixin.py:115} INFO - [2023-01-04 19:44:57,130] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:57,902] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:44:57,924] {logging_mixin.py:115} INFO - [2023-01-04 19:44:57,924] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:44:57,943] {logging_mixin.py:115} INFO - [2023-01-04 19:44:57,943] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:44:57,952] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.829 seconds
[2023-01-04 19:45:28,052] {processor.py:153} INFO - Started process (PID=2880) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:28,052] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:45:28,053] {logging_mixin.py:115} INFO - [2023-01-04 19:45:28,053] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:28,835] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:28,857] {logging_mixin.py:115} INFO - [2023-01-04 19:45:28,857] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:45:28,877] {logging_mixin.py:115} INFO - [2023-01-04 19:45:28,876] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:45:28,886] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 19:45:58,972] {processor.py:153} INFO - Started process (PID=2896) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:58,974] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:45:58,974] {logging_mixin.py:115} INFO - [2023-01-04 19:45:58,974] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:59,887] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:45:59,909] {logging_mixin.py:115} INFO - [2023-01-04 19:45:59,909] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:45:59,929] {logging_mixin.py:115} INFO - [2023-01-04 19:45:59,929] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:45:59,938] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.970 seconds
[2023-01-04 19:46:30,016] {processor.py:153} INFO - Started process (PID=2919) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:46:30,017] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:46:30,018] {logging_mixin.py:115} INFO - [2023-01-04 19:46:30,018] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:46:30,809] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:46:30,830] {logging_mixin.py:115} INFO - [2023-01-04 19:46:30,829] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:46:30,850] {logging_mixin.py:115} INFO - [2023-01-04 19:46:30,849] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:46:30,858] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 19:47:00,924] {processor.py:153} INFO - Started process (PID=2942) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:00,925] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:47:00,926] {logging_mixin.py:115} INFO - [2023-01-04 19:47:00,926] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:01,690] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:01,711] {logging_mixin.py:115} INFO - [2023-01-04 19:47:01,710] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:47:01,730] {logging_mixin.py:115} INFO - [2023-01-04 19:47:01,730] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:47:01,739] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.820 seconds
[2023-01-04 19:47:31,804] {processor.py:153} INFO - Started process (PID=2965) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:31,805] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:47:31,805] {logging_mixin.py:115} INFO - [2023-01-04 19:47:31,805] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:32,574] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:47:32,600] {logging_mixin.py:115} INFO - [2023-01-04 19:47:32,599] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:47:32,619] {logging_mixin.py:115} INFO - [2023-01-04 19:47:32,619] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:47:32,628] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.829 seconds
[2023-01-04 19:48:02,691] {processor.py:153} INFO - Started process (PID=2982) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:02,693] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:48:02,693] {logging_mixin.py:115} INFO - [2023-01-04 19:48:02,693] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:03,541] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:03,572] {logging_mixin.py:115} INFO - [2023-01-04 19:48:03,572] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:48:03,594] {logging_mixin.py:115} INFO - [2023-01-04 19:48:03,593] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:48:03,603] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-04 19:48:33,649] {processor.py:153} INFO - Started process (PID=3005) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:33,650] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:48:33,651] {logging_mixin.py:115} INFO - [2023-01-04 19:48:33,650] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:34,404] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:48:34,425] {logging_mixin.py:115} INFO - [2023-01-04 19:48:34,424] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:48:34,444] {logging_mixin.py:115} INFO - [2023-01-04 19:48:34,444] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:48:34,453] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 19:49:04,546] {processor.py:153} INFO - Started process (PID=3028) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:04,546] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:49:04,547] {logging_mixin.py:115} INFO - [2023-01-04 19:49:04,547] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:05,324] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:05,345] {logging_mixin.py:115} INFO - [2023-01-04 19:49:05,345] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:49:05,365] {logging_mixin.py:115} INFO - [2023-01-04 19:49:05,365] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:49:05,374] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 19:49:35,463] {processor.py:153} INFO - Started process (PID=3051) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:35,474] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:49:35,475] {logging_mixin.py:115} INFO - [2023-01-04 19:49:35,475] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:36,313] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:49:36,335] {logging_mixin.py:115} INFO - [2023-01-04 19:49:36,334] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:49:36,354] {logging_mixin.py:115} INFO - [2023-01-04 19:49:36,354] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:49:36,363] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.906 seconds
[2023-01-04 19:50:06,462] {processor.py:153} INFO - Started process (PID=3068) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:06,464] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:50:06,464] {logging_mixin.py:115} INFO - [2023-01-04 19:50:06,464] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:07,225] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:07,246] {logging_mixin.py:115} INFO - [2023-01-04 19:50:07,246] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:50:07,265] {logging_mixin.py:115} INFO - [2023-01-04 19:50:07,265] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:50:07,274] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.817 seconds
[2023-01-04 19:50:37,338] {processor.py:153} INFO - Started process (PID=3092) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:37,339] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:50:37,339] {logging_mixin.py:115} INFO - [2023-01-04 19:50:37,339] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:38,086] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:50:38,108] {logging_mixin.py:115} INFO - [2023-01-04 19:50:38,108] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:50:38,127] {logging_mixin.py:115} INFO - [2023-01-04 19:50:38,127] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:50:38,136] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.803 seconds
[2023-01-04 19:51:08,201] {processor.py:153} INFO - Started process (PID=3116) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:08,202] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:51:08,203] {logging_mixin.py:115} INFO - [2023-01-04 19:51:08,203] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:09,008] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:09,031] {logging_mixin.py:115} INFO - [2023-01-04 19:51:09,030] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:51:09,050] {logging_mixin.py:115} INFO - [2023-01-04 19:51:09,050] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:51:09,058] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 19:51:39,130] {processor.py:153} INFO - Started process (PID=3140) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:39,130] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:51:39,131] {logging_mixin.py:115} INFO - [2023-01-04 19:51:39,131] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:39,938] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:51:39,960] {logging_mixin.py:115} INFO - [2023-01-04 19:51:39,960] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:51:39,979] {logging_mixin.py:115} INFO - [2023-01-04 19:51:39,979] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:51:39,988] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 19:52:10,033] {processor.py:153} INFO - Started process (PID=3157) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:10,035] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:52:10,035] {logging_mixin.py:115} INFO - [2023-01-04 19:52:10,035] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:10,805] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:10,827] {logging_mixin.py:115} INFO - [2023-01-04 19:52:10,826] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:52:10,846] {logging_mixin.py:115} INFO - [2023-01-04 19:52:10,846] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:52:10,855] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.826 seconds
[2023-01-04 19:52:40,950] {processor.py:153} INFO - Started process (PID=3180) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:40,950] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:52:40,951] {logging_mixin.py:115} INFO - [2023-01-04 19:52:40,951] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:41,804] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:52:41,825] {logging_mixin.py:115} INFO - [2023-01-04 19:52:41,824] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:52:41,843] {logging_mixin.py:115} INFO - [2023-01-04 19:52:41,843] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:52:41,852] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.907 seconds
[2023-01-04 19:53:11,941] {processor.py:153} INFO - Started process (PID=3203) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:11,943] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:53:11,943] {logging_mixin.py:115} INFO - [2023-01-04 19:53:11,943] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:12,707] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:12,728] {logging_mixin.py:115} INFO - [2023-01-04 19:53:12,728] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:53:12,747] {logging_mixin.py:115} INFO - [2023-01-04 19:53:12,747] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:53:12,756] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.819 seconds
[2023-01-04 19:53:42,846] {processor.py:153} INFO - Started process (PID=3226) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:42,847] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:53:42,848] {logging_mixin.py:115} INFO - [2023-01-04 19:53:42,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:43,667] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:53:43,689] {logging_mixin.py:115} INFO - [2023-01-04 19:53:43,688] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:53:43,709] {logging_mixin.py:115} INFO - [2023-01-04 19:53:43,709] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:53:43,718] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.877 seconds
[2023-01-04 19:54:13,807] {processor.py:153} INFO - Started process (PID=3243) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:13,808] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:54:13,808] {logging_mixin.py:115} INFO - [2023-01-04 19:54:13,808] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:14,606] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:14,631] {logging_mixin.py:115} INFO - [2023-01-04 19:54:14,630] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:54:14,651] {logging_mixin.py:115} INFO - [2023-01-04 19:54:14,650] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:54:14,659] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.857 seconds
[2023-01-04 19:54:44,724] {processor.py:153} INFO - Started process (PID=3265) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:44,726] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:54:44,727] {logging_mixin.py:115} INFO - [2023-01-04 19:54:44,726] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:45,494] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:54:45,515] {logging_mixin.py:115} INFO - [2023-01-04 19:54:45,515] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:54:45,535] {logging_mixin.py:115} INFO - [2023-01-04 19:54:45,535] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:54:45,543] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.824 seconds
[2023-01-04 19:55:15,621] {processor.py:153} INFO - Started process (PID=3288) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:15,623] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:55:15,623] {logging_mixin.py:115} INFO - [2023-01-04 19:55:15,623] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:16,378] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:16,399] {logging_mixin.py:115} INFO - [2023-01-04 19:55:16,399] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:55:16,419] {logging_mixin.py:115} INFO - [2023-01-04 19:55:16,419] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:55:16,428] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.811 seconds
[2023-01-04 19:55:46,522] {processor.py:153} INFO - Started process (PID=3304) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:46,523] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:55:46,524] {logging_mixin.py:115} INFO - [2023-01-04 19:55:46,524] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:47,687] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:55:47,715] {logging_mixin.py:115} INFO - [2023-01-04 19:55:47,714] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:55:47,738] {logging_mixin.py:115} INFO - [2023-01-04 19:55:47,738] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:55:47,748] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.237 seconds
[2023-01-04 19:56:17,824] {processor.py:153} INFO - Started process (PID=3328) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:17,824] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:56:17,825] {logging_mixin.py:115} INFO - [2023-01-04 19:56:17,825] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:18,600] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:18,621] {logging_mixin.py:115} INFO - [2023-01-04 19:56:18,621] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:56:18,641] {logging_mixin.py:115} INFO - [2023-01-04 19:56:18,641] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:56:18,649] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.830 seconds
[2023-01-04 19:56:48,720] {processor.py:153} INFO - Started process (PID=3351) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:48,723] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:56:48,724] {logging_mixin.py:115} INFO - [2023-01-04 19:56:48,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:49,527] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:56:49,549] {logging_mixin.py:115} INFO - [2023-01-04 19:56:49,548] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:56:49,568] {logging_mixin.py:115} INFO - [2023-01-04 19:56:49,568] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:56:49,577] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 19:57:19,671] {processor.py:153} INFO - Started process (PID=3375) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:19,673] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:57:19,673] {logging_mixin.py:115} INFO - [2023-01-04 19:57:19,673] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:20,449] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:20,471] {logging_mixin.py:115} INFO - [2023-01-04 19:57:20,471] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:57:20,491] {logging_mixin.py:115} INFO - [2023-01-04 19:57:20,491] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:57:20,500] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 19:57:50,595] {processor.py:153} INFO - Started process (PID=3391) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:50,598] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:57:50,599] {logging_mixin.py:115} INFO - [2023-01-04 19:57:50,599] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:51,419] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:57:51,444] {logging_mixin.py:115} INFO - [2023-01-04 19:57:51,444] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:57:51,466] {logging_mixin.py:115} INFO - [2023-01-04 19:57:51,466] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:57:51,476] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.886 seconds
[2023-01-04 19:58:21,572] {processor.py:153} INFO - Started process (PID=3414) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:21,572] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:58:21,573] {logging_mixin.py:115} INFO - [2023-01-04 19:58:21,573] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:22,375] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:22,397] {logging_mixin.py:115} INFO - [2023-01-04 19:58:22,397] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:58:22,417] {logging_mixin.py:115} INFO - [2023-01-04 19:58:22,417] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:58:22,426] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 19:58:52,516] {processor.py:153} INFO - Started process (PID=3437) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:52,517] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:58:52,517] {logging_mixin.py:115} INFO - [2023-01-04 19:58:52,517] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:53,322] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:58:53,343] {logging_mixin.py:115} INFO - [2023-01-04 19:58:53,343] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:58:53,363] {logging_mixin.py:115} INFO - [2023-01-04 19:58:53,363] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:58:53,372] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.861 seconds
[2023-01-04 19:59:23,439] {processor.py:153} INFO - Started process (PID=3460) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:23,441] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:59:23,441] {logging_mixin.py:115} INFO - [2023-01-04 19:59:23,441] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:24,216] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:24,238] {logging_mixin.py:115} INFO - [2023-01-04 19:59:24,238] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:59:24,258] {logging_mixin.py:115} INFO - [2023-01-04 19:59:24,258] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:59:24,267] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.832 seconds
[2023-01-04 19:59:54,336] {processor.py:153} INFO - Started process (PID=3476) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:54,336] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 19:59:54,337] {logging_mixin.py:115} INFO - [2023-01-04 19:59:54,337] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:55,153] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 19:59:55,177] {logging_mixin.py:115} INFO - [2023-01-04 19:59:55,176] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 19:59:55,196] {logging_mixin.py:115} INFO - [2023-01-04 19:59:55,196] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 19:59:55,205] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 20:00:25,273] {processor.py:153} INFO - Started process (PID=3501) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:25,276] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:00:25,277] {logging_mixin.py:115} INFO - [2023-01-04 20:00:25,277] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:26,053] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:26,075] {logging_mixin.py:115} INFO - [2023-01-04 20:00:26,075] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:00:26,095] {logging_mixin.py:115} INFO - [2023-01-04 20:00:26,094] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:00:26,103] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 20:00:56,139] {processor.py:153} INFO - Started process (PID=3522) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:56,143] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:00:56,143] {logging_mixin.py:115} INFO - [2023-01-04 20:00:56,143] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:56,910] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:00:56,932] {logging_mixin.py:115} INFO - [2023-01-04 20:00:56,932] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:00:56,951] {logging_mixin.py:115} INFO - [2023-01-04 20:00:56,951] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:00:56,960] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.826 seconds
[2023-01-04 20:01:27,056] {processor.py:153} INFO - Started process (PID=3547) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:27,057] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:01:27,058] {logging_mixin.py:115} INFO - [2023-01-04 20:01:27,058] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:27,975] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:28,010] {logging_mixin.py:115} INFO - [2023-01-04 20:01:28,009] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:01:28,042] {logging_mixin.py:115} INFO - [2023-01-04 20:01:28,042] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:01:28,054] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.003 seconds
[2023-01-04 20:01:58,124] {processor.py:153} INFO - Started process (PID=3565) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:58,128] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:01:58,129] {logging_mixin.py:115} INFO - [2023-01-04 20:01:58,129] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:58,904] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:01:58,925] {logging_mixin.py:115} INFO - [2023-01-04 20:01:58,925] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:01:58,945] {logging_mixin.py:115} INFO - [2023-01-04 20:01:58,945] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:01:58,953] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 20:02:29,056] {processor.py:153} INFO - Started process (PID=3589) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:02:29,057] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:02:29,058] {logging_mixin.py:115} INFO - [2023-01-04 20:02:29,058] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:02:30,190] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:02:30,218] {logging_mixin.py:115} INFO - [2023-01-04 20:02:30,218] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:02:30,244] {logging_mixin.py:115} INFO - [2023-01-04 20:02:30,243] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:02:30,263] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.212 seconds
[2023-01-04 20:03:00,337] {processor.py:153} INFO - Started process (PID=3613) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:00,338] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:03:00,339] {logging_mixin.py:115} INFO - [2023-01-04 20:03:00,339] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:01,269] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:01,298] {logging_mixin.py:115} INFO - [2023-01-04 20:03:01,297] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:03:01,319] {logging_mixin.py:115} INFO - [2023-01-04 20:03:01,319] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:03:01,329] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.996 seconds
[2023-01-04 20:03:31,417] {processor.py:153} INFO - Started process (PID=3637) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:31,418] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:03:31,419] {logging_mixin.py:115} INFO - [2023-01-04 20:03:31,418] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:32,253] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:03:32,278] {logging_mixin.py:115} INFO - [2023-01-04 20:03:32,278] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:03:32,300] {logging_mixin.py:115} INFO - [2023-01-04 20:03:32,300] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:03:32,310] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.899 seconds
[2023-01-04 20:04:02,400] {processor.py:153} INFO - Started process (PID=3653) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:02,401] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:04:02,402] {logging_mixin.py:115} INFO - [2023-01-04 20:04:02,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:03,201] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:03,223] {logging_mixin.py:115} INFO - [2023-01-04 20:04:03,222] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:04:03,242] {logging_mixin.py:115} INFO - [2023-01-04 20:04:03,242] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:04:03,251] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 20:04:33,346] {processor.py:153} INFO - Started process (PID=3676) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:33,347] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:04:33,347] {logging_mixin.py:115} INFO - [2023-01-04 20:04:33,347] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:34,147] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:04:34,169] {logging_mixin.py:115} INFO - [2023-01-04 20:04:34,168] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:04:34,188] {logging_mixin.py:115} INFO - [2023-01-04 20:04:34,188] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:04:34,197] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 20:05:04,317] {processor.py:153} INFO - Started process (PID=3702) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:04,320] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:05:04,321] {logging_mixin.py:115} INFO - [2023-01-04 20:05:04,321] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:05,094] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:05,116] {logging_mixin.py:115} INFO - [2023-01-04 20:05:05,116] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:05:05,136] {logging_mixin.py:115} INFO - [2023-01-04 20:05:05,135] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:05:05,144] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.832 seconds
[2023-01-04 20:05:35,238] {processor.py:153} INFO - Started process (PID=3726) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:35,238] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:05:35,239] {logging_mixin.py:115} INFO - [2023-01-04 20:05:35,239] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:36,024] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:05:36,045] {logging_mixin.py:115} INFO - [2023-01-04 20:05:36,045] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:05:36,065] {logging_mixin.py:115} INFO - [2023-01-04 20:05:36,065] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:05:36,074] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.841 seconds
[2023-01-04 20:06:06,142] {processor.py:153} INFO - Started process (PID=3742) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:06,143] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:06:06,144] {logging_mixin.py:115} INFO - [2023-01-04 20:06:06,144] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:06,931] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:06,953] {logging_mixin.py:115} INFO - [2023-01-04 20:06:06,952] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:06:06,973] {logging_mixin.py:115} INFO - [2023-01-04 20:06:06,973] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:06:06,982] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 20:06:37,056] {processor.py:153} INFO - Started process (PID=3764) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:37,056] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:06:37,057] {logging_mixin.py:115} INFO - [2023-01-04 20:06:37,057] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:37,829] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:06:37,853] {logging_mixin.py:115} INFO - [2023-01-04 20:06:37,853] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:06:37,875] {logging_mixin.py:115} INFO - [2023-01-04 20:06:37,875] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:06:37,884] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.833 seconds
[2023-01-04 20:07:07,953] {processor.py:153} INFO - Started process (PID=3788) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:07,954] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:07:07,954] {logging_mixin.py:115} INFO - [2023-01-04 20:07:07,954] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:08,733] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:08,757] {logging_mixin.py:115} INFO - [2023-01-04 20:07:08,757] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:07:08,779] {logging_mixin.py:115} INFO - [2023-01-04 20:07:08,779] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:07:08,788] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 20:07:22,786] {processor.py:153} INFO - Started process (PID=3796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:22,787] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:07:22,787] {logging_mixin.py:115} INFO - [2023-01-04 20:07:22,787] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:22,791] {logging_mixin.py:115} INFO - [2023-01-04 20:07:22,790] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 62
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:07:22,791] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:22,816] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.035 seconds
[2023-01-04 20:07:52,896] {processor.py:153} INFO - Started process (PID=3818) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:52,897] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:07:52,898] {logging_mixin.py:115} INFO - [2023-01-04 20:07:52,898] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:52,902] {logging_mixin.py:115} INFO - [2023-01-04 20:07:52,901] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 62
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:07:52,903] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:07:52,930] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.039 seconds
[2023-01-04 20:08:22,970] {processor.py:153} INFO - Started process (PID=3833) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:22,971] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:08:22,972] {logging_mixin.py:115} INFO - [2023-01-04 20:08:22,972] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:22,975] {logging_mixin.py:115} INFO - [2023-01-04 20:08:22,974] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 62
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:08:22,975] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:23,005] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.040 seconds
[2023-01-04 20:08:53,055] {processor.py:153} INFO - Started process (PID=3857) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:53,056] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:08:53,057] {logging_mixin.py:115} INFO - [2023-01-04 20:08:53,057] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:53,060] {logging_mixin.py:115} INFO - [2023-01-04 20:08:53,060] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 62
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:08:53,061] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:08:53,085] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.035 seconds
[2023-01-04 20:09:23,146] {processor.py:153} INFO - Started process (PID=3879) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:23,148] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:09:23,148] {logging_mixin.py:115} INFO - [2023-01-04 20:09:23,148] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:23,152] {logging_mixin.py:115} INFO - [2023-01-04 20:09:23,151] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 62
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:09:23,152] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:23,174] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.034 seconds
[2023-01-04 20:09:25,159] {processor.py:153} INFO - Started process (PID=3880) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:25,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:09:25,161] {logging_mixin.py:115} INFO - [2023-01-04 20:09:25,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:25,164] {logging_mixin.py:115} INFO - [2023-01-04 20:09:25,163] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 61
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:09:25,164] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:25,187] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 20:09:55,261] {processor.py:153} INFO - Started process (PID=3902) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:55,262] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:09:55,262] {logging_mixin.py:115} INFO - [2023-01-04 20:09:55,262] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:55,266] {logging_mixin.py:115} INFO - [2023-01-04 20:09:55,265] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 61
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:09:55,266] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:09:55,289] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 20:10:25,332] {processor.py:153} INFO - Started process (PID=3917) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:25,333] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:10:25,334] {logging_mixin.py:115} INFO - [2023-01-04 20:10:25,334] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:25,338] {logging_mixin.py:115} INFO - [2023-01-04 20:10:25,337] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 61
    create_cluster_dataproc_task = DataprocCreateClusterOperator(
                                                                 ^
IndentationError: unindent does not match any outer indentation level
[2023-01-04 20:10:25,338] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:25,361] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 20:10:54,415] {processor.py:153} INFO - Started process (PID=3940) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:54,416] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:10:54,416] {logging_mixin.py:115} INFO - [2023-01-04 20:10:54,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:55,205] {logging_mixin.py:115} INFO - [2023-01-04 20:10:55,204] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_S&P_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_S' is not defined
[2023-01-04 20:10:55,205] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:10:55,223] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.812 seconds
[2023-01-04 20:11:25,317] {processor.py:153} INFO - Started process (PID=3965) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:25,319] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:11:25,319] {logging_mixin.py:115} INFO - [2023-01-04 20:11:25,319] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:26,105] {logging_mixin.py:115} INFO - [2023-01-04 20:11:26,105] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_S&P_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_S' is not defined
[2023-01-04 20:11:26,106] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:26,124] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.811 seconds
[2023-01-04 20:11:56,217] {processor.py:153} INFO - Started process (PID=3982) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:56,218] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:11:56,219] {logging_mixin.py:115} INFO - [2023-01-04 20:11:56,219] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:57,019] {logging_mixin.py:115} INFO - [2023-01-04 20:11:57,019] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_S&P_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_S' is not defined
[2023-01-04 20:11:57,020] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:11:57,037] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.825 seconds
[2023-01-04 20:12:27,112] {processor.py:153} INFO - Started process (PID=4006) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:27,115] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:12:27,116] {logging_mixin.py:115} INFO - [2023-01-04 20:12:27,116] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:27,930] {logging_mixin.py:115} INFO - [2023-01-04 20:12:27,929] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_S&P_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_S' is not defined
[2023-01-04 20:12:27,930] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:27,951] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 20:12:31,985] {processor.py:153} INFO - Started process (PID=4016) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:31,986] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:12:31,987] {logging_mixin.py:115} INFO - [2023-01-04 20:12:31,987] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:32,883] {logging_mixin.py:115} INFO - [2023-01-04 20:12:32,882] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:12:32,884] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:12:32,901] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-04 20:13:02,968] {processor.py:153} INFO - Started process (PID=4034) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:02,969] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:13:02,970] {logging_mixin.py:115} INFO - [2023-01-04 20:13:02,969] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:03,739] {logging_mixin.py:115} INFO - [2023-01-04 20:13:03,738] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:13:03,739] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:03,756] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.794 seconds
[2023-01-04 20:13:33,823] {processor.py:153} INFO - Started process (PID=4057) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:33,824] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:13:33,824] {logging_mixin.py:115} INFO - [2023-01-04 20:13:33,824] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:34,589] {logging_mixin.py:115} INFO - [2023-01-04 20:13:34,588] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:13:34,589] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:13:34,607] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.789 seconds
[2023-01-04 20:14:04,702] {processor.py:153} INFO - Started process (PID=4082) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:04,704] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:14:04,704] {logging_mixin.py:115} INFO - [2023-01-04 20:14:04,704] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:05,507] {logging_mixin.py:115} INFO - [2023-01-04 20:14:05,506] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:14:05,507] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:05,529] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.832 seconds
[2023-01-04 20:14:35,625] {processor.py:153} INFO - Started process (PID=4098) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:35,625] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:14:35,626] {logging_mixin.py:115} INFO - [2023-01-04 20:14:35,626] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:36,492] {logging_mixin.py:115} INFO - [2023-01-04 20:14:36,491] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:14:36,492] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:14:36,511] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.892 seconds
[2023-01-04 20:15:06,616] {processor.py:153} INFO - Started process (PID=4121) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:06,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:15:06,619] {logging_mixin.py:115} INFO - [2023-01-04 20:15:06,619] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:07,422] {logging_mixin.py:115} INFO - [2023-01-04 20:15:07,421] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:15:07,422] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:07,439] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.828 seconds
[2023-01-04 20:15:37,511] {processor.py:153} INFO - Started process (PID=4144) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:37,512] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:15:37,512] {logging_mixin.py:115} INFO - [2023-01-04 20:15:37,512] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:38,362] {logging_mixin.py:115} INFO - [2023-01-04 20:15:38,361] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:15:38,363] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:15:38,381] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.876 seconds
[2023-01-04 20:16:08,451] {processor.py:153} INFO - Started process (PID=4166) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:08,452] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:16:08,453] {logging_mixin.py:115} INFO - [2023-01-04 20:16:08,453] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:09,206] {logging_mixin.py:115} INFO - [2023-01-04 20:16:09,205] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 57, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS.main
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 20:16:09,206] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:09,224] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.777 seconds
[2023-01-04 20:16:25,274] {processor.py:153} INFO - Started process (PID=4176) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:25,274] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:16:25,275] {logging_mixin.py:115} INFO - [2023-01-04 20:16:25,275] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:26,050] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:16:26,051] {logging_mixin.py:115} INFO - [2023-01-04 20:16:26,051] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:16:26,052] {logging_mixin.py:115} INFO - [2023-01-04 20:16:26,051] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:16:26,057] {logging_mixin.py:115} INFO - [2023-01-04 20:16:26,052] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:16:26,057] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:26,073] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 20:16:56,157] {processor.py:153} INFO - Started process (PID=4199) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:56,160] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:16:56,161] {logging_mixin.py:115} INFO - [2023-01-04 20:16:56,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:56,936] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:16:56,937] {logging_mixin.py:115} INFO - [2023-01-04 20:16:56,937] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:16:56,938] {logging_mixin.py:115} INFO - [2023-01-04 20:16:56,938] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:16:56,941] {logging_mixin.py:115} INFO - [2023-01-04 20:16:56,938] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:16:56,941] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:16:56,958] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.806 seconds
[2023-01-04 20:17:27,052] {processor.py:153} INFO - Started process (PID=4222) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:27,054] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:17:27,054] {logging_mixin.py:115} INFO - [2023-01-04 20:17:27,054] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:27,851] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:17:27,853] {logging_mixin.py:115} INFO - [2023-01-04 20:17:27,853] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:17:27,853] {logging_mixin.py:115} INFO - [2023-01-04 20:17:27,853] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:17:27,856] {logging_mixin.py:115} INFO - [2023-01-04 20:17:27,854] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:17:27,856] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:27,874] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.826 seconds
[2023-01-04 20:17:57,966] {processor.py:153} INFO - Started process (PID=4239) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:57,966] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:17:57,967] {logging_mixin.py:115} INFO - [2023-01-04 20:17:57,967] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:58,778] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:17:58,779] {logging_mixin.py:115} INFO - [2023-01-04 20:17:58,779] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:17:58,780] {logging_mixin.py:115} INFO - [2023-01-04 20:17:58,780] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:17:58,783] {logging_mixin.py:115} INFO - [2023-01-04 20:17:58,780] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:17:58,783] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:17:58,800] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 20:18:28,886] {processor.py:153} INFO - Started process (PID=4264) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:18:28,887] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:18:28,888] {logging_mixin.py:115} INFO - [2023-01-04 20:18:28,887] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:18:29,669] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:18:29,670] {logging_mixin.py:115} INFO - [2023-01-04 20:18:29,670] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:18:29,670] {logging_mixin.py:115} INFO - [2023-01-04 20:18:29,670] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:18:29,673] {logging_mixin.py:115} INFO - [2023-01-04 20:18:29,671] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:18:29,673] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:18:29,690] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 20:18:59,757] {processor.py:153} INFO - Started process (PID=4287) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:18:59,759] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:18:59,760] {logging_mixin.py:115} INFO - [2023-01-04 20:18:59,759] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:19:00,525] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:19:00,526] {logging_mixin.py:115} INFO - [2023-01-04 20:19:00,526] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:19:00,527] {logging_mixin.py:115} INFO - [2023-01-04 20:19:00,527] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:19:00,530] {logging_mixin.py:115} INFO - [2023-01-04 20:19:00,527] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:19:00,530] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:19:00,547] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.794 seconds
[2023-01-04 20:19:30,617] {processor.py:153} INFO - Started process (PID=4310) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:19:30,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:19:30,618] {logging_mixin.py:115} INFO - [2023-01-04 20:19:30,618] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:19:31,480] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:19:31,481] {logging_mixin.py:115} INFO - [2023-01-04 20:19:31,481] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:19:31,482] {logging_mixin.py:115} INFO - [2023-01-04 20:19:31,481] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:19:31,484] {logging_mixin.py:115} INFO - [2023-01-04 20:19:31,482] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:19:31,484] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:19:31,502] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-04 20:20:01,567] {processor.py:153} INFO - Started process (PID=4326) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:01,572] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:20:01,573] {logging_mixin.py:115} INFO - [2023-01-04 20:20:01,572] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:02,385] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:20:02,386] {logging_mixin.py:115} INFO - [2023-01-04 20:20:02,386] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:20:02,387] {logging_mixin.py:115} INFO - [2023-01-04 20:20:02,386] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:20:02,389] {logging_mixin.py:115} INFO - [2023-01-04 20:20:02,387] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:20:02,390] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:02,408] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.846 seconds
[2023-01-04 20:20:32,520] {processor.py:153} INFO - Started process (PID=4349) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:32,521] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:20:32,522] {logging_mixin.py:115} INFO - [2023-01-04 20:20:32,521] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:33,297] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:20:33,298] {logging_mixin.py:115} INFO - [2023-01-04 20:20:33,298] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:20:33,298] {logging_mixin.py:115} INFO - [2023-01-04 20:20:33,298] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:20:33,301] {logging_mixin.py:115} INFO - [2023-01-04 20:20:33,299] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:20:33,302] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:20:33,319] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.804 seconds
[2023-01-04 20:21:03,408] {processor.py:153} INFO - Started process (PID=4372) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:03,409] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:21:03,410] {logging_mixin.py:115} INFO - [2023-01-04 20:21:03,410] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:04,212] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:21:04,213] {logging_mixin.py:115} INFO - [2023-01-04 20:21:04,213] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:21:04,213] {logging_mixin.py:115} INFO - [2023-01-04 20:21:04,213] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:21:04,216] {logging_mixin.py:115} INFO - [2023-01-04 20:21:04,214] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:21:04,216] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:04,233] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.830 seconds
[2023-01-04 20:21:34,322] {processor.py:153} INFO - Started process (PID=4395) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:34,324] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:21:34,325] {logging_mixin.py:115} INFO - [2023-01-04 20:21:34,324] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:35,106] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:21:35,107] {logging_mixin.py:115} INFO - [2023-01-04 20:21:35,107] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:21:35,107] {logging_mixin.py:115} INFO - [2023-01-04 20:21:35,107] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:21:35,110] {logging_mixin.py:115} INFO - [2023-01-04 20:21:35,108] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:21:35,110] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:21:35,128] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.810 seconds
[2023-01-04 20:22:05,195] {processor.py:153} INFO - Started process (PID=4412) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:05,197] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:22:05,197] {logging_mixin.py:115} INFO - [2023-01-04 20:22:05,197] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:05,995] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:22:05,997] {logging_mixin.py:115} INFO - [2023-01-04 20:22:05,997] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:22:05,997] {logging_mixin.py:115} INFO - [2023-01-04 20:22:05,997] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:22:06,000] {logging_mixin.py:115} INFO - [2023-01-04 20:22:05,998] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:22:06,000] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:06,018] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.828 seconds
[2023-01-04 20:22:36,087] {processor.py:153} INFO - Started process (PID=4436) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:36,088] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:22:36,089] {logging_mixin.py:115} INFO - [2023-01-04 20:22:36,089] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:36,929] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:22:36,930] {logging_mixin.py:115} INFO - [2023-01-04 20:22:36,930] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:22:36,931] {logging_mixin.py:115} INFO - [2023-01-04 20:22:36,930] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:22:36,934] {logging_mixin.py:115} INFO - [2023-01-04 20:22:36,931] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:22:36,935] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:22:36,968] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.886 seconds
[2023-01-04 20:23:07,037] {processor.py:153} INFO - Started process (PID=4459) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:07,038] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:23:07,038] {logging_mixin.py:115} INFO - [2023-01-04 20:23:07,038] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:07,834] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:23:07,836] {logging_mixin.py:115} INFO - [2023-01-04 20:23:07,836] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:23:07,836] {logging_mixin.py:115} INFO - [2023-01-04 20:23:07,836] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:23:07,839] {logging_mixin.py:115} INFO - [2023-01-04 20:23:07,837] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:23:07,839] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:07,856] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.824 seconds
[2023-01-04 20:23:37,935] {processor.py:153} INFO - Started process (PID=4483) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:37,936] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:23:37,936] {logging_mixin.py:115} INFO - [2023-01-04 20:23:37,936] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:38,728] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:23:38,729] {logging_mixin.py:115} INFO - [2023-01-04 20:23:38,729] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:23:38,729] {logging_mixin.py:115} INFO - [2023-01-04 20:23:38,729] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:23:38,732] {logging_mixin.py:115} INFO - [2023-01-04 20:23:38,730] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:23:38,732] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:23:38,749] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.818 seconds
[2023-01-04 20:24:08,845] {processor.py:153} INFO - Started process (PID=4499) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:08,846] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:24:08,847] {logging_mixin.py:115} INFO - [2023-01-04 20:24:08,847] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:09,989] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:24:09,991] {logging_mixin.py:115} INFO - [2023-01-04 20:24:09,991] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:24:09,992] {logging_mixin.py:115} INFO - [2023-01-04 20:24:09,991] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:24:09,995] {logging_mixin.py:115} INFO - [2023-01-04 20:24:09,992] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:24:09,996] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:10,018] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.179 seconds
[2023-01-04 20:24:40,054] {processor.py:153} INFO - Started process (PID=4522) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:40,058] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:24:40,058] {logging_mixin.py:115} INFO - [2023-01-04 20:24:40,058] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:40,866] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:24:40,867] {logging_mixin.py:115} INFO - [2023-01-04 20:24:40,867] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:24:40,868] {logging_mixin.py:115} INFO - [2023-01-04 20:24:40,868] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:24:40,870] {logging_mixin.py:115} INFO - [2023-01-04 20:24:40,868] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:24:40,871] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:24:40,888] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 20:25:10,998] {processor.py:153} INFO - Started process (PID=4546) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:11,000] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:25:11,000] {logging_mixin.py:115} INFO - [2023-01-04 20:25:11,000] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:11,781] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:25:11,783] {logging_mixin.py:115} INFO - [2023-01-04 20:25:11,783] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:25:11,783] {logging_mixin.py:115} INFO - [2023-01-04 20:25:11,783] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:25:11,786] {logging_mixin.py:115} INFO - [2023-01-04 20:25:11,784] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:25:11,786] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:11,803] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.809 seconds
[2023-01-04 20:25:41,896] {processor.py:153} INFO - Started process (PID=4570) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:41,897] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:25:41,898] {logging_mixin.py:115} INFO - [2023-01-04 20:25:41,898] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:42,704] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:25:42,705] {logging_mixin.py:115} INFO - [2023-01-04 20:25:42,705] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:25:42,706] {logging_mixin.py:115} INFO - [2023-01-04 20:25:42,705] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:25:42,708] {logging_mixin.py:115} INFO - [2023-01-04 20:25:42,706] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:25:42,709] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:25:42,726] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 20:26:12,813] {processor.py:153} INFO - Started process (PID=4585) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:12,816] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:26:12,817] {logging_mixin.py:115} INFO - [2023-01-04 20:26:12,817] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:13,637] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:26:13,639] {logging_mixin.py:115} INFO - [2023-01-04 20:26:13,639] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:26:13,639] {logging_mixin.py:115} INFO - [2023-01-04 20:26:13,639] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:26:13,642] {logging_mixin.py:115} INFO - [2023-01-04 20:26:13,640] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:26:13,642] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:13,659] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 20:26:43,723] {processor.py:153} INFO - Started process (PID=4609) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:43,723] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:26:43,724] {logging_mixin.py:115} INFO - [2023-01-04 20:26:43,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:44,493] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:26:44,494] {logging_mixin.py:115} INFO - [2023-01-04 20:26:44,494] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:26:44,494] {logging_mixin.py:115} INFO - [2023-01-04 20:26:44,494] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:26:44,497] {logging_mixin.py:115} INFO - [2023-01-04 20:26:44,495] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:26:44,497] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:26:44,515] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.797 seconds
[2023-01-04 20:27:14,584] {processor.py:153} INFO - Started process (PID=4632) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:14,586] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:27:14,587] {logging_mixin.py:115} INFO - [2023-01-04 20:27:14,586] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:15,438] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:27:15,439] {logging_mixin.py:115} INFO - [2023-01-04 20:27:15,439] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:27:15,440] {logging_mixin.py:115} INFO - [2023-01-04 20:27:15,440] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:27:15,443] {logging_mixin.py:115} INFO - [2023-01-04 20:27:15,441] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:27:15,443] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:15,460] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.881 seconds
[2023-01-04 20:27:45,526] {processor.py:153} INFO - Started process (PID=4656) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:45,527] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:27:45,527] {logging_mixin.py:115} INFO - [2023-01-04 20:27:45,527] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:46,298] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:27:46,300] {logging_mixin.py:115} INFO - [2023-01-04 20:27:46,299] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:27:46,300] {logging_mixin.py:115} INFO - [2023-01-04 20:27:46,300] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:27:46,303] {logging_mixin.py:115} INFO - [2023-01-04 20:27:46,301] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:27:46,303] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:27:46,320] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.799 seconds
[2023-01-04 20:28:16,415] {processor.py:153} INFO - Started process (PID=4671) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:16,415] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:28:16,416] {logging_mixin.py:115} INFO - [2023-01-04 20:28:16,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:17,200] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:28:17,201] {logging_mixin.py:115} INFO - [2023-01-04 20:28:17,201] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:28:17,202] {logging_mixin.py:115} INFO - [2023-01-04 20:28:17,202] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:28:17,204] {logging_mixin.py:115} INFO - [2023-01-04 20:28:17,202] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:28:17,205] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:17,221] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.811 seconds
[2023-01-04 20:28:47,312] {processor.py:153} INFO - Started process (PID=4694) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:47,314] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:28:47,314] {logging_mixin.py:115} INFO - [2023-01-04 20:28:47,314] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:48,125] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:28:48,126] {logging_mixin.py:115} INFO - [2023-01-04 20:28:48,126] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:28:48,127] {logging_mixin.py:115} INFO - [2023-01-04 20:28:48,126] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:28:48,129] {logging_mixin.py:115} INFO - [2023-01-04 20:28:48,127] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:28:48,130] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:28:48,147] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 20:29:18,215] {processor.py:153} INFO - Started process (PID=4717) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:18,216] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:29:18,217] {logging_mixin.py:115} INFO - [2023-01-04 20:29:18,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:18,987] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:29:18,988] {logging_mixin.py:115} INFO - [2023-01-04 20:29:18,988] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:29:18,989] {logging_mixin.py:115} INFO - [2023-01-04 20:29:18,988] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:29:18,991] {logging_mixin.py:115} INFO - [2023-01-04 20:29:18,989] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:29:18,992] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:19,008] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.798 seconds
[2023-01-04 20:29:49,075] {processor.py:153} INFO - Started process (PID=4740) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:49,076] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:29:49,077] {logging_mixin.py:115} INFO - [2023-01-04 20:29:49,077] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:49,850] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:29:49,851] {logging_mixin.py:115} INFO - [2023-01-04 20:29:49,851] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:29:49,852] {logging_mixin.py:115} INFO - [2023-01-04 20:29:49,852] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:29:49,855] {logging_mixin.py:115} INFO - [2023-01-04 20:29:49,852] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:29:49,855] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:29:49,872] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.802 seconds
[2023-01-04 20:30:19,937] {processor.py:153} INFO - Started process (PID=4756) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:19,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:30:19,938] {logging_mixin.py:115} INFO - [2023-01-04 20:30:19,938] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:20,738] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:30:20,739] {logging_mixin.py:115} INFO - [2023-01-04 20:30:20,739] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:30:20,740] {logging_mixin.py:115} INFO - [2023-01-04 20:30:20,739] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:30:20,743] {logging_mixin.py:115} INFO - [2023-01-04 20:30:20,740] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:30:20,743] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:20,763] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.831 seconds
[2023-01-04 20:30:50,865] {processor.py:153} INFO - Started process (PID=4780) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:50,867] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:30:50,867] {logging_mixin.py:115} INFO - [2023-01-04 20:30:50,867] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:51,650] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:30:51,652] {logging_mixin.py:115} INFO - [2023-01-04 20:30:51,652] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:30:51,652] {logging_mixin.py:115} INFO - [2023-01-04 20:30:51,652] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:30:51,655] {logging_mixin.py:115} INFO - [2023-01-04 20:30:51,653] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:30:51,655] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:30:51,672] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.811 seconds
[2023-01-04 20:31:21,766] {processor.py:153} INFO - Started process (PID=4803) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:21,767] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:31:21,768] {logging_mixin.py:115} INFO - [2023-01-04 20:31:21,768] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:22,539] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:31:22,540] {logging_mixin.py:115} INFO - [2023-01-04 20:31:22,540] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:31:22,541] {logging_mixin.py:115} INFO - [2023-01-04 20:31:22,540] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:31:22,544] {logging_mixin.py:115} INFO - [2023-01-04 20:31:22,541] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:31:22,544] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:22,561] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.800 seconds
[2023-01-04 20:31:52,630] {processor.py:153} INFO - Started process (PID=4826) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:52,631] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:31:52,632] {logging_mixin.py:115} INFO - [2023-01-04 20:31:52,631] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:53,396] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:31:53,398] {logging_mixin.py:115} INFO - [2023-01-04 20:31:53,398] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:31:53,398] {logging_mixin.py:115} INFO - [2023-01-04 20:31:53,398] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:31:53,401] {logging_mixin.py:115} INFO - [2023-01-04 20:31:53,399] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:31:53,401] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:31:53,418] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.792 seconds
[2023-01-04 20:32:23,483] {processor.py:153} INFO - Started process (PID=4842) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:23,485] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:32:23,486] {logging_mixin.py:115} INFO - [2023-01-04 20:32:23,485] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:24,265] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:32:24,267] {logging_mixin.py:115} INFO - [2023-01-04 20:32:24,266] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:32:24,267] {logging_mixin.py:115} INFO - [2023-01-04 20:32:24,267] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:32:24,270] {logging_mixin.py:115} INFO - [2023-01-04 20:32:24,267] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:32:24,270] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:24,292] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.813 seconds
[2023-01-04 20:32:54,358] {processor.py:153} INFO - Started process (PID=4865) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:54,360] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:32:54,361] {logging_mixin.py:115} INFO - [2023-01-04 20:32:54,361] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:55,137] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:32:55,138] {logging_mixin.py:115} INFO - [2023-01-04 20:32:55,138] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:32:55,138] {logging_mixin.py:115} INFO - [2023-01-04 20:32:55,138] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:32:55,141] {logging_mixin.py:115} INFO - [2023-01-04 20:32:55,139] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:32:55,141] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:32:55,158] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.804 seconds
[2023-01-04 20:33:25,256] {processor.py:153} INFO - Started process (PID=4887) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:25,257] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:33:25,258] {logging_mixin.py:115} INFO - [2023-01-04 20:33:25,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:26,038] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:33:26,039] {logging_mixin.py:115} INFO - [2023-01-04 20:33:26,039] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:33:26,040] {logging_mixin.py:115} INFO - [2023-01-04 20:33:26,039] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:33:26,043] {logging_mixin.py:115} INFO - [2023-01-04 20:33:26,040] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:33:26,043] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:26,061] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.810 seconds
[2023-01-04 20:33:56,151] {processor.py:153} INFO - Started process (PID=4910) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:56,151] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:33:56,152] {logging_mixin.py:115} INFO - [2023-01-04 20:33:56,152] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:56,926] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:33:56,928] {logging_mixin.py:115} INFO - [2023-01-04 20:33:56,928] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:33:56,928] {logging_mixin.py:115} INFO - [2023-01-04 20:33:56,928] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:33:56,931] {logging_mixin.py:115} INFO - [2023-01-04 20:33:56,929] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:33:56,932] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:33:56,949] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.803 seconds
[2023-01-04 20:34:27,017] {processor.py:153} INFO - Started process (PID=4925) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:27,018] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:34:27,019] {logging_mixin.py:115} INFO - [2023-01-04 20:34:27,019] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:27,822] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:34:27,823] {logging_mixin.py:115} INFO - [2023-01-04 20:34:27,823] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:34:27,824] {logging_mixin.py:115} INFO - [2023-01-04 20:34:27,823] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:34:27,826] {logging_mixin.py:115} INFO - [2023-01-04 20:34:27,824] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:34:27,827] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:27,850] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 20:34:57,922] {processor.py:153} INFO - Started process (PID=4948) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:57,924] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:34:57,924] {logging_mixin.py:115} INFO - [2023-01-04 20:34:57,924] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:58,735] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:34:58,736] {logging_mixin.py:115} INFO - [2023-01-04 20:34:58,736] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:34:58,737] {logging_mixin.py:115} INFO - [2023-01-04 20:34:58,736] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:34:58,739] {logging_mixin.py:115} INFO - [2023-01-04 20:34:58,737] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:34:58,740] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:34:58,757] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 20:35:28,846] {processor.py:153} INFO - Started process (PID=4971) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:35:28,847] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:35:28,847] {logging_mixin.py:115} INFO - [2023-01-04 20:35:28,847] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:35:29,670] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:35:29,671] {logging_mixin.py:115} INFO - [2023-01-04 20:35:29,671] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:35:29,672] {logging_mixin.py:115} INFO - [2023-01-04 20:35:29,671] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:35:29,674] {logging_mixin.py:115} INFO - [2023-01-04 20:35:29,672] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:35:29,675] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:35:29,692] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 20:35:59,760] {processor.py:153} INFO - Started process (PID=4994) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:35:59,762] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:35:59,763] {logging_mixin.py:115} INFO - [2023-01-04 20:35:59,763] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:36:00,683] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:36:00,684] {logging_mixin.py:115} INFO - [2023-01-04 20:36:00,684] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:36:00,685] {logging_mixin.py:115} INFO - [2023-01-04 20:36:00,684] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:36:00,688] {logging_mixin.py:115} INFO - [2023-01-04 20:36:00,685] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:36:00,689] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:36:00,707] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.952 seconds
[2023-01-04 20:36:30,748] {processor.py:153} INFO - Started process (PID=5010) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:36:30,749] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:36:30,750] {logging_mixin.py:115} INFO - [2023-01-04 20:36:30,749] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:36:31,585] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:36:31,586] {logging_mixin.py:115} INFO - [2023-01-04 20:36:31,586] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:36:31,586] {logging_mixin.py:115} INFO - [2023-01-04 20:36:31,586] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:36:31,590] {logging_mixin.py:115} INFO - [2023-01-04 20:36:31,587] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:36:31,590] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:36:31,608] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 20:37:01,702] {processor.py:153} INFO - Started process (PID=5033) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:01,703] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:37:01,704] {logging_mixin.py:115} INFO - [2023-01-04 20:37:01,704] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:02,473] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:37:02,474] {logging_mixin.py:115} INFO - [2023-01-04 20:37:02,474] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:37:02,475] {logging_mixin.py:115} INFO - [2023-01-04 20:37:02,474] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:37:02,477] {logging_mixin.py:115} INFO - [2023-01-04 20:37:02,475] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:37:02,477] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:02,495] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.798 seconds
[2023-01-04 20:37:32,585] {processor.py:153} INFO - Started process (PID=5057) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:32,585] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:37:32,586] {logging_mixin.py:115} INFO - [2023-01-04 20:37:32,586] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:33,402] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:37:33,403] {logging_mixin.py:115} INFO - [2023-01-04 20:37:33,403] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:37:33,403] {logging_mixin.py:115} INFO - [2023-01-04 20:37:33,403] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:37:33,406] {logging_mixin.py:115} INFO - [2023-01-04 20:37:33,404] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:37:33,406] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:37:33,423] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 20:38:03,507] {processor.py:153} INFO - Started process (PID=5081) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:03,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:38:03,509] {logging_mixin.py:115} INFO - [2023-01-04 20:38:03,509] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:04,297] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:38:04,298] {logging_mixin.py:115} INFO - [2023-01-04 20:38:04,298] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:38:04,298] {logging_mixin.py:115} INFO - [2023-01-04 20:38:04,298] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:38:04,301] {logging_mixin.py:115} INFO - [2023-01-04 20:38:04,299] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:38:04,301] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:04,318] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.815 seconds
[2023-01-04 20:38:34,384] {processor.py:153} INFO - Started process (PID=5097) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:34,385] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:38:34,385] {logging_mixin.py:115} INFO - [2023-01-04 20:38:34,385] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:35,174] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:38:35,175] {logging_mixin.py:115} INFO - [2023-01-04 20:38:35,175] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:38:35,176] {logging_mixin.py:115} INFO - [2023-01-04 20:38:35,176] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:38:35,178] {logging_mixin.py:115} INFO - [2023-01-04 20:38:35,176] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:38:35,179] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:38:35,197] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.818 seconds
[2023-01-04 20:39:05,265] {processor.py:153} INFO - Started process (PID=5120) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:39:05,267] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:39:05,267] {logging_mixin.py:115} INFO - [2023-01-04 20:39:05,267] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:39:06,138] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:39:06,140] {logging_mixin.py:115} INFO - [2023-01-04 20:39:06,140] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:39:06,141] {logging_mixin.py:115} INFO - [2023-01-04 20:39:06,140] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:39:06,145] {logging_mixin.py:115} INFO - [2023-01-04 20:39:06,142] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 69, in <module>
    submit_spark_job_task = DataprocSubmitPySparkJobOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 1448, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 912, in __init__
    self.project_id = self.hook.project_id if project_id is None else project_id
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 341, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 247, in _get_credentials_and_project_id
    credentials, project_id = get_credentials_and_project_id(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 332, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 231, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 280, in _get_credentials_using_key_path
    credentials = google.oauth2.service_account.Credentials.from_service_account_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/oauth2/service_account.py", line 238, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.9/site-packages/google/auth/_service_account_info.py", line 79, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[2023-01-04 20:39:06,145] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:39:06,163] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-04 20:41:01,484] {processor.py:153} INFO - Started process (PID=32) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:01,487] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:41:01,488] {logging_mixin.py:115} INFO - [2023-01-04 20:41:01,488] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:04,828] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:41:04,830] {logging_mixin.py:115} INFO - [2023-01-04 20:41:04,830] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:41:04,831] {logging_mixin.py:115} INFO - [2023-01-04 20:41:04,831] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:41:04,870] {logging_mixin.py:115} INFO - [2023-01-04 20:41:04,859] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 85, in <module>
    download_stock_data_from_datalake_task >> create_cluster_dataproc_task >> submit_spark_job_task >> delete_cluster_dataproc_task
NameError: name 'download_stock_data_from_datalake_task' is not defined
[2023-01-04 20:41:04,873] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:04,955] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 3.484 seconds
[2023-01-04 20:41:35,087] {processor.py:153} INFO - Started process (PID=48) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:35,092] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:41:35,093] {logging_mixin.py:115} INFO - [2023-01-04 20:41:35,092] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:36,205] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:41:36,206] {logging_mixin.py:115} INFO - [2023-01-04 20:41:36,206] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:41:36,207] {logging_mixin.py:115} INFO - [2023-01-04 20:41:36,207] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:41:36,214] {logging_mixin.py:115} INFO - [2023-01-04 20:41:36,213] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 85, in <module>
    download_stock_data_from_datalake_task >> create_cluster_dataproc_task >> submit_spark_job_task >> delete_cluster_dataproc_task
NameError: name 'download_stock_data_from_datalake_task' is not defined
[2023-01-04 20:41:36,214] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:41:36,234] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.156 seconds
[2023-01-04 20:42:06,304] {processor.py:153} INFO - Started process (PID=71) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:06,305] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:42:06,305] {logging_mixin.py:115} INFO - [2023-01-04 20:42:06,305] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:07,171] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:42:07,172] {logging_mixin.py:115} INFO - [2023-01-04 20:42:07,172] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:42:07,173] {logging_mixin.py:115} INFO - [2023-01-04 20:42:07,172] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:42:07,179] {logging_mixin.py:115} INFO - [2023-01-04 20:42:07,179] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 85, in <module>
    download_stock_data_from_datalake_task >> create_cluster_dataproc_task >> submit_spark_job_task >> delete_cluster_dataproc_task
NameError: name 'download_stock_data_from_datalake_task' is not defined
[2023-01-04 20:42:07,180] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:07,205] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.905 seconds
[2023-01-04 20:42:16,178] {processor.py:153} INFO - Started process (PID=79) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:16,179] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:42:16,179] {logging_mixin.py:115} INFO - [2023-01-04 20:42:16,179] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:16,973] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:42:16,974] {logging_mixin.py:115} INFO - [2023-01-04 20:42:16,974] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:42:16,975] {logging_mixin.py:115} INFO - [2023-01-04 20:42:16,975] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:42:16,982] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:17,038] {logging_mixin.py:115} INFO - [2023-01-04 20:42:17,038] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:42:17,062] {logging_mixin.py:115} INFO - [2023-01-04 20:42:17,062] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00
[2023-01-04 20:42:17,080] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.906 seconds
[2023-01-04 20:42:47,177] {processor.py:153} INFO - Started process (PID=102) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:47,178] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:42:47,179] {logging_mixin.py:115} INFO - [2023-01-04 20:42:47,179] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:47,982] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:42:47,983] {logging_mixin.py:115} INFO - [2023-01-04 20:42:47,983] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:42:47,983] {logging_mixin.py:115} INFO - [2023-01-04 20:42:47,983] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:42:47,990] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:42:48,012] {logging_mixin.py:115} INFO - [2023-01-04 20:42:48,011] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:42:48,038] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.866 seconds
[2023-01-04 20:43:18,136] {processor.py:153} INFO - Started process (PID=117) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:18,137] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:43:18,138] {logging_mixin.py:115} INFO - [2023-01-04 20:43:18,137] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:18,927] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:43:18,929] {logging_mixin.py:115} INFO - [2023-01-04 20:43:18,929] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:43:18,930] {logging_mixin.py:115} INFO - [2023-01-04 20:43:18,929] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:43:18,936] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:18,963] {logging_mixin.py:115} INFO - [2023-01-04 20:43:18,963] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:43:18,992] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.861 seconds
[2023-01-04 20:43:49,087] {processor.py:153} INFO - Started process (PID=140) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:49,088] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:43:49,089] {logging_mixin.py:115} INFO - [2023-01-04 20:43:49,089] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:49,891] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:43:49,893] {logging_mixin.py:115} INFO - [2023-01-04 20:43:49,893] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:43:49,893] {logging_mixin.py:115} INFO - [2023-01-04 20:43:49,893] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:43:49,900] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:43:49,923] {logging_mixin.py:115} INFO - [2023-01-04 20:43:49,922] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:43:49,951] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.868 seconds
[2023-01-04 20:44:20,038] {processor.py:153} INFO - Started process (PID=164) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:20,039] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:44:20,039] {logging_mixin.py:115} INFO - [2023-01-04 20:44:20,039] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:20,811] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:44:20,812] {logging_mixin.py:115} INFO - [2023-01-04 20:44:20,812] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:44:20,813] {logging_mixin.py:115} INFO - [2023-01-04 20:44:20,812] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:44:20,820] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:20,844] {logging_mixin.py:115} INFO - [2023-01-04 20:44:20,844] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:44:20,876] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.842 seconds
[2023-01-04 20:44:50,948] {processor.py:153} INFO - Started process (PID=187) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:50,950] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:44:50,951] {logging_mixin.py:115} INFO - [2023-01-04 20:44:50,950] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:52,025] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:44:52,027] {logging_mixin.py:115} INFO - [2023-01-04 20:44:52,027] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:44:52,028] {logging_mixin.py:115} INFO - [2023-01-04 20:44:52,028] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:44:52,040] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:44:52,074] {logging_mixin.py:115} INFO - [2023-01-04 20:44:52,074] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:44:52,105] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.163 seconds
[2023-01-04 20:45:22,176] {processor.py:153} INFO - Started process (PID=203) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:22,178] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:45:22,179] {logging_mixin.py:115} INFO - [2023-01-04 20:45:22,179] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:22,982] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:45:22,984] {logging_mixin.py:115} INFO - [2023-01-04 20:45:22,984] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:45:22,984] {logging_mixin.py:115} INFO - [2023-01-04 20:45:22,984] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:45:22,991] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:23,013] {logging_mixin.py:115} INFO - [2023-01-04 20:45:23,013] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:45:23,040] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.868 seconds
[2023-01-04 20:45:53,088] {processor.py:153} INFO - Started process (PID=226) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:53,089] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:45:53,090] {logging_mixin.py:115} INFO - [2023-01-04 20:45:53,090] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:53,883] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:45:53,885] {logging_mixin.py:115} INFO - [2023-01-04 20:45:53,884] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:45:53,885] {logging_mixin.py:115} INFO - [2023-01-04 20:45:53,885] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:45:53,892] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:45:53,914] {logging_mixin.py:115} INFO - [2023-01-04 20:45:53,914] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:45:53,940] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 20:46:24,051] {processor.py:153} INFO - Started process (PID=249) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:24,052] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:46:24,053] {logging_mixin.py:115} INFO - [2023-01-04 20:46:24,053] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:24,850] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:46:24,851] {logging_mixin.py:115} INFO - [2023-01-04 20:46:24,851] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:46:24,851] {logging_mixin.py:115} INFO - [2023-01-04 20:46:24,851] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:46:24,858] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:24,879] {logging_mixin.py:115} INFO - [2023-01-04 20:46:24,879] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:46:24,907] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.860 seconds
[2023-01-04 20:46:55,000] {processor.py:153} INFO - Started process (PID=265) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:55,001] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:46:55,001] {logging_mixin.py:115} INFO - [2023-01-04 20:46:55,001] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:55,812] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:46:55,815] {logging_mixin.py:115} INFO - [2023-01-04 20:46:55,815] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:46:55,815] {logging_mixin.py:115} INFO - [2023-01-04 20:46:55,815] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:46:55,827] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:46:55,867] {logging_mixin.py:115} INFO - [2023-01-04 20:46:55,866] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:46:55,899] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-04 20:47:26,003] {processor.py:153} INFO - Started process (PID=287) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:26,004] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:47:26,005] {logging_mixin.py:115} INFO - [2023-01-04 20:47:26,005] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:26,784] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:47:26,785] {logging_mixin.py:115} INFO - [2023-01-04 20:47:26,785] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:47:26,786] {logging_mixin.py:115} INFO - [2023-01-04 20:47:26,786] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:47:26,793] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:26,814] {logging_mixin.py:115} INFO - [2023-01-04 20:47:26,814] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:47:26,842] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 20:47:56,937] {processor.py:153} INFO - Started process (PID=310) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:56,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:47:56,939] {logging_mixin.py:115} INFO - [2023-01-04 20:47:56,939] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:57,723] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:47:57,725] {logging_mixin.py:115} INFO - [2023-01-04 20:47:57,725] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:47:57,725] {logging_mixin.py:115} INFO - [2023-01-04 20:47:57,725] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:47:57,732] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:47:57,754] {logging_mixin.py:115} INFO - [2023-01-04 20:47:57,753] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:47:57,781] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 20:48:27,850] {processor.py:153} INFO - Started process (PID=333) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:27,851] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:48:27,852] {logging_mixin.py:115} INFO - [2023-01-04 20:48:27,852] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:28,633] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:48:28,635] {logging_mixin.py:115} INFO - [2023-01-04 20:48:28,635] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:48:28,635] {logging_mixin.py:115} INFO - [2023-01-04 20:48:28,635] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:48:28,642] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:28,665] {logging_mixin.py:115} INFO - [2023-01-04 20:48:28,664] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:48:28,697] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.852 seconds
[2023-01-04 20:48:58,788] {processor.py:153} INFO - Started process (PID=350) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:58,789] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:48:58,790] {logging_mixin.py:115} INFO - [2023-01-04 20:48:58,790] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:59,650] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:48:59,651] {logging_mixin.py:115} INFO - [2023-01-04 20:48:59,651] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:48:59,652] {logging_mixin.py:115} INFO - [2023-01-04 20:48:59,651] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:48:59,658] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:48:59,701] {logging_mixin.py:115} INFO - [2023-01-04 20:48:59,700] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:48:59,744] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-04 20:49:29,820] {processor.py:153} INFO - Started process (PID=373) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:49:29,821] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:49:29,821] {logging_mixin.py:115} INFO - [2023-01-04 20:49:29,821] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:49:30,597] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:49:30,598] {logging_mixin.py:115} INFO - [2023-01-04 20:49:30,598] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:49:30,599] {logging_mixin.py:115} INFO - [2023-01-04 20:49:30,599] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:49:30,606] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:49:30,627] {logging_mixin.py:115} INFO - [2023-01-04 20:49:30,627] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:49:30,654] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 20:50:00,723] {processor.py:153} INFO - Started process (PID=397) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:00,723] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:50:00,724] {logging_mixin.py:115} INFO - [2023-01-04 20:50:00,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:01,527] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:50:01,528] {logging_mixin.py:115} INFO - [2023-01-04 20:50:01,528] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:50:01,529] {logging_mixin.py:115} INFO - [2023-01-04 20:50:01,529] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:50:01,535] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:01,557] {logging_mixin.py:115} INFO - [2023-01-04 20:50:01,557] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:50:01,584] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.866 seconds
[2023-01-04 20:50:31,651] {processor.py:153} INFO - Started process (PID=420) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:31,652] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:50:31,652] {logging_mixin.py:115} INFO - [2023-01-04 20:50:31,652] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:32,444] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:50:32,445] {logging_mixin.py:115} INFO - [2023-01-04 20:50:32,445] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:50:32,446] {logging_mixin.py:115} INFO - [2023-01-04 20:50:32,445] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:50:32,452] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:50:32,474] {logging_mixin.py:115} INFO - [2023-01-04 20:50:32,473] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:50:32,501] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 20:51:02,616] {processor.py:153} INFO - Started process (PID=436) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:02,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:51:02,618] {logging_mixin.py:115} INFO - [2023-01-04 20:51:02,618] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:03,451] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:51:03,453] {logging_mixin.py:115} INFO - [2023-01-04 20:51:03,452] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:51:03,453] {logging_mixin.py:115} INFO - [2023-01-04 20:51:03,453] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:51:03,460] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:03,482] {logging_mixin.py:115} INFO - [2023-01-04 20:51:03,482] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:51:03,517] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.907 seconds
[2023-01-04 20:51:33,617] {processor.py:153} INFO - Started process (PID=460) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:33,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:51:33,619] {logging_mixin.py:115} INFO - [2023-01-04 20:51:33,619] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:34,395] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:51:34,396] {logging_mixin.py:115} INFO - [2023-01-04 20:51:34,396] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:51:34,397] {logging_mixin.py:115} INFO - [2023-01-04 20:51:34,397] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:51:34,404] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:51:34,425] {logging_mixin.py:115} INFO - [2023-01-04 20:51:34,425] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:51:34,452] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 20:52:04,547] {processor.py:153} INFO - Started process (PID=483) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:04,551] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:52:04,552] {logging_mixin.py:115} INFO - [2023-01-04 20:52:04,552] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:05,328] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:52:05,329] {logging_mixin.py:115} INFO - [2023-01-04 20:52:05,329] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:52:05,330] {logging_mixin.py:115} INFO - [2023-01-04 20:52:05,330] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:52:05,337] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:05,358] {logging_mixin.py:115} INFO - [2023-01-04 20:52:05,358] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:52:05,384] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.842 seconds
[2023-01-04 20:52:35,484] {processor.py:153} INFO - Started process (PID=507) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:35,486] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:52:35,486] {logging_mixin.py:115} INFO - [2023-01-04 20:52:35,486] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:36,302] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:52:36,303] {logging_mixin.py:115} INFO - [2023-01-04 20:52:36,303] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:52:36,303] {logging_mixin.py:115} INFO - [2023-01-04 20:52:36,303] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:52:36,310] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:52:36,336] {logging_mixin.py:115} INFO - [2023-01-04 20:52:36,336] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:52:36,363] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.883 seconds
[2023-01-04 20:53:06,442] {processor.py:153} INFO - Started process (PID=523) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:06,443] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:53:06,444] {logging_mixin.py:115} INFO - [2023-01-04 20:53:06,444] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:07,301] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:53:07,303] {logging_mixin.py:115} INFO - [2023-01-04 20:53:07,303] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:53:07,304] {logging_mixin.py:115} INFO - [2023-01-04 20:53:07,303] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:53:07,311] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:07,335] {logging_mixin.py:115} INFO - [2023-01-04 20:53:07,334] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:53:07,363] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-04 20:53:37,439] {processor.py:153} INFO - Started process (PID=548) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:37,440] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:53:37,441] {logging_mixin.py:115} INFO - [2023-01-04 20:53:37,441] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:38,231] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:53:38,233] {logging_mixin.py:115} INFO - [2023-01-04 20:53:38,233] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:53:38,233] {logging_mixin.py:115} INFO - [2023-01-04 20:53:38,233] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:53:38,240] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:53:38,262] {logging_mixin.py:115} INFO - [2023-01-04 20:53:38,262] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:53:38,289] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 20:54:08,370] {processor.py:153} INFO - Started process (PID=572) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:08,373] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:54:08,374] {logging_mixin.py:115} INFO - [2023-01-04 20:54:08,374] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:09,157] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 20:54:09,158] {logging_mixin.py:115} INFO - [2023-01-04 20:54:09,158] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 20:54:09,158] {logging_mixin.py:115} INFO - [2023-01-04 20:54:09,158] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 20:54:09,165] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:09,187] {logging_mixin.py:115} INFO - [2023-01-04 20:54:09,187] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 20:54:09,215] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 20:54:09,962] {processor.py:153} INFO - Started process (PID=574) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:09,963] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:54:09,964] {logging_mixin.py:115} INFO - [2023-01-04 20:54:09,964] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:09,970] {logging_mixin.py:115} INFO - [2023-01-04 20:54:09,969] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:54:09,971] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:10,002] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.045 seconds
[2023-01-04 20:54:40,031] {processor.py:153} INFO - Started process (PID=595) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:40,032] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:54:40,033] {logging_mixin.py:115} INFO - [2023-01-04 20:54:40,033] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:40,037] {logging_mixin.py:115} INFO - [2023-01-04 20:54:40,036] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:54:40,038] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:54:40,062] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.036 seconds
[2023-01-04 20:55:10,128] {processor.py:153} INFO - Started process (PID=611) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:10,129] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:55:10,129] {logging_mixin.py:115} INFO - [2023-01-04 20:55:10,129] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:10,133] {logging_mixin.py:115} INFO - [2023-01-04 20:55:10,132] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:55:10,134] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:10,157] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 20:55:40,199] {processor.py:153} INFO - Started process (PID=636) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:40,203] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:55:40,203] {logging_mixin.py:115} INFO - [2023-01-04 20:55:40,203] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:40,207] {logging_mixin.py:115} INFO - [2023-01-04 20:55:40,206] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:55:40,208] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:55:40,232] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.037 seconds
[2023-01-04 20:56:10,297] {processor.py:153} INFO - Started process (PID=659) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:10,298] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:56:10,298] {logging_mixin.py:115} INFO - [2023-01-04 20:56:10,298] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:10,302] {logging_mixin.py:115} INFO - [2023-01-04 20:56:10,301] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:56:10,302] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:10,324] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.031 seconds
[2023-01-04 20:56:40,372] {processor.py:153} INFO - Started process (PID=674) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:40,373] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:56:40,374] {logging_mixin.py:115} INFO - [2023-01-04 20:56:40,374] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:40,377] {logging_mixin.py:115} INFO - [2023-01-04 20:56:40,377] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:56:40,378] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:56:40,401] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.034 seconds
[2023-01-04 20:57:10,449] {processor.py:153} INFO - Started process (PID=697) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:10,449] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:57:10,450] {logging_mixin.py:115} INFO - [2023-01-04 20:57:10,450] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:10,453] {logging_mixin.py:115} INFO - [2023-01-04 20:57:10,453] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:57:10,454] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:10,477] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.032 seconds
[2023-01-04 20:57:40,525] {processor.py:153} INFO - Started process (PID=719) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:40,527] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:57:40,527] {logging_mixin.py:115} INFO - [2023-01-04 20:57:40,527] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:40,531] {logging_mixin.py:115} INFO - [2023-01-04 20:57:40,530] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:57:40,531] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:57:40,554] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.034 seconds
[2023-01-04 20:58:10,605] {processor.py:153} INFO - Started process (PID=734) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:10,606] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:58:10,606] {logging_mixin.py:115} INFO - [2023-01-04 20:58:10,606] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:10,610] {logging_mixin.py:115} INFO - [2023-01-04 20:58:10,609] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:58:10,610] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:10,641] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.040 seconds
[2023-01-04 20:58:36,673] {processor.py:153} INFO - Started process (PID=756) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:36,674] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:58:36,675] {logging_mixin.py:115} INFO - [2023-01-04 20:58:36,674] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:36,679] {logging_mixin.py:115} INFO - [2023-01-04 20:58:36,679] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:58:36,680] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:58:36,702] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.034 seconds
[2023-01-04 20:59:06,764] {processor.py:153} INFO - Started process (PID=771) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:06,765] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:59:06,765] {logging_mixin.py:115} INFO - [2023-01-04 20:59:06,765] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:06,769] {logging_mixin.py:115} INFO - [2023-01-04 20:59:06,768] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:59:06,769] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:06,793] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 20:59:36,836] {processor.py:153} INFO - Started process (PID=792) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:36,837] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 20:59:36,837] {logging_mixin.py:115} INFO - [2023-01-04 20:59:36,837] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:36,841] {logging_mixin.py:115} INFO - [2023-01-04 20:59:36,840] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 20:59:36,841] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 20:59:36,864] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 21:00:06,919] {processor.py:153} INFO - Started process (PID=813) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:06,920] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:00:06,921] {logging_mixin.py:115} INFO - [2023-01-04 21:00:06,921] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:06,924] {logging_mixin.py:115} INFO - [2023-01-04 21:00:06,923] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    from my_script import my_python_function
ModuleNotFoundError: No module named 'my_script'
[2023-01-04 21:00:06,925] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:06,947] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.033 seconds
[2023-01-04 21:00:36,990] {processor.py:153} INFO - Started process (PID=828) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:36,991] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:00:36,992] {logging_mixin.py:115} INFO - [2023-01-04 21:00:36,992] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:37,829] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:00:37,831] {logging_mixin.py:115} INFO - [2023-01-04 21:00:37,830] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:00:37,831] {logging_mixin.py:115} INFO - [2023-01-04 21:00:37,831] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:00:37,838] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:00:37,899] {logging_mixin.py:115} INFO - [2023-01-04 21:00:37,898] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:00:37,933] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.947 seconds
[2023-01-04 21:01:08,049] {processor.py:153} INFO - Started process (PID=852) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:08,051] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:01:08,051] {logging_mixin.py:115} INFO - [2023-01-04 21:01:08,051] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:08,853] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:01:08,854] {logging_mixin.py:115} INFO - [2023-01-04 21:01:08,854] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:01:08,855] {logging_mixin.py:115} INFO - [2023-01-04 21:01:08,855] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:01:08,862] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:08,884] {logging_mixin.py:115} INFO - [2023-01-04 21:01:08,883] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:01:08,918] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 21:01:39,022] {processor.py:153} INFO - Started process (PID=875) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:39,023] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:01:39,024] {logging_mixin.py:115} INFO - [2023-01-04 21:01:39,024] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:39,802] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:01:39,803] {logging_mixin.py:115} INFO - [2023-01-04 21:01:39,803] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:01:39,803] {logging_mixin.py:115} INFO - [2023-01-04 21:01:39,803] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:01:39,811] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:01:39,837] {logging_mixin.py:115} INFO - [2023-01-04 21:01:39,837] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:01:39,866] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 21:02:09,960] {processor.py:153} INFO - Started process (PID=897) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:09,962] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:02:09,962] {logging_mixin.py:115} INFO - [2023-01-04 21:02:09,962] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:10,883] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:02:10,884] {logging_mixin.py:115} INFO - [2023-01-04 21:02:10,884] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:02:10,885] {logging_mixin.py:115} INFO - [2023-01-04 21:02:10,884] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:02:10,891] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:10,912] {logging_mixin.py:115} INFO - [2023-01-04 21:02:10,912] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:02:10,938] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.983 seconds
[2023-01-04 21:02:41,033] {processor.py:153} INFO - Started process (PID=913) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:41,034] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:02:41,035] {logging_mixin.py:115} INFO - [2023-01-04 21:02:41,034] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:41,887] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:02:41,889] {logging_mixin.py:115} INFO - [2023-01-04 21:02:41,889] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:02:41,890] {logging_mixin.py:115} INFO - [2023-01-04 21:02:41,889] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:02:41,907] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:02:41,951] {logging_mixin.py:115} INFO - [2023-01-04 21:02:41,951] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:02:41,991] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.962 seconds
[2023-01-04 21:03:12,089] {processor.py:153} INFO - Started process (PID=939) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:12,091] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:03:12,091] {logging_mixin.py:115} INFO - [2023-01-04 21:03:12,091] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:12,879] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:03:12,880] {logging_mixin.py:115} INFO - [2023-01-04 21:03:12,880] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:03:12,881] {logging_mixin.py:115} INFO - [2023-01-04 21:03:12,881] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:03:12,888] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:12,909] {logging_mixin.py:115} INFO - [2023-01-04 21:03:12,909] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:03:12,936] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 21:03:43,032] {processor.py:153} INFO - Started process (PID=961) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:43,033] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:03:43,034] {logging_mixin.py:115} INFO - [2023-01-04 21:03:43,034] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:43,819] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:03:43,821] {logging_mixin.py:115} INFO - [2023-01-04 21:03:43,820] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:03:43,821] {logging_mixin.py:115} INFO - [2023-01-04 21:03:43,821] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:03:43,828] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:03:43,849] {logging_mixin.py:115} INFO - [2023-01-04 21:03:43,849] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:03:43,875] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 21:04:13,969] {processor.py:153} INFO - Started process (PID=984) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:13,970] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:04:13,972] {logging_mixin.py:115} INFO - [2023-01-04 21:04:13,972] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:14,845] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:04:14,847] {logging_mixin.py:115} INFO - [2023-01-04 21:04:14,847] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:04:14,847] {logging_mixin.py:115} INFO - [2023-01-04 21:04:14,847] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:04:14,858] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:14,887] {logging_mixin.py:115} INFO - [2023-01-04 21:04:14,886] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:04:14,920] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.956 seconds
[2023-01-04 21:04:17,962] {processor.py:153} INFO - Started process (PID=986) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:17,963] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:04:17,964] {logging_mixin.py:115} INFO - [2023-01-04 21:04:17,963] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:18,769] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:04:18,771] {logging_mixin.py:115} INFO - [2023-01-04 21:04:18,770] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:04:18,771] {logging_mixin.py:115} INFO - [2023-01-04 21:04:18,771] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:04:18,778] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:18,830] {logging_mixin.py:115} INFO - [2023-01-04 21:04:18,829] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:04:18,861] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-04 21:04:48,932] {processor.py:153} INFO - Started process (PID=1009) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:48,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:04:48,939] {logging_mixin.py:115} INFO - [2023-01-04 21:04:48,939] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:49,760] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:04:49,761] {logging_mixin.py:115} INFO - [2023-01-04 21:04:49,761] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:04:49,761] {logging_mixin.py:115} INFO - [2023-01-04 21:04:49,761] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:04:49,768] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:04:49,796] {logging_mixin.py:115} INFO - [2023-01-04 21:04:49,796] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:04:49,827] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.899 seconds
[2023-01-04 21:05:19,904] {processor.py:153} INFO - Started process (PID=1025) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:19,905] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:05:19,906] {logging_mixin.py:115} INFO - [2023-01-04 21:05:19,906] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:20,702] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:05:20,703] {logging_mixin.py:115} INFO - [2023-01-04 21:05:20,703] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:05:20,704] {logging_mixin.py:115} INFO - [2023-01-04 21:05:20,703] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:05:20,710] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:20,733] {logging_mixin.py:115} INFO - [2023-01-04 21:05:20,732] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:05:20,761] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 21:05:50,833] {processor.py:153} INFO - Started process (PID=1049) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:50,834] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:05:50,835] {logging_mixin.py:115} INFO - [2023-01-04 21:05:50,835] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:51,598] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:05:51,600] {logging_mixin.py:115} INFO - [2023-01-04 21:05:51,599] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:05:51,600] {logging_mixin.py:115} INFO - [2023-01-04 21:05:51,600] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:05:51,607] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:05:51,629] {logging_mixin.py:115} INFO - [2023-01-04 21:05:51,628] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:05:51,656] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.828 seconds
[2023-01-04 21:06:21,726] {processor.py:153} INFO - Started process (PID=1073) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:21,727] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:06:21,728] {logging_mixin.py:115} INFO - [2023-01-04 21:06:21,728] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:22,511] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:06:22,513] {logging_mixin.py:115} INFO - [2023-01-04 21:06:22,513] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:06:22,513] {logging_mixin.py:115} INFO - [2023-01-04 21:06:22,513] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:06:22,520] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:22,542] {logging_mixin.py:115} INFO - [2023-01-04 21:06:22,541] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:06:22,569] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 21:06:52,617] {processor.py:153} INFO - Started process (PID=1095) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:52,618] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:06:52,618] {logging_mixin.py:115} INFO - [2023-01-04 21:06:52,618] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:53,387] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:06:53,388] {logging_mixin.py:115} INFO - [2023-01-04 21:06:53,388] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:06:53,388] {logging_mixin.py:115} INFO - [2023-01-04 21:06:53,388] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:06:53,395] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:06:53,416] {logging_mixin.py:115} INFO - [2023-01-04 21:06:53,416] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:06:53,443] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.830 seconds
[2023-01-04 21:07:23,541] {processor.py:153} INFO - Started process (PID=1111) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:23,542] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:07:23,543] {logging_mixin.py:115} INFO - [2023-01-04 21:07:23,543] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:24,373] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:07:24,375] {logging_mixin.py:115} INFO - [2023-01-04 21:07:24,374] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:07:24,375] {logging_mixin.py:115} INFO - [2023-01-04 21:07:24,375] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:07:24,382] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:24,403] {logging_mixin.py:115} INFO - [2023-01-04 21:07:24,403] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:07:24,430] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.895 seconds
[2023-01-04 21:07:54,536] {processor.py:153} INFO - Started process (PID=1134) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:54,537] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:07:54,537] {logging_mixin.py:115} INFO - [2023-01-04 21:07:54,537] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:55,314] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:07:55,315] {logging_mixin.py:115} INFO - [2023-01-04 21:07:55,315] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:07:55,316] {logging_mixin.py:115} INFO - [2023-01-04 21:07:55,316] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:07:55,323] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:07:55,344] {logging_mixin.py:115} INFO - [2023-01-04 21:07:55,344] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:07:55,371] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 21:08:25,463] {processor.py:153} INFO - Started process (PID=1158) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:25,464] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:08:25,465] {logging_mixin.py:115} INFO - [2023-01-04 21:08:25,465] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:26,246] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:08:26,247] {logging_mixin.py:115} INFO - [2023-01-04 21:08:26,247] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:08:26,248] {logging_mixin.py:115} INFO - [2023-01-04 21:08:26,247] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:08:26,254] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:26,276] {logging_mixin.py:115} INFO - [2023-01-04 21:08:26,275] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:08:26,302] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 21:08:56,373] {processor.py:153} INFO - Started process (PID=1183) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:56,374] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:08:56,374] {logging_mixin.py:115} INFO - [2023-01-04 21:08:56,374] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:57,137] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:08:57,138] {logging_mixin.py:115} INFO - [2023-01-04 21:08:57,138] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:08:57,139] {logging_mixin.py:115} INFO - [2023-01-04 21:08:57,138] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:08:57,145] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:08:57,166] {logging_mixin.py:115} INFO - [2023-01-04 21:08:57,166] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:08:57,192] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.824 seconds
[2023-01-04 21:09:27,263] {processor.py:153} INFO - Started process (PID=1199) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:27,264] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:09:27,265] {logging_mixin.py:115} INFO - [2023-01-04 21:09:27,265] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:28,056] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:09:28,057] {logging_mixin.py:115} INFO - [2023-01-04 21:09:28,057] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:09:28,057] {logging_mixin.py:115} INFO - [2023-01-04 21:09:28,057] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:09:28,064] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:28,086] {logging_mixin.py:115} INFO - [2023-01-04 21:09:28,086] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:09:28,113] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 21:09:58,185] {processor.py:153} INFO - Started process (PID=1221) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:58,186] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:09:58,187] {logging_mixin.py:115} INFO - [2023-01-04 21:09:58,187] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:59,052] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:09:59,054] {logging_mixin.py:115} INFO - [2023-01-04 21:09:59,054] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:09:59,054] {logging_mixin.py:115} INFO - [2023-01-04 21:09:59,054] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:09:59,062] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:09:59,086] {logging_mixin.py:115} INFO - [2023-01-04 21:09:59,085] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:09:59,112] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.932 seconds
[2023-01-04 21:10:29,179] {processor.py:153} INFO - Started process (PID=1245) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:10:29,180] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:10:29,180] {logging_mixin.py:115} INFO - [2023-01-04 21:10:29,180] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:10:29,963] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:10:29,964] {logging_mixin.py:115} INFO - [2023-01-04 21:10:29,964] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:10:29,965] {logging_mixin.py:115} INFO - [2023-01-04 21:10:29,965] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:10:29,972] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:10:29,994] {logging_mixin.py:115} INFO - [2023-01-04 21:10:29,994] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:10:30,021] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 21:11:00,064] {processor.py:153} INFO - Started process (PID=1269) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:00,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:11:00,066] {logging_mixin.py:115} INFO - [2023-01-04 21:11:00,066] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:00,903] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:11:00,904] {logging_mixin.py:115} INFO - [2023-01-04 21:11:00,904] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:11:00,905] {logging_mixin.py:115} INFO - [2023-01-04 21:11:00,904] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:11:00,911] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:00,933] {logging_mixin.py:115} INFO - [2023-01-04 21:11:00,932] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:11:00,959] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.900 seconds
[2023-01-04 21:11:16,055] {processor.py:153} INFO - Started process (PID=1278) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:16,055] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:11:16,056] {logging_mixin.py:115} INFO - [2023-01-04 21:11:16,056] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:16,850] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:11:16,852] {logging_mixin.py:115} INFO - [2023-01-04 21:11:16,852] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:11:16,852] {logging_mixin.py:115} INFO - [2023-01-04 21:11:16,852] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:11:16,859] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:16,910] {logging_mixin.py:115} INFO - [2023-01-04 21:11:16,910] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:11:16,940] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-04 21:11:45,033] {processor.py:153} INFO - Started process (PID=1301) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:45,034] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:11:45,035] {logging_mixin.py:115} INFO - [2023-01-04 21:11:45,034] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:45,882] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:11:45,883] {logging_mixin.py:115} INFO - [2023-01-04 21:11:45,883] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:11:45,883] {logging_mixin.py:115} INFO - [2023-01-04 21:11:45,883] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:11:45,890] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:11:45,902] {logging_mixin.py:115} INFO - [2023-01-04 21:11:45,902] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:11:45,933] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.905 seconds
[2023-01-04 21:12:16,032] {processor.py:153} INFO - Started process (PID=1317) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:16,033] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:12:16,034] {logging_mixin.py:115} INFO - [2023-01-04 21:12:16,034] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:16,818] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:12:16,819] {logging_mixin.py:115} INFO - [2023-01-04 21:12:16,819] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:12:16,820] {logging_mixin.py:115} INFO - [2023-01-04 21:12:16,819] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:12:16,826] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:16,879] {logging_mixin.py:115} INFO - [2023-01-04 21:12:16,879] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:12:16,898] {logging_mixin.py:115} INFO - [2023-01-04 21:12:16,898] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:12:16,916] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.890 seconds
[2023-01-04 21:12:47,014] {processor.py:153} INFO - Started process (PID=1340) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:47,015] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:12:47,016] {logging_mixin.py:115} INFO - [2023-01-04 21:12:47,016] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:47,822] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:12:47,824] {logging_mixin.py:115} INFO - [2023-01-04 21:12:47,824] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:12:47,824] {logging_mixin.py:115} INFO - [2023-01-04 21:12:47,824] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:12:47,831] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:12:47,853] {logging_mixin.py:115} INFO - [2023-01-04 21:12:47,853] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:12:47,873] {logging_mixin.py:115} INFO - [2023-01-04 21:12:47,873] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:12:47,883] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.875 seconds
[2023-01-04 21:13:17,978] {processor.py:153} INFO - Started process (PID=1363) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:17,978] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:13:17,979] {logging_mixin.py:115} INFO - [2023-01-04 21:13:17,979] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:18,768] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:13:18,769] {logging_mixin.py:115} INFO - [2023-01-04 21:13:18,769] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:13:18,770] {logging_mixin.py:115} INFO - [2023-01-04 21:13:18,770] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:13:18,777] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:18,798] {logging_mixin.py:115} INFO - [2023-01-04 21:13:18,798] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:13:18,818] {logging_mixin.py:115} INFO - [2023-01-04 21:13:18,818] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:13:18,828] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 21:13:48,901] {processor.py:153} INFO - Started process (PID=1386) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:48,903] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:13:48,903] {logging_mixin.py:115} INFO - [2023-01-04 21:13:48,903] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:49,710] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:13:49,712] {logging_mixin.py:115} INFO - [2023-01-04 21:13:49,712] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:13:49,712] {logging_mixin.py:115} INFO - [2023-01-04 21:13:49,712] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:13:49,721] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:13:49,744] {logging_mixin.py:115} INFO - [2023-01-04 21:13:49,744] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:13:49,764] {logging_mixin.py:115} INFO - [2023-01-04 21:13:49,764] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:13:49,773] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.876 seconds
[2023-01-04 21:14:19,846] {processor.py:153} INFO - Started process (PID=1404) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:19,848] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:14:19,848] {logging_mixin.py:115} INFO - [2023-01-04 21:14:19,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:20,638] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:14:20,639] {logging_mixin.py:115} INFO - [2023-01-04 21:14:20,639] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:14:20,640] {logging_mixin.py:115} INFO - [2023-01-04 21:14:20,639] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:14:20,646] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:20,669] {logging_mixin.py:115} INFO - [2023-01-04 21:14:20,669] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:14:20,690] {logging_mixin.py:115} INFO - [2023-01-04 21:14:20,690] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:14:20,700] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 21:14:31,394] {processor.py:153} INFO - Started process (PID=1413) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:31,395] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:14:31,395] {logging_mixin.py:115} INFO - [2023-01-04 21:14:31,395] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:32,256] {logging_mixin.py:115} INFO - [2023-01-04 21:14:32,255] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_S&P_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_S' is not defined
[2023-01-04 21:14:32,256] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:32,293] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.903 seconds
[2023-01-04 21:14:57,785] {processor.py:153} INFO - Started process (PID=1436) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:57,786] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:14:57,787] {logging_mixin.py:115} INFO - [2023-01-04 21:14:57,787] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:58,600] {logging_mixin.py:115} INFO - [2023-01-04 21:14:58,599] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:14:58,600] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:14:58,617] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 21:15:28,689] {processor.py:153} INFO - Started process (PID=1451) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:15:28,694] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:15:28,695] {logging_mixin.py:115} INFO - [2023-01-04 21:15:28,695] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:15:29,491] {logging_mixin.py:115} INFO - [2023-01-04 21:15:29,490] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:15:29,491] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:15:29,509] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.824 seconds
[2023-01-04 21:15:59,575] {processor.py:153} INFO - Started process (PID=1474) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:15:59,577] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:15:59,577] {logging_mixin.py:115} INFO - [2023-01-04 21:15:59,577] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:16:00,348] {logging_mixin.py:115} INFO - [2023-01-04 21:16:00,347] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:16:00,349] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:16:00,366] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.795 seconds
[2023-01-04 21:16:30,484] {processor.py:153} INFO - Started process (PID=1498) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:16:30,487] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:16:30,488] {logging_mixin.py:115} INFO - [2023-01-04 21:16:30,488] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:16:31,254] {logging_mixin.py:115} INFO - [2023-01-04 21:16:31,253] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:16:31,255] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:16:31,272] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.793 seconds
[2023-01-04 21:17:01,368] {processor.py:153} INFO - Started process (PID=1522) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:01,369] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:17:01,370] {logging_mixin.py:115} INFO - [2023-01-04 21:17:01,370] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:02,180] {logging_mixin.py:115} INFO - [2023-01-04 21:17:02,180] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:17:02,181] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:02,198] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 21:17:32,267] {processor.py:153} INFO - Started process (PID=1539) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:32,268] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:17:32,268] {logging_mixin.py:115} INFO - [2023-01-04 21:17:32,268] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:33,050] {logging_mixin.py:115} INFO - [2023-01-04 21:17:33,049] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:17:33,050] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:17:33,067] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.805 seconds
[2023-01-04 21:18:03,136] {processor.py:153} INFO - Started process (PID=1562) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:03,137] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:18:03,138] {logging_mixin.py:115} INFO - [2023-01-04 21:18:03,138] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:03,926] {logging_mixin.py:115} INFO - [2023-01-04 21:18:03,925] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:18:03,926] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:03,943] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.812 seconds
[2023-01-04 21:18:34,008] {processor.py:153} INFO - Started process (PID=1587) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:34,009] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:18:34,009] {logging_mixin.py:115} INFO - [2023-01-04 21:18:34,009] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:34,790] {logging_mixin.py:115} INFO - [2023-01-04 21:18:34,789] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:18:34,791] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:18:34,808] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.804 seconds
[2023-01-04 21:19:04,907] {processor.py:153} INFO - Started process (PID=1611) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:04,909] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:19:04,910] {logging_mixin.py:115} INFO - [2023-01-04 21:19:04,910] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:05,802] {logging_mixin.py:115} INFO - [2023-01-04 21:19:05,801] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:19:05,803] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:05,821] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-04 21:19:35,924] {processor.py:153} INFO - Started process (PID=1627) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:35,925] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:19:35,925] {logging_mixin.py:115} INFO - [2023-01-04 21:19:35,925] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:36,731] {logging_mixin.py:115} INFO - [2023-01-04 21:19:36,730] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:19:36,731] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:19:36,748] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.829 seconds
[2023-01-04 21:20:06,840] {processor.py:153} INFO - Started process (PID=1650) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:06,843] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:20:06,844] {logging_mixin.py:115} INFO - [2023-01-04 21:20:06,844] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:07,612] {logging_mixin.py:115} INFO - [2023-01-04 21:20:07,611] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:20:07,612] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:07,629] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.794 seconds
[2023-01-04 21:20:37,697] {processor.py:153} INFO - Started process (PID=1675) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:37,699] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:20:37,699] {logging_mixin.py:115} INFO - [2023-01-04 21:20:37,699] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:38,465] {logging_mixin.py:115} INFO - [2023-01-04 21:20:38,464] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:20:38,465] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:20:38,483] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.790 seconds
[2023-01-04 21:21:08,564] {processor.py:153} INFO - Started process (PID=1691) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:08,565] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:21:08,566] {logging_mixin.py:115} INFO - [2023-01-04 21:21:08,566] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:09,753] {logging_mixin.py:115} INFO - [2023-01-04 21:21:09,751] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:21:09,753] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:09,776] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.217 seconds
[2023-01-04 21:21:39,850] {processor.py:153} INFO - Started process (PID=1716) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:39,852] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:21:39,852] {logging_mixin.py:115} INFO - [2023-01-04 21:21:39,852] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:40,649] {logging_mixin.py:115} INFO - [2023-01-04 21:21:40,648] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:21:40,649] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:21:40,666] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.821 seconds
[2023-01-04 21:22:10,713] {processor.py:153} INFO - Started process (PID=1740) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:10,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:22:10,714] {logging_mixin.py:115} INFO - [2023-01-04 21:22:10,714] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:11,480] {logging_mixin.py:115} INFO - [2023-01-04 21:22:11,479] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:22:11,480] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:11,498] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.790 seconds
[2023-01-04 21:22:41,591] {processor.py:153} INFO - Started process (PID=1764) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:41,593] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:22:41,593] {logging_mixin.py:115} INFO - [2023-01-04 21:22:41,593] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:42,376] {logging_mixin.py:115} INFO - [2023-01-04 21:22:42,375] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 58, in <module>
    python_callable=Load_Current_SP_500_End_of_Day_Prices_Into_GCS,
NameError: name 'Load_Current_SP_500_End_of_Day_Prices_Into_GCS' is not defined
[2023-01-04 21:22:42,376] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:22:42,394] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.807 seconds
[2023-01-04 21:23:00,460] {processor.py:153} INFO - Started process (PID=1773) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:00,461] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:23:00,462] {logging_mixin.py:115} INFO - [2023-01-04 21:23:00,462] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:01,253] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:23:01,255] {logging_mixin.py:115} INFO - [2023-01-04 21:23:01,255] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:23:01,255] {logging_mixin.py:115} INFO - [2023-01-04 21:23:01,255] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:23:01,262] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:01,284] {logging_mixin.py:115} INFO - [2023-01-04 21:23:01,284] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:23:01,304] {logging_mixin.py:115} INFO - [2023-01-04 21:23:01,304] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:23:01,315] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.860 seconds
[2023-01-04 21:23:31,384] {processor.py:153} INFO - Started process (PID=1796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:31,385] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:23:31,385] {logging_mixin.py:115} INFO - [2023-01-04 21:23:31,385] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:32,160] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:23:32,162] {logging_mixin.py:115} INFO - [2023-01-04 21:23:32,161] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:23:32,162] {logging_mixin.py:115} INFO - [2023-01-04 21:23:32,162] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:23:32,169] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:23:32,190] {logging_mixin.py:115} INFO - [2023-01-04 21:23:32,190] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:23:32,210] {logging_mixin.py:115} INFO - [2023-01-04 21:23:32,210] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:23:32,219] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.840 seconds
[2023-01-04 21:24:02,291] {processor.py:153} INFO - Started process (PID=1821) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:02,293] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:24:02,294] {logging_mixin.py:115} INFO - [2023-01-04 21:24:02,294] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:03,078] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:24:03,079] {logging_mixin.py:115} INFO - [2023-01-04 21:24:03,079] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:24:03,080] {logging_mixin.py:115} INFO - [2023-01-04 21:24:03,079] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:24:03,086] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:03,109] {logging_mixin.py:115} INFO - [2023-01-04 21:24:03,109] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:24:03,129] {logging_mixin.py:115} INFO - [2023-01-04 21:24:03,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:24:03,139] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.852 seconds
[2023-01-04 21:24:33,210] {processor.py:153} INFO - Started process (PID=1845) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:33,210] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:24:33,211] {logging_mixin.py:115} INFO - [2023-01-04 21:24:33,211] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:34,201] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:24:34,202] {logging_mixin.py:115} INFO - [2023-01-04 21:24:34,202] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:24:34,203] {logging_mixin.py:115} INFO - [2023-01-04 21:24:34,203] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:24:34,210] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:24:34,232] {logging_mixin.py:115} INFO - [2023-01-04 21:24:34,232] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:24:34,252] {logging_mixin.py:115} INFO - [2023-01-04 21:24:34,252] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:24:34,262] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.058 seconds
[2023-01-04 21:25:04,328] {processor.py:153} INFO - Started process (PID=1861) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:04,329] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:25:04,330] {logging_mixin.py:115} INFO - [2023-01-04 21:25:04,330] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:05,107] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:25:05,108] {logging_mixin.py:115} INFO - [2023-01-04 21:25:05,108] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:25:05,109] {logging_mixin.py:115} INFO - [2023-01-04 21:25:05,108] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:25:05,115] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:05,137] {logging_mixin.py:115} INFO - [2023-01-04 21:25:05,136] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:25:05,157] {logging_mixin.py:115} INFO - [2023-01-04 21:25:05,157] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:25:05,166] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 21:25:35,265] {processor.py:153} INFO - Started process (PID=1884) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:35,266] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:25:35,266] {logging_mixin.py:115} INFO - [2023-01-04 21:25:35,266] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:36,048] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:25:36,049] {logging_mixin.py:115} INFO - [2023-01-04 21:25:36,049] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:25:36,049] {logging_mixin.py:115} INFO - [2023-01-04 21:25:36,049] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:25:36,056] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:25:36,078] {logging_mixin.py:115} INFO - [2023-01-04 21:25:36,078] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:25:36,098] {logging_mixin.py:115} INFO - [2023-01-04 21:25:36,098] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:25:36,107] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 21:26:06,216] {processor.py:153} INFO - Started process (PID=1909) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:06,217] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:26:06,218] {logging_mixin.py:115} INFO - [2023-01-04 21:26:06,218] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:06,994] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:26:06,996] {logging_mixin.py:115} INFO - [2023-01-04 21:26:06,996] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:26:06,996] {logging_mixin.py:115} INFO - [2023-01-04 21:26:06,996] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:26:07,003] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:07,025] {logging_mixin.py:115} INFO - [2023-01-04 21:26:07,025] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:26:07,045] {logging_mixin.py:115} INFO - [2023-01-04 21:26:07,045] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:26:07,055] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 21:26:27,123] {processor.py:153} INFO - Started process (PID=1925) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:27,124] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:26:27,125] {logging_mixin.py:115} INFO - [2023-01-04 21:26:27,125] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:27,913] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:26:27,915] {logging_mixin.py:115} INFO - [2023-01-04 21:26:27,914] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:26:27,915] {logging_mixin.py:115} INFO - [2023-01-04 21:26:27,915] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:26:27,922] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:27,943] {logging_mixin.py:115} INFO - [2023-01-04 21:26:27,943] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:26:27,964] {logging_mixin.py:115} INFO - [2023-01-04 21:26:27,964] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:26:27,975] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.857 seconds
[2023-01-04 21:26:58,044] {processor.py:153} INFO - Started process (PID=1941) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:58,045] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:26:58,046] {logging_mixin.py:115} INFO - [2023-01-04 21:26:58,046] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:58,835] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:26:58,836] {logging_mixin.py:115} INFO - [2023-01-04 21:26:58,836] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:26:58,837] {logging_mixin.py:115} INFO - [2023-01-04 21:26:58,836] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:26:58,843] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:26:58,865] {logging_mixin.py:115} INFO - [2023-01-04 21:26:58,865] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:26:58,885] {logging_mixin.py:115} INFO - [2023-01-04 21:26:58,885] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:26:58,895] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 21:27:28,969] {processor.py:153} INFO - Started process (PID=1964) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:27:28,973] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:27:28,973] {logging_mixin.py:115} INFO - [2023-01-04 21:27:28,973] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:27:29,788] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:27:29,789] {logging_mixin.py:115} INFO - [2023-01-04 21:27:29,789] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:27:29,790] {logging_mixin.py:115} INFO - [2023-01-04 21:27:29,789] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:27:29,796] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:27:29,818] {logging_mixin.py:115} INFO - [2023-01-04 21:27:29,818] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:27:29,838] {logging_mixin.py:115} INFO - [2023-01-04 21:27:29,838] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:27:29,847] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.883 seconds
[2023-01-04 21:27:59,921] {processor.py:153} INFO - Started process (PID=1988) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:27:59,922] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:27:59,923] {logging_mixin.py:115} INFO - [2023-01-04 21:27:59,923] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:28:00,751] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:28:00,752] {logging_mixin.py:115} INFO - [2023-01-04 21:28:00,752] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:28:00,753] {logging_mixin.py:115} INFO - [2023-01-04 21:28:00,752] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:28:00,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:28:00,781] {logging_mixin.py:115} INFO - [2023-01-04 21:28:00,781] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:28:00,801] {logging_mixin.py:115} INFO - [2023-01-04 21:28:00,801] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:28:00,810] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.893 seconds
[2023-01-04 21:28:30,881] {processor.py:153} INFO - Started process (PID=2004) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:28:30,883] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:28:30,883] {logging_mixin.py:115} INFO - [2023-01-04 21:28:30,883] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:28:31,704] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:28:31,706] {logging_mixin.py:115} INFO - [2023-01-04 21:28:31,706] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:28:31,706] {logging_mixin.py:115} INFO - [2023-01-04 21:28:31,706] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:28:31,713] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:28:31,735] {logging_mixin.py:115} INFO - [2023-01-04 21:28:31,735] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:28:31,761] {logging_mixin.py:115} INFO - [2023-01-04 21:28:31,761] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:28:31,773] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.896 seconds
[2023-01-04 21:29:01,843] {processor.py:153} INFO - Started process (PID=2027) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:01,844] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:29:01,845] {logging_mixin.py:115} INFO - [2023-01-04 21:29:01,845] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:02,616] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:29:02,617] {logging_mixin.py:115} INFO - [2023-01-04 21:29:02,617] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:29:02,618] {logging_mixin.py:115} INFO - [2023-01-04 21:29:02,618] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:29:02,625] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:02,646] {logging_mixin.py:115} INFO - [2023-01-04 21:29:02,646] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:29:02,666] {logging_mixin.py:115} INFO - [2023-01-04 21:29:02,666] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:29:02,675] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.836 seconds
[2023-01-04 21:29:32,773] {processor.py:153} INFO - Started process (PID=2050) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:32,774] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:29:32,775] {logging_mixin.py:115} INFO - [2023-01-04 21:29:32,775] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:33,569] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:29:33,570] {logging_mixin.py:115} INFO - [2023-01-04 21:29:33,570] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:29:33,571] {logging_mixin.py:115} INFO - [2023-01-04 21:29:33,570] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:29:33,577] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:29:33,599] {logging_mixin.py:115} INFO - [2023-01-04 21:29:33,599] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:29:33,619] {logging_mixin.py:115} INFO - [2023-01-04 21:29:33,619] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:29:33,628] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.860 seconds
[2023-01-04 21:30:03,722] {processor.py:153} INFO - Started process (PID=2073) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:03,723] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:30:03,724] {logging_mixin.py:115} INFO - [2023-01-04 21:30:03,724] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:04,506] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:30:04,507] {logging_mixin.py:115} INFO - [2023-01-04 21:30:04,507] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:30:04,508] {logging_mixin.py:115} INFO - [2023-01-04 21:30:04,507] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:30:04,514] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:04,536] {logging_mixin.py:115} INFO - [2023-01-04 21:30:04,536] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:30:04,556] {logging_mixin.py:115} INFO - [2023-01-04 21:30:04,556] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:30:04,565] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 21:30:34,657] {processor.py:153} INFO - Started process (PID=2088) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:34,658] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:30:34,658] {logging_mixin.py:115} INFO - [2023-01-04 21:30:34,658] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:35,441] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:30:35,442] {logging_mixin.py:115} INFO - [2023-01-04 21:30:35,442] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:30:35,443] {logging_mixin.py:115} INFO - [2023-01-04 21:30:35,443] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:30:35,449] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:30:35,471] {logging_mixin.py:115} INFO - [2023-01-04 21:30:35,471] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:30:35,491] {logging_mixin.py:115} INFO - [2023-01-04 21:30:35,491] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:30:35,503] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 21:31:05,579] {processor.py:153} INFO - Started process (PID=2111) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:05,581] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:31:05,581] {logging_mixin.py:115} INFO - [2023-01-04 21:31:05,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:06,374] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:31:06,375] {logging_mixin.py:115} INFO - [2023-01-04 21:31:06,375] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:31:06,376] {logging_mixin.py:115} INFO - [2023-01-04 21:31:06,375] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:31:06,382] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:06,404] {logging_mixin.py:115} INFO - [2023-01-04 21:31:06,403] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:31:06,423] {logging_mixin.py:115} INFO - [2023-01-04 21:31:06,423] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:31:06,432] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 21:31:36,515] {processor.py:153} INFO - Started process (PID=2134) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:36,519] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:31:36,520] {logging_mixin.py:115} INFO - [2023-01-04 21:31:36,520] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:37,306] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:31:37,308] {logging_mixin.py:115} INFO - [2023-01-04 21:31:37,308] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:31:37,308] {logging_mixin.py:115} INFO - [2023-01-04 21:31:37,308] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:31:37,315] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:31:37,336] {logging_mixin.py:115} INFO - [2023-01-04 21:31:37,336] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:31:37,356] {logging_mixin.py:115} INFO - [2023-01-04 21:31:37,355] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:31:37,365] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 21:32:07,440] {processor.py:153} INFO - Started process (PID=2157) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:07,442] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:32:07,442] {logging_mixin.py:115} INFO - [2023-01-04 21:32:07,442] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:08,223] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:32:08,225] {logging_mixin.py:115} INFO - [2023-01-04 21:32:08,224] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:32:08,225] {logging_mixin.py:115} INFO - [2023-01-04 21:32:08,225] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:32:08,232] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:08,253] {logging_mixin.py:115} INFO - [2023-01-04 21:32:08,253] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:32:08,273] {logging_mixin.py:115} INFO - [2023-01-04 21:32:08,273] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:32:08,282] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 21:32:11,138] {processor.py:153} INFO - Started process (PID=2159) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:11,138] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:32:11,139] {logging_mixin.py:115} INFO - [2023-01-04 21:32:11,139] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:11,938] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:32:11,939] {logging_mixin.py:115} INFO - [2023-01-04 21:32:11,939] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:32:11,939] {logging_mixin.py:115} INFO - [2023-01-04 21:32:11,939] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:32:11,946] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:11,998] {logging_mixin.py:115} INFO - [2023-01-04 21:32:11,997] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:32:12,017] {logging_mixin.py:115} INFO - [2023-01-04 21:32:12,017] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:32:12,029] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.897 seconds
[2023-01-04 21:32:42,128] {processor.py:153} INFO - Started process (PID=2182) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:42,129] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:32:42,130] {logging_mixin.py:115} INFO - [2023-01-04 21:32:42,129] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:42,948] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:32:42,949] {logging_mixin.py:115} INFO - [2023-01-04 21:32:42,949] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:32:42,950] {logging_mixin.py:115} INFO - [2023-01-04 21:32:42,950] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:32:42,956] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:32:42,979] {logging_mixin.py:115} INFO - [2023-01-04 21:32:42,978] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:32:42,999] {logging_mixin.py:115} INFO - [2023-01-04 21:32:42,999] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:32:43,009] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.886 seconds
[2023-01-04 21:33:13,104] {processor.py:153} INFO - Started process (PID=2199) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:13,104] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:33:13,105] {logging_mixin.py:115} INFO - [2023-01-04 21:33:13,105] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:13,926] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:33:13,928] {logging_mixin.py:115} INFO - [2023-01-04 21:33:13,928] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:33:13,929] {logging_mixin.py:115} INFO - [2023-01-04 21:33:13,928] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:33:13,940] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:13,963] {logging_mixin.py:115} INFO - [2023-01-04 21:33:13,963] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:33:13,983] {logging_mixin.py:115} INFO - [2023-01-04 21:33:13,983] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:33:13,993] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.894 seconds
[2023-01-04 21:33:44,083] {processor.py:153} INFO - Started process (PID=2224) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:44,084] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:33:44,085] {logging_mixin.py:115} INFO - [2023-01-04 21:33:44,085] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:44,866] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:33:44,867] {logging_mixin.py:115} INFO - [2023-01-04 21:33:44,867] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:33:44,868] {logging_mixin.py:115} INFO - [2023-01-04 21:33:44,867] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:33:44,874] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:33:44,896] {logging_mixin.py:115} INFO - [2023-01-04 21:33:44,896] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:33:44,916] {logging_mixin.py:115} INFO - [2023-01-04 21:33:44,916] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:33:44,926] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 21:34:14,998] {processor.py:153} INFO - Started process (PID=2248) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:14,999] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:34:15,000] {logging_mixin.py:115} INFO - [2023-01-04 21:34:15,000] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:15,784] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:34:15,785] {logging_mixin.py:115} INFO - [2023-01-04 21:34:15,785] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:34:15,786] {logging_mixin.py:115} INFO - [2023-01-04 21:34:15,786] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:34:15,793] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:15,815] {logging_mixin.py:115} INFO - [2023-01-04 21:34:15,815] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:34:15,836] {logging_mixin.py:115} INFO - [2023-01-04 21:34:15,836] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:34:15,846] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 21:34:45,924] {processor.py:153} INFO - Started process (PID=2269) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:45,925] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:34:45,926] {logging_mixin.py:115} INFO - [2023-01-04 21:34:45,926] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:47,061] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:34:47,063] {logging_mixin.py:115} INFO - [2023-01-04 21:34:47,063] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:34:47,063] {logging_mixin.py:115} INFO - [2023-01-04 21:34:47,063] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:34:47,070] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:34:47,093] {logging_mixin.py:115} INFO - [2023-01-04 21:34:47,093] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:34:47,115] {logging_mixin.py:115} INFO - [2023-01-04 21:34:47,115] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:34:47,124] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.206 seconds
[2023-01-04 21:35:17,196] {processor.py:153} INFO - Started process (PID=2285) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:17,197] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:35:17,198] {logging_mixin.py:115} INFO - [2023-01-04 21:35:17,197] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:18,026] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:35:18,027] {logging_mixin.py:115} INFO - [2023-01-04 21:35:18,027] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:35:18,028] {logging_mixin.py:115} INFO - [2023-01-04 21:35:18,027] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:35:18,034] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:18,057] {logging_mixin.py:115} INFO - [2023-01-04 21:35:18,057] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:35:18,077] {logging_mixin.py:115} INFO - [2023-01-04 21:35:18,077] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:35:18,087] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.896 seconds
[2023-01-04 21:35:48,161] {processor.py:153} INFO - Started process (PID=2310) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:48,169] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:35:48,170] {logging_mixin.py:115} INFO - [2023-01-04 21:35:48,170] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:48,986] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:35:48,988] {logging_mixin.py:115} INFO - [2023-01-04 21:35:48,988] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:35:48,988] {logging_mixin.py:115} INFO - [2023-01-04 21:35:48,988] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:35:48,995] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:35:49,018] {logging_mixin.py:115} INFO - [2023-01-04 21:35:49,017] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:35:49,038] {logging_mixin.py:115} INFO - [2023-01-04 21:35:49,038] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:35:49,049] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.892 seconds
[2023-01-04 21:36:19,157] {processor.py:153} INFO - Started process (PID=2332) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:19,159] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:36:19,160] {logging_mixin.py:115} INFO - [2023-01-04 21:36:19,159] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:19,995] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:36:19,996] {logging_mixin.py:115} INFO - [2023-01-04 21:36:19,996] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:36:19,997] {logging_mixin.py:115} INFO - [2023-01-04 21:36:19,996] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:36:20,003] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:20,027] {logging_mixin.py:115} INFO - [2023-01-04 21:36:20,027] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:36:20,054] {logging_mixin.py:115} INFO - [2023-01-04 21:36:20,054] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:36:20,067] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-04 21:36:50,167] {processor.py:153} INFO - Started process (PID=2350) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:50,168] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:36:50,168] {logging_mixin.py:115} INFO - [2023-01-04 21:36:50,168] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:50,976] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:36:50,978] {logging_mixin.py:115} INFO - [2023-01-04 21:36:50,978] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:36:50,978] {logging_mixin.py:115} INFO - [2023-01-04 21:36:50,978] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:36:50,985] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:36:51,007] {logging_mixin.py:115} INFO - [2023-01-04 21:36:51,006] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:36:51,027] {logging_mixin.py:115} INFO - [2023-01-04 21:36:51,026] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:36:51,036] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 21:37:21,135] {processor.py:153} INFO - Started process (PID=2373) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:21,136] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:37:21,137] {logging_mixin.py:115} INFO - [2023-01-04 21:37:21,137] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:21,960] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:37:21,961] {logging_mixin.py:115} INFO - [2023-01-04 21:37:21,961] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:37:21,962] {logging_mixin.py:115} INFO - [2023-01-04 21:37:21,962] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:37:21,969] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:21,992] {logging_mixin.py:115} INFO - [2023-01-04 21:37:21,992] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:37:22,013] {logging_mixin.py:115} INFO - [2023-01-04 21:37:22,013] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:37:22,023] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.893 seconds
[2023-01-04 21:37:52,124] {processor.py:153} INFO - Started process (PID=2396) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:52,125] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:37:52,126] {logging_mixin.py:115} INFO - [2023-01-04 21:37:52,125] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:52,946] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:37:52,948] {logging_mixin.py:115} INFO - [2023-01-04 21:37:52,947] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:37:52,948] {logging_mixin.py:115} INFO - [2023-01-04 21:37:52,948] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:37:52,955] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:37:52,977] {logging_mixin.py:115} INFO - [2023-01-04 21:37:52,977] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:37:52,998] {logging_mixin.py:115} INFO - [2023-01-04 21:37:52,997] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:37:53,008] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.889 seconds
[2023-01-04 21:38:23,109] {processor.py:153} INFO - Started process (PID=2418) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:23,110] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:38:23,111] {logging_mixin.py:115} INFO - [2023-01-04 21:38:23,111] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:24,075] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:38:24,076] {logging_mixin.py:115} INFO - [2023-01-04 21:38:24,076] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:38:24,077] {logging_mixin.py:115} INFO - [2023-01-04 21:38:24,077] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:38:24,088] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:24,120] {logging_mixin.py:115} INFO - [2023-01-04 21:38:24,120] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:38:24,151] {logging_mixin.py:115} INFO - [2023-01-04 21:38:24,151] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:38:24,164] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.060 seconds
[2023-01-04 21:38:54,241] {processor.py:153} INFO - Started process (PID=2434) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:54,242] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:38:54,242] {logging_mixin.py:115} INFO - [2023-01-04 21:38:54,242] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:55,080] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:38:55,081] {logging_mixin.py:115} INFO - [2023-01-04 21:38:55,081] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:38:55,081] {logging_mixin.py:115} INFO - [2023-01-04 21:38:55,081] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:38:55,088] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:38:55,110] {logging_mixin.py:115} INFO - [2023-01-04 21:38:55,109] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:38:55,129] {logging_mixin.py:115} INFO - [2023-01-04 21:38:55,129] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:38:55,138] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.902 seconds
[2023-01-04 21:39:25,242] {processor.py:153} INFO - Started process (PID=2457) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:25,243] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:39:25,244] {logging_mixin.py:115} INFO - [2023-01-04 21:39:25,244] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:26,093] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:39:26,095] {logging_mixin.py:115} INFO - [2023-01-04 21:39:26,094] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:39:26,095] {logging_mixin.py:115} INFO - [2023-01-04 21:39:26,095] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:39:26,102] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:26,124] {logging_mixin.py:115} INFO - [2023-01-04 21:39:26,124] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:39:26,144] {logging_mixin.py:115} INFO - [2023-01-04 21:39:26,144] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:39:26,153] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-04 21:39:56,256] {processor.py:153} INFO - Started process (PID=2481) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:56,257] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:39:56,258] {logging_mixin.py:115} INFO - [2023-01-04 21:39:56,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:57,075] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:39:57,077] {logging_mixin.py:115} INFO - [2023-01-04 21:39:57,076] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:39:57,077] {logging_mixin.py:115} INFO - [2023-01-04 21:39:57,077] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:39:57,084] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:39:57,105] {logging_mixin.py:115} INFO - [2023-01-04 21:39:57,105] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:39:57,125] {logging_mixin.py:115} INFO - [2023-01-04 21:39:57,125] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:39:57,134] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.883 seconds
[2023-01-04 21:40:27,232] {processor.py:153} INFO - Started process (PID=2497) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:27,233] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:40:27,234] {logging_mixin.py:115} INFO - [2023-01-04 21:40:27,233] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:28,046] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:40:28,047] {logging_mixin.py:115} INFO - [2023-01-04 21:40:28,047] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:40:28,047] {logging_mixin.py:115} INFO - [2023-01-04 21:40:28,047] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:40:28,055] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:28,091] {logging_mixin.py:115} INFO - [2023-01-04 21:40:28,091] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:40:28,113] {logging_mixin.py:115} INFO - [2023-01-04 21:40:28,113] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:40:28,123] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.896 seconds
[2023-01-04 21:40:53,220] {processor.py:153} INFO - Started process (PID=2521) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:53,223] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:40:53,224] {logging_mixin.py:115} INFO - [2023-01-04 21:40:53,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:54,031] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:40:54,032] {logging_mixin.py:115} INFO - [2023-01-04 21:40:54,032] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:40:54,033] {logging_mixin.py:115} INFO - [2023-01-04 21:40:54,032] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:40:54,039] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:40:54,093] {logging_mixin.py:115} INFO - [2023-01-04 21:40:54,093] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:40:54,111] {logging_mixin.py:115} INFO - [2023-01-04 21:40:54,111] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:40:54,125] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.910 seconds
[2023-01-04 21:41:24,244] {processor.py:153} INFO - Started process (PID=2538) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:24,244] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:41:24,245] {logging_mixin.py:115} INFO - [2023-01-04 21:41:24,245] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:25,286] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:41:25,287] {logging_mixin.py:115} INFO - [2023-01-04 21:41:25,287] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:41:25,288] {logging_mixin.py:115} INFO - [2023-01-04 21:41:25,288] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:41:25,299] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:25,329] {logging_mixin.py:115} INFO - [2023-01-04 21:41:25,329] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:41:25,358] {logging_mixin.py:115} INFO - [2023-01-04 21:41:25,358] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:41:25,370] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.132 seconds
[2023-01-04 21:41:47,304] {processor.py:153} INFO - Started process (PID=2554) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:47,305] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:41:47,306] {logging_mixin.py:115} INFO - [2023-01-04 21:41:47,306] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:48,473] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:41:48,474] {logging_mixin.py:115} INFO - [2023-01-04 21:41:48,474] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:41:48,475] {logging_mixin.py:115} INFO - [2023-01-04 21:41:48,475] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:41:48,486] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:41:48,568] {logging_mixin.py:115} INFO - [2023-01-04 21:41:48,567] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:41:48,595] {logging_mixin.py:115} INFO - [2023-01-04 21:41:48,594] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:41:48,612] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.315 seconds
[2023-01-04 21:42:18,692] {processor.py:153} INFO - Started process (PID=2578) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:18,692] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:42:18,693] {logging_mixin.py:115} INFO - [2023-01-04 21:42:18,693] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:19,479] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:42:19,480] {logging_mixin.py:115} INFO - [2023-01-04 21:42:19,480] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:42:19,481] {logging_mixin.py:115} INFO - [2023-01-04 21:42:19,481] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:42:19,487] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:19,509] {logging_mixin.py:115} INFO - [2023-01-04 21:42:19,509] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:42:19,529] {logging_mixin.py:115} INFO - [2023-01-04 21:42:19,529] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:42:19,539] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 21:42:49,582] {processor.py:153} INFO - Started process (PID=2602) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:49,584] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:42:49,585] {logging_mixin.py:115} INFO - [2023-01-04 21:42:49,585] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:50,382] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:42:50,383] {logging_mixin.py:115} INFO - [2023-01-04 21:42:50,383] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:42:50,384] {logging_mixin.py:115} INFO - [2023-01-04 21:42:50,383] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:42:50,391] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:42:50,415] {logging_mixin.py:115} INFO - [2023-01-04 21:42:50,415] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:42:50,435] {logging_mixin.py:115} INFO - [2023-01-04 21:42:50,435] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:42:50,444] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.867 seconds
[2023-01-04 21:43:20,548] {processor.py:153} INFO - Started process (PID=2625) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:20,549] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:43:20,550] {logging_mixin.py:115} INFO - [2023-01-04 21:43:20,550] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:21,430] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:43:21,431] {logging_mixin.py:115} INFO - [2023-01-04 21:43:21,431] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:43:21,432] {logging_mixin.py:115} INFO - [2023-01-04 21:43:21,432] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:43:21,439] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:21,460] {logging_mixin.py:115} INFO - [2023-01-04 21:43:21,460] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:43:21,486] {logging_mixin.py:115} INFO - [2023-01-04 21:43:21,486] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:43:21,496] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.955 seconds
[2023-01-04 21:43:51,596] {processor.py:153} INFO - Started process (PID=2642) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:51,597] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:43:51,598] {logging_mixin.py:115} INFO - [2023-01-04 21:43:51,598] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:52,389] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:43:52,391] {logging_mixin.py:115} INFO - [2023-01-04 21:43:52,391] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:43:52,391] {logging_mixin.py:115} INFO - [2023-01-04 21:43:52,391] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:43:52,398] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:43:52,420] {logging_mixin.py:115} INFO - [2023-01-04 21:43:52,420] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:43:52,440] {logging_mixin.py:115} INFO - [2023-01-04 21:43:52,440] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:43:52,450] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 21:44:22,547] {processor.py:153} INFO - Started process (PID=2665) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:22,548] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:44:22,549] {logging_mixin.py:115} INFO - [2023-01-04 21:44:22,549] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:23,326] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:44:23,327] {logging_mixin.py:115} INFO - [2023-01-04 21:44:23,327] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:44:23,328] {logging_mixin.py:115} INFO - [2023-01-04 21:44:23,328] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:44:23,334] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:23,356] {logging_mixin.py:115} INFO - [2023-01-04 21:44:23,356] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:44:23,376] {logging_mixin.py:115} INFO - [2023-01-04 21:44:23,376] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:44:23,385] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 21:44:53,478] {processor.py:153} INFO - Started process (PID=2688) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:53,480] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:44:53,480] {logging_mixin.py:115} INFO - [2023-01-04 21:44:53,480] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:54,276] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:44:54,277] {logging_mixin.py:115} INFO - [2023-01-04 21:44:54,277] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:44:54,278] {logging_mixin.py:115} INFO - [2023-01-04 21:44:54,277] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:44:54,285] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:44:54,307] {logging_mixin.py:115} INFO - [2023-01-04 21:44:54,307] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:44:54,327] {logging_mixin.py:115} INFO - [2023-01-04 21:44:54,327] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:44:54,336] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 21:45:24,415] {processor.py:153} INFO - Started process (PID=2714) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:24,415] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:45:24,416] {logging_mixin.py:115} INFO - [2023-01-04 21:45:24,416] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:25,464] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:45:25,465] {logging_mixin.py:115} INFO - [2023-01-04 21:45:25,465] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:45:25,466] {logging_mixin.py:115} INFO - [2023-01-04 21:45:25,465] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:45:25,472] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:25,494] {logging_mixin.py:115} INFO - [2023-01-04 21:45:25,494] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:45:25,514] {logging_mixin.py:115} INFO - [2023-01-04 21:45:25,513] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:45:25,523] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.115 seconds
[2023-01-04 21:45:55,596] {processor.py:153} INFO - Started process (PID=2731) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:55,597] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:45:55,598] {logging_mixin.py:115} INFO - [2023-01-04 21:45:55,598] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:56,413] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:45:56,415] {logging_mixin.py:115} INFO - [2023-01-04 21:45:56,415] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:45:56,415] {logging_mixin.py:115} INFO - [2023-01-04 21:45:56,415] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:45:56,422] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:45:56,443] {logging_mixin.py:115} INFO - [2023-01-04 21:45:56,443] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:45:56,465] {logging_mixin.py:115} INFO - [2023-01-04 21:45:56,465] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:45:56,475] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.884 seconds
[2023-01-04 21:46:26,567] {processor.py:153} INFO - Started process (PID=2754) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:26,568] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:46:26,568] {logging_mixin.py:115} INFO - [2023-01-04 21:46:26,568] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:27,375] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:46:27,377] {logging_mixin.py:115} INFO - [2023-01-04 21:46:27,377] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:46:27,377] {logging_mixin.py:115} INFO - [2023-01-04 21:46:27,377] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:46:27,384] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:27,406] {logging_mixin.py:115} INFO - [2023-01-04 21:46:27,406] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:46:27,426] {logging_mixin.py:115} INFO - [2023-01-04 21:46:27,426] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:46:27,435] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.873 seconds
[2023-01-04 21:46:57,539] {processor.py:153} INFO - Started process (PID=2777) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:57,540] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:46:57,541] {logging_mixin.py:115} INFO - [2023-01-04 21:46:57,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:58,336] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:46:58,337] {logging_mixin.py:115} INFO - [2023-01-04 21:46:58,337] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:46:58,338] {logging_mixin.py:115} INFO - [2023-01-04 21:46:58,337] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:46:58,344] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:46:58,366] {logging_mixin.py:115} INFO - [2023-01-04 21:46:58,366] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:46:58,386] {logging_mixin.py:115} INFO - [2023-01-04 21:46:58,386] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:46:58,396] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 21:47:28,498] {processor.py:153} INFO - Started process (PID=2793) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:47:28,498] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:47:28,499] {logging_mixin.py:115} INFO - [2023-01-04 21:47:28,499] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:47:29,310] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:47:29,311] {logging_mixin.py:115} INFO - [2023-01-04 21:47:29,311] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:47:29,312] {logging_mixin.py:115} INFO - [2023-01-04 21:47:29,311] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:47:29,319] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:47:29,341] {logging_mixin.py:115} INFO - [2023-01-04 21:47:29,340] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:47:29,360] {logging_mixin.py:115} INFO - [2023-01-04 21:47:29,360] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:47:29,370] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.877 seconds
[2023-01-04 21:47:59,466] {processor.py:153} INFO - Started process (PID=2817) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:47:59,467] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:47:59,468] {logging_mixin.py:115} INFO - [2023-01-04 21:47:59,468] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:48:00,263] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:48:00,265] {logging_mixin.py:115} INFO - [2023-01-04 21:48:00,264] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:48:00,265] {logging_mixin.py:115} INFO - [2023-01-04 21:48:00,265] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:48:00,272] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:48:00,295] {logging_mixin.py:115} INFO - [2023-01-04 21:48:00,295] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:48:00,318] {logging_mixin.py:115} INFO - [2023-01-04 21:48:00,318] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:48:00,329] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.868 seconds
[2023-01-04 21:48:30,427] {processor.py:153} INFO - Started process (PID=2840) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:48:30,428] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:48:30,428] {logging_mixin.py:115} INFO - [2023-01-04 21:48:30,428] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:48:31,224] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:48:31,226] {logging_mixin.py:115} INFO - [2023-01-04 21:48:31,225] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:48:31,226] {logging_mixin.py:115} INFO - [2023-01-04 21:48:31,226] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:48:31,233] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:48:31,255] {logging_mixin.py:115} INFO - [2023-01-04 21:48:31,255] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:48:31,275] {logging_mixin.py:115} INFO - [2023-01-04 21:48:31,274] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:48:31,284] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 21:49:01,362] {processor.py:153} INFO - Started process (PID=2861) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:01,363] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:49:01,364] {logging_mixin.py:115} INFO - [2023-01-04 21:49:01,364] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:02,255] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:49:02,256] {logging_mixin.py:115} INFO - [2023-01-04 21:49:02,256] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:49:02,256] {logging_mixin.py:115} INFO - [2023-01-04 21:49:02,256] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:49:02,263] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:02,285] {logging_mixin.py:115} INFO - [2023-01-04 21:49:02,284] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:49:02,304] {logging_mixin.py:115} INFO - [2023-01-04 21:49:02,304] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:49:02,313] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-04 21:49:32,384] {processor.py:153} INFO - Started process (PID=2876) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:32,388] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:49:32,388] {logging_mixin.py:115} INFO - [2023-01-04 21:49:32,388] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:33,182] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:49:33,184] {logging_mixin.py:115} INFO - [2023-01-04 21:49:33,184] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:49:33,184] {logging_mixin.py:115} INFO - [2023-01-04 21:49:33,184] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:49:33,191] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:49:33,213] {logging_mixin.py:115} INFO - [2023-01-04 21:49:33,212] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:49:33,232] {logging_mixin.py:115} INFO - [2023-01-04 21:49:33,232] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:49:33,241] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 21:50:03,312] {processor.py:153} INFO - Started process (PID=2899) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:03,313] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:50:03,314] {logging_mixin.py:115} INFO - [2023-01-04 21:50:03,314] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:04,099] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:50:04,101] {logging_mixin.py:115} INFO - [2023-01-04 21:50:04,100] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:50:04,101] {logging_mixin.py:115} INFO - [2023-01-04 21:50:04,101] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:50:04,108] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:04,129] {logging_mixin.py:115} INFO - [2023-01-04 21:50:04,129] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:50:04,151] {logging_mixin.py:115} INFO - [2023-01-04 21:50:04,151] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:50:04,160] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 21:50:15,218] {processor.py:153} INFO - Started process (PID=2908) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:15,219] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:50:15,219] {logging_mixin.py:115} INFO - [2023-01-04 21:50:15,219] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:16,042] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:50:16,043] {logging_mixin.py:115} INFO - [2023-01-04 21:50:16,043] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:50:16,044] {logging_mixin.py:115} INFO - [2023-01-04 21:50:16,044] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:50:16,051] {logging_mixin.py:115} INFO - [2023-01-04 21:50:16,050] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 86, in <module>
    download_stock_data_from_datalake_task >> create_cluster_dataproc_task >> submit_spark_job_task >> delete_cluster_dataproc_task
NameError: name 'download_stock_data_from_datalake_task' is not defined
[2023-01-04 21:50:16,051] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:16,069] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 21:50:46,141] {processor.py:153} INFO - Started process (PID=2931) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:46,142] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:50:46,143] {logging_mixin.py:115} INFO - [2023-01-04 21:50:46,143] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:46,971] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:50:46,972] {logging_mixin.py:115} INFO - [2023-01-04 21:50:46,972] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:50:46,973] {logging_mixin.py:115} INFO - [2023-01-04 21:50:46,972] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:50:46,979] {logging_mixin.py:115} INFO - [2023-01-04 21:50:46,979] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 86, in <module>
    download_stock_data_from_datalake_task >> create_cluster_dataproc_task >> submit_spark_job_task >> delete_cluster_dataproc_task
NameError: name 'download_stock_data_from_datalake_task' is not defined
[2023-01-04 21:50:46,980] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:46,997] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.861 seconds
[2023-01-04 21:50:59,025] {processor.py:153} INFO - Started process (PID=2940) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:59,025] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:50:59,026] {logging_mixin.py:115} INFO - [2023-01-04 21:50:59,026] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:59,826] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:50:59,828] {logging_mixin.py:115} INFO - [2023-01-04 21:50:59,828] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:50:59,828] {logging_mixin.py:115} INFO - [2023-01-04 21:50:59,828] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:50:59,835] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:50:59,890] {logging_mixin.py:115} INFO - [2023-01-04 21:50:59,890] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:50:59,911] {logging_mixin.py:115} INFO - [2023-01-04 21:50:59,911] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:50:59,924] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.905 seconds
[2023-01-04 21:51:30,036] {processor.py:153} INFO - Started process (PID=2963) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:51:30,037] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:51:30,037] {logging_mixin.py:115} INFO - [2023-01-04 21:51:30,037] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:51:30,835] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:51:30,836] {logging_mixin.py:115} INFO - [2023-01-04 21:51:30,836] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:51:30,837] {logging_mixin.py:115} INFO - [2023-01-04 21:51:30,836] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:51:30,843] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:51:30,865] {logging_mixin.py:115} INFO - [2023-01-04 21:51:30,864] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:51:30,884] {logging_mixin.py:115} INFO - [2023-01-04 21:51:30,884] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:51:30,893] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 21:52:00,987] {processor.py:153} INFO - Started process (PID=2979) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:00,988] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:52:00,988] {logging_mixin.py:115} INFO - [2023-01-04 21:52:00,988] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:01,830] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:52:01,831] {logging_mixin.py:115} INFO - [2023-01-04 21:52:01,831] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:52:01,832] {logging_mixin.py:115} INFO - [2023-01-04 21:52:01,831] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:52:01,838] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:01,866] {logging_mixin.py:115} INFO - [2023-01-04 21:52:01,866] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:52:01,895] {logging_mixin.py:115} INFO - [2023-01-04 21:52:01,895] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:52:01,907] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.924 seconds
[2023-01-04 21:52:32,009] {processor.py:153} INFO - Started process (PID=3003) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:32,011] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:52:32,011] {logging_mixin.py:115} INFO - [2023-01-04 21:52:32,011] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:32,813] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:52:32,815] {logging_mixin.py:115} INFO - [2023-01-04 21:52:32,814] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:52:32,815] {logging_mixin.py:115} INFO - [2023-01-04 21:52:32,815] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:52:32,822] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:52:32,847] {logging_mixin.py:115} INFO - [2023-01-04 21:52:32,847] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:52:32,870] {logging_mixin.py:115} INFO - [2023-01-04 21:52:32,870] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:52:32,879] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.875 seconds
[2023-01-04 21:53:02,977] {processor.py:153} INFO - Started process (PID=3027) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:02,978] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:53:02,978] {logging_mixin.py:115} INFO - [2023-01-04 21:53:02,978] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:03,775] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:53:03,776] {logging_mixin.py:115} INFO - [2023-01-04 21:53:03,776] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:53:03,777] {logging_mixin.py:115} INFO - [2023-01-04 21:53:03,776] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:53:03,783] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:03,805] {logging_mixin.py:115} INFO - [2023-01-04 21:53:03,805] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:53:03,826] {logging_mixin.py:115} INFO - [2023-01-04 21:53:03,826] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:53:03,836] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 21:53:33,927] {processor.py:153} INFO - Started process (PID=3050) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:33,928] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:53:33,929] {logging_mixin.py:115} INFO - [2023-01-04 21:53:33,929] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:34,715] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:53:34,717] {logging_mixin.py:115} INFO - [2023-01-04 21:53:34,717] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:53:34,717] {logging_mixin.py:115} INFO - [2023-01-04 21:53:34,717] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:53:34,724] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:53:34,746] {logging_mixin.py:115} INFO - [2023-01-04 21:53:34,746] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:53:34,767] {logging_mixin.py:115} INFO - [2023-01-04 21:53:34,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:53:34,778] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 21:54:04,849] {processor.py:153} INFO - Started process (PID=3066) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:04,850] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:54:04,850] {logging_mixin.py:115} INFO - [2023-01-04 21:54:04,850] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:05,653] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:54:05,654] {logging_mixin.py:115} INFO - [2023-01-04 21:54:05,654] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:54:05,655] {logging_mixin.py:115} INFO - [2023-01-04 21:54:05,654] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:54:05,663] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:05,702] {logging_mixin.py:115} INFO - [2023-01-04 21:54:05,701] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:54:05,723] {logging_mixin.py:115} INFO - [2023-01-04 21:54:05,723] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:54:05,736] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.892 seconds
[2023-01-04 21:54:35,811] {processor.py:153} INFO - Started process (PID=3090) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:35,813] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:54:35,814] {logging_mixin.py:115} INFO - [2023-01-04 21:54:35,814] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:36,618] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:54:36,619] {logging_mixin.py:115} INFO - [2023-01-04 21:54:36,619] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:54:36,620] {logging_mixin.py:115} INFO - [2023-01-04 21:54:36,620] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:54:36,627] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:54:36,648] {logging_mixin.py:115} INFO - [2023-01-04 21:54:36,648] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:54:36,668] {logging_mixin.py:115} INFO - [2023-01-04 21:54:36,668] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:54:36,677] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.871 seconds
[2023-01-04 21:55:06,750] {processor.py:153} INFO - Started process (PID=3113) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:06,751] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:55:06,752] {logging_mixin.py:115} INFO - [2023-01-04 21:55:06,752] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:07,580] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:55:07,581] {logging_mixin.py:115} INFO - [2023-01-04 21:55:07,581] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:55:07,582] {logging_mixin.py:115} INFO - [2023-01-04 21:55:07,582] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:55:07,589] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:07,610] {logging_mixin.py:115} INFO - [2023-01-04 21:55:07,610] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:55:07,629] {logging_mixin.py:115} INFO - [2023-01-04 21:55:07,629] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:55:07,639] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.894 seconds
[2023-01-04 21:55:37,732] {processor.py:153} INFO - Started process (PID=3137) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:37,734] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:55:37,735] {logging_mixin.py:115} INFO - [2023-01-04 21:55:37,734] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:38,906] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:55:38,908] {logging_mixin.py:115} INFO - [2023-01-04 21:55:38,908] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:55:38,909] {logging_mixin.py:115} INFO - [2023-01-04 21:55:38,908] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:55:38,920] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:55:38,950] {logging_mixin.py:115} INFO - [2023-01-04 21:55:38,949] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:55:38,972] {logging_mixin.py:115} INFO - [2023-01-04 21:55:38,972] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:55:38,981] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.259 seconds
[2023-01-04 21:56:09,063] {processor.py:153} INFO - Started process (PID=3154) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:09,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:56:09,065] {logging_mixin.py:115} INFO - [2023-01-04 21:56:09,065] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:09,884] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:56:09,886] {logging_mixin.py:115} INFO - [2023-01-04 21:56:09,886] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:56:09,886] {logging_mixin.py:115} INFO - [2023-01-04 21:56:09,886] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:56:09,893] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:09,915] {logging_mixin.py:115} INFO - [2023-01-04 21:56:09,915] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:56:09,935] {logging_mixin.py:115} INFO - [2023-01-04 21:56:09,935] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:56:09,945] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.886 seconds
[2023-01-04 21:56:40,015] {processor.py:153} INFO - Started process (PID=3177) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:40,016] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:56:40,017] {logging_mixin.py:115} INFO - [2023-01-04 21:56:40,016] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:40,827] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:56:40,828] {logging_mixin.py:115} INFO - [2023-01-04 21:56:40,828] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:56:40,829] {logging_mixin.py:115} INFO - [2023-01-04 21:56:40,829] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:56:40,836] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:56:40,857] {logging_mixin.py:115} INFO - [2023-01-04 21:56:40,857] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:56:40,877] {logging_mixin.py:115} INFO - [2023-01-04 21:56:40,877] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:56:40,887] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.877 seconds
[2023-01-04 21:57:10,930] {processor.py:153} INFO - Started process (PID=3201) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:10,931] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:57:10,932] {logging_mixin.py:115} INFO - [2023-01-04 21:57:10,932] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:11,761] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:57:11,763] {logging_mixin.py:115} INFO - [2023-01-04 21:57:11,763] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:57:11,763] {logging_mixin.py:115} INFO - [2023-01-04 21:57:11,763] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:57:11,770] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:11,792] {logging_mixin.py:115} INFO - [2023-01-04 21:57:11,792] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:57:11,812] {logging_mixin.py:115} INFO - [2023-01-04 21:57:11,812] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:57:11,822] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.897 seconds
[2023-01-04 21:57:41,925] {processor.py:153} INFO - Started process (PID=3217) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:41,926] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:57:41,926] {logging_mixin.py:115} INFO - [2023-01-04 21:57:41,926] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:42,941] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:57:42,942] {logging_mixin.py:115} INFO - [2023-01-04 21:57:42,942] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:57:42,943] {logging_mixin.py:115} INFO - [2023-01-04 21:57:42,943] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:57:42,950] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:57:42,972] {logging_mixin.py:115} INFO - [2023-01-04 21:57:42,972] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:57:42,993] {logging_mixin.py:115} INFO - [2023-01-04 21:57:42,993] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:57:43,006] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.086 seconds
[2023-01-04 21:58:13,067] {processor.py:153} INFO - Started process (PID=3242) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:13,070] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:58:13,070] {logging_mixin.py:115} INFO - [2023-01-04 21:58:13,070] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:13,875] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:58:13,876] {logging_mixin.py:115} INFO - [2023-01-04 21:58:13,876] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:58:13,876] {logging_mixin.py:115} INFO - [2023-01-04 21:58:13,876] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:58:13,883] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:13,905] {logging_mixin.py:115} INFO - [2023-01-04 21:58:13,905] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:58:13,927] {logging_mixin.py:115} INFO - [2023-01-04 21:58:13,926] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:58:13,937] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 21:58:44,040] {processor.py:153} INFO - Started process (PID=3265) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:44,040] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:58:44,041] {logging_mixin.py:115} INFO - [2023-01-04 21:58:44,041] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:44,869] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:58:44,871] {logging_mixin.py:115} INFO - [2023-01-04 21:58:44,871] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:58:44,872] {logging_mixin.py:115} INFO - [2023-01-04 21:58:44,871] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:58:44,882] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:58:44,913] {logging_mixin.py:115} INFO - [2023-01-04 21:58:44,913] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:58:44,942] {logging_mixin.py:115} INFO - [2023-01-04 21:58:44,941] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:58:44,953] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.918 seconds
[2023-01-04 21:59:15,057] {processor.py:153} INFO - Started process (PID=3288) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:15,059] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:59:15,060] {logging_mixin.py:115} INFO - [2023-01-04 21:59:15,059] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:15,876] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:59:15,877] {logging_mixin.py:115} INFO - [2023-01-04 21:59:15,877] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:59:15,878] {logging_mixin.py:115} INFO - [2023-01-04 21:59:15,877] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:59:15,884] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:15,906] {logging_mixin.py:115} INFO - [2023-01-04 21:59:15,906] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:59:15,926] {logging_mixin.py:115} INFO - [2023-01-04 21:59:15,926] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:59:15,937] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.885 seconds
[2023-01-04 21:59:46,037] {processor.py:153} INFO - Started process (PID=3304) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:46,038] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:59:46,039] {logging_mixin.py:115} INFO - [2023-01-04 21:59:46,039] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:46,885] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:59:46,887] {logging_mixin.py:115} INFO - [2023-01-04 21:59:46,887] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:59:46,888] {logging_mixin.py:115} INFO - [2023-01-04 21:59:46,887] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:59:46,899] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:46,922] {logging_mixin.py:115} INFO - [2023-01-04 21:59:46,922] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:59:46,942] {logging_mixin.py:115} INFO - [2023-01-04 21:59:46,942] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:59:46,952] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-04 21:59:54,000] {processor.py:153} INFO - Started process (PID=3313) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:54,000] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 21:59:54,001] {logging_mixin.py:115} INFO - [2023-01-04 21:59:54,001] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:54,813] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 21:59:54,814] {logging_mixin.py:115} INFO - [2023-01-04 21:59:54,814] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 21:59:54,815] {logging_mixin.py:115} INFO - [2023-01-04 21:59:54,814] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 21:59:54,821] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 21:59:54,844] {logging_mixin.py:115} INFO - [2023-01-04 21:59:54,844] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 21:59:54,865] {logging_mixin.py:115} INFO - [2023-01-04 21:59:54,865] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 21:59:54,877] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.881 seconds
[2023-01-04 22:00:24,975] {processor.py:153} INFO - Started process (PID=3337) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:24,981] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:00:24,982] {logging_mixin.py:115} INFO - [2023-01-04 22:00:24,982] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:25,814] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:00:25,815] {logging_mixin.py:115} INFO - [2023-01-04 22:00:25,815] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:00:25,816] {logging_mixin.py:115} INFO - [2023-01-04 22:00:25,816] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:00:25,823] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:25,845] {logging_mixin.py:115} INFO - [2023-01-04 22:00:25,844] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:00:25,865] {logging_mixin.py:115} INFO - [2023-01-04 22:00:25,865] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:00:25,875] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.904 seconds
[2023-01-04 22:00:55,964] {processor.py:153} INFO - Started process (PID=3362) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:55,965] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:00:55,966] {logging_mixin.py:115} INFO - [2023-01-04 22:00:55,966] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:56,756] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:00:56,758] {logging_mixin.py:115} INFO - [2023-01-04 22:00:56,758] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:00:56,758] {logging_mixin.py:115} INFO - [2023-01-04 22:00:56,758] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:00:56,765] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:00:56,786] {logging_mixin.py:115} INFO - [2023-01-04 22:00:56,786] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:00:56,806] {logging_mixin.py:115} INFO - [2023-01-04 22:00:56,806] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:00:56,815] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 22:01:26,890] {processor.py:153} INFO - Started process (PID=3379) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:26,892] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:01:26,892] {logging_mixin.py:115} INFO - [2023-01-04 22:01:26,892] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:27,715] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:01:27,717] {logging_mixin.py:115} INFO - [2023-01-04 22:01:27,717] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:01:27,717] {logging_mixin.py:115} INFO - [2023-01-04 22:01:27,717] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:01:27,724] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:27,747] {logging_mixin.py:115} INFO - [2023-01-04 22:01:27,746] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:01:27,767] {logging_mixin.py:115} INFO - [2023-01-04 22:01:27,767] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:01:27,777] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.891 seconds
[2023-01-04 22:01:57,851] {processor.py:153} INFO - Started process (PID=3402) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:57,855] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:01:57,855] {logging_mixin.py:115} INFO - [2023-01-04 22:01:57,855] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:58,645] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:01:58,647] {logging_mixin.py:115} INFO - [2023-01-04 22:01:58,647] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:01:58,648] {logging_mixin.py:115} INFO - [2023-01-04 22:01:58,647] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:01:58,658] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:01:58,680] {logging_mixin.py:115} INFO - [2023-01-04 22:01:58,680] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:01:58,700] {logging_mixin.py:115} INFO - [2023-01-04 22:01:58,700] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:01:58,712] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.866 seconds
[2023-01-04 22:02:28,787] {processor.py:153} INFO - Started process (PID=3425) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:02:28,788] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:02:28,789] {logging_mixin.py:115} INFO - [2023-01-04 22:02:28,789] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:02:29,620] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:02:29,622] {logging_mixin.py:115} INFO - [2023-01-04 22:02:29,622] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:02:29,622] {logging_mixin.py:115} INFO - [2023-01-04 22:02:29,622] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:02:29,629] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:02:29,650] {logging_mixin.py:115} INFO - [2023-01-04 22:02:29,649] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:02:29,669] {logging_mixin.py:115} INFO - [2023-01-04 22:02:29,669] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:02:29,678] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.897 seconds
[2023-01-04 22:02:59,747] {processor.py:153} INFO - Started process (PID=3442) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:02:59,749] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:02:59,750] {logging_mixin.py:115} INFO - [2023-01-04 22:02:59,749] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:00,681] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:03:00,682] {logging_mixin.py:115} INFO - [2023-01-04 22:03:00,682] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:03:00,683] {logging_mixin.py:115} INFO - [2023-01-04 22:03:00,682] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:03:00,689] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:00,711] {logging_mixin.py:115} INFO - [2023-01-04 22:03:00,711] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:03:00,732] {logging_mixin.py:115} INFO - [2023-01-04 22:03:00,732] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:03:00,742] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.999 seconds
[2023-01-04 22:03:07,769] {processor.py:153} INFO - Started process (PID=3452) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:07,770] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:03:07,770] {logging_mixin.py:115} INFO - [2023-01-04 22:03:07,770] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:08,593] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:03:08,595] {logging_mixin.py:115} INFO - [2023-01-04 22:03:08,595] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:03:08,595] {logging_mixin.py:115} INFO - [2023-01-04 22:03:08,595] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:03:08,602] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:08,623] {logging_mixin.py:115} INFO - [2023-01-04 22:03:08,623] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:03:08,642] {logging_mixin.py:115} INFO - [2023-01-04 22:03:08,642] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:03:08,653] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.888 seconds
[2023-01-04 22:03:38,759] {processor.py:153} INFO - Started process (PID=3477) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:38,760] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:03:38,761] {logging_mixin.py:115} INFO - [2023-01-04 22:03:38,761] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:39,560] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:03:39,561] {logging_mixin.py:115} INFO - [2023-01-04 22:03:39,561] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:03:39,562] {logging_mixin.py:115} INFO - [2023-01-04 22:03:39,561] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:03:39,568] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:03:39,590] {logging_mixin.py:115} INFO - [2023-01-04 22:03:39,589] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:03:39,609] {logging_mixin.py:115} INFO - [2023-01-04 22:03:39,609] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:03:39,619] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 22:04:09,715] {processor.py:153} INFO - Started process (PID=3500) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:09,715] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:04:09,716] {logging_mixin.py:115} INFO - [2023-01-04 22:04:09,716] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:10,494] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:04:10,495] {logging_mixin.py:115} INFO - [2023-01-04 22:04:10,495] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:04:10,496] {logging_mixin.py:115} INFO - [2023-01-04 22:04:10,495] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:04:10,502] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:10,523] {logging_mixin.py:115} INFO - [2023-01-04 22:04:10,523] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:04:10,542] {logging_mixin.py:115} INFO - [2023-01-04 22:04:10,542] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:04:10,551] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.841 seconds
[2023-01-04 22:04:40,648] {processor.py:153} INFO - Started process (PID=3517) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:40,649] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:04:40,650] {logging_mixin.py:115} INFO - [2023-01-04 22:04:40,649] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:41,450] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:04:41,452] {logging_mixin.py:115} INFO - [2023-01-04 22:04:41,452] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:04:41,452] {logging_mixin.py:115} INFO - [2023-01-04 22:04:41,452] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:04:41,459] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:04:41,480] {logging_mixin.py:115} INFO - [2023-01-04 22:04:41,480] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:04:41,500] {logging_mixin.py:115} INFO - [2023-01-04 22:04:41,499] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:04:41,509] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.866 seconds
[2023-01-04 22:05:11,603] {processor.py:153} INFO - Started process (PID=3540) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:11,603] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:05:11,604] {logging_mixin.py:115} INFO - [2023-01-04 22:05:11,604] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:12,397] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:05:12,398] {logging_mixin.py:115} INFO - [2023-01-04 22:05:12,398] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:05:12,399] {logging_mixin.py:115} INFO - [2023-01-04 22:05:12,398] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:05:12,405] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:12,426] {logging_mixin.py:115} INFO - [2023-01-04 22:05:12,426] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:05:12,446] {logging_mixin.py:115} INFO - [2023-01-04 22:05:12,446] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:05:12,455] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 22:05:42,525] {processor.py:153} INFO - Started process (PID=3563) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:42,527] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:05:42,527] {logging_mixin.py:115} INFO - [2023-01-04 22:05:42,527] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:43,308] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:05:43,309] {logging_mixin.py:115} INFO - [2023-01-04 22:05:43,309] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:05:43,310] {logging_mixin.py:115} INFO - [2023-01-04 22:05:43,309] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:05:43,316] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:05:43,337] {logging_mixin.py:115} INFO - [2023-01-04 22:05:43,337] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:05:43,357] {logging_mixin.py:115} INFO - [2023-01-04 22:05:43,357] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:05:43,366] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.845 seconds
[2023-01-04 22:06:13,450] {processor.py:153} INFO - Started process (PID=3589) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:13,451] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:06:13,451] {logging_mixin.py:115} INFO - [2023-01-04 22:06:13,451] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:14,240] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:06:14,241] {logging_mixin.py:115} INFO - [2023-01-04 22:06:14,241] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:06:14,242] {logging_mixin.py:115} INFO - [2023-01-04 22:06:14,242] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:06:14,248] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:14,269] {logging_mixin.py:115} INFO - [2023-01-04 22:06:14,269] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:06:14,289] {logging_mixin.py:115} INFO - [2023-01-04 22:06:14,288] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:06:14,298] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 22:06:44,366] {processor.py:153} INFO - Started process (PID=3605) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:44,368] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:06:44,368] {logging_mixin.py:115} INFO - [2023-01-04 22:06:44,368] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:45,329] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:06:45,331] {logging_mixin.py:115} INFO - [2023-01-04 22:06:45,331] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:06:45,332] {logging_mixin.py:115} INFO - [2023-01-04 22:06:45,331] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:06:45,343] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:06:45,375] {logging_mixin.py:115} INFO - [2023-01-04 22:06:45,374] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:06:45,404] {logging_mixin.py:115} INFO - [2023-01-04 22:06:45,404] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:06:45,427] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.065 seconds
[2023-01-04 22:07:15,499] {processor.py:153} INFO - Started process (PID=3629) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:15,500] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:07:15,501] {logging_mixin.py:115} INFO - [2023-01-04 22:07:15,501] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:16,281] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:07:16,283] {logging_mixin.py:115} INFO - [2023-01-04 22:07:16,283] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:07:16,283] {logging_mixin.py:115} INFO - [2023-01-04 22:07:16,283] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:07:16,290] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:16,312] {logging_mixin.py:115} INFO - [2023-01-04 22:07:16,311] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:07:16,331] {logging_mixin.py:115} INFO - [2023-01-04 22:07:16,331] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:07:16,340] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.846 seconds
[2023-01-04 22:07:46,446] {processor.py:153} INFO - Started process (PID=3651) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:46,447] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:07:46,447] {logging_mixin.py:115} INFO - [2023-01-04 22:07:46,447] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:47,238] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:07:47,239] {logging_mixin.py:115} INFO - [2023-01-04 22:07:47,239] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:07:47,240] {logging_mixin.py:115} INFO - [2023-01-04 22:07:47,239] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:07:47,246] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:07:47,268] {logging_mixin.py:115} INFO - [2023-01-04 22:07:47,267] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:07:47,288] {logging_mixin.py:115} INFO - [2023-01-04 22:07:47,288] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:07:47,297] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:08:17,403] {processor.py:153} INFO - Started process (PID=3675) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:17,405] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:08:17,406] {logging_mixin.py:115} INFO - [2023-01-04 22:08:17,406] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:18,432] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:08:18,434] {logging_mixin.py:115} INFO - [2023-01-04 22:08:18,433] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:08:18,434] {logging_mixin.py:115} INFO - [2023-01-04 22:08:18,434] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:08:18,445] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:18,474] {logging_mixin.py:115} INFO - [2023-01-04 22:08:18,474] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:08:18,498] {logging_mixin.py:115} INFO - [2023-01-04 22:08:18,497] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:08:18,507] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.110 seconds
[2023-01-04 22:08:48,582] {processor.py:153} INFO - Started process (PID=3692) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:48,583] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:08:48,583] {logging_mixin.py:115} INFO - [2023-01-04 22:08:48,583] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:49,360] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:08:49,362] {logging_mixin.py:115} INFO - [2023-01-04 22:08:49,362] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:08:49,362] {logging_mixin.py:115} INFO - [2023-01-04 22:08:49,362] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:08:49,369] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:08:49,390] {logging_mixin.py:115} INFO - [2023-01-04 22:08:49,389] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:08:49,409] {logging_mixin.py:115} INFO - [2023-01-04 22:08:49,409] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:08:49,418] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.841 seconds
[2023-01-04 22:09:19,514] {processor.py:153} INFO - Started process (PID=3716) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:19,518] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:09:19,518] {logging_mixin.py:115} INFO - [2023-01-04 22:09:19,518] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:20,306] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:09:20,307] {logging_mixin.py:115} INFO - [2023-01-04 22:09:20,307] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:09:20,308] {logging_mixin.py:115} INFO - [2023-01-04 22:09:20,308] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:09:20,315] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:20,335] {logging_mixin.py:115} INFO - [2023-01-04 22:09:20,335] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:09:20,358] {logging_mixin.py:115} INFO - [2023-01-04 22:09:20,358] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:09:20,368] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 22:09:50,461] {processor.py:153} INFO - Started process (PID=3739) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:50,461] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:09:50,462] {logging_mixin.py:115} INFO - [2023-01-04 22:09:50,462] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:51,258] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:09:51,259] {logging_mixin.py:115} INFO - [2023-01-04 22:09:51,259] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:09:51,260] {logging_mixin.py:115} INFO - [2023-01-04 22:09:51,260] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:09:51,267] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:09:51,288] {logging_mixin.py:115} INFO - [2023-01-04 22:09:51,288] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:09:51,309] {logging_mixin.py:115} INFO - [2023-01-04 22:09:51,309] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:09:51,320] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 22:10:21,419] {processor.py:153} INFO - Started process (PID=3764) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:21,420] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:10:21,421] {logging_mixin.py:115} INFO - [2023-01-04 22:10:21,420] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:22,555] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:10:22,557] {logging_mixin.py:115} INFO - [2023-01-04 22:10:22,557] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:10:22,558] {logging_mixin.py:115} INFO - [2023-01-04 22:10:22,558] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:10:22,569] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:22,610] {logging_mixin.py:115} INFO - [2023-01-04 22:10:22,609] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:10:22,650] {logging_mixin.py:115} INFO - [2023-01-04 22:10:22,649] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:10:22,666] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.253 seconds
[2023-01-04 22:10:52,750] {processor.py:153} INFO - Started process (PID=3780) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:52,751] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:10:52,752] {logging_mixin.py:115} INFO - [2023-01-04 22:10:52,752] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:53,527] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:10:53,529] {logging_mixin.py:115} INFO - [2023-01-04 22:10:53,529] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:10:53,529] {logging_mixin.py:115} INFO - [2023-01-04 22:10:53,529] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:10:53,537] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:10:53,560] {logging_mixin.py:115} INFO - [2023-01-04 22:10:53,560] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:10:53,589] {logging_mixin.py:115} INFO - [2023-01-04 22:10:53,589] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:10:53,602] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 22:11:23,683] {processor.py:153} INFO - Started process (PID=3803) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:23,684] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:11:23,684] {logging_mixin.py:115} INFO - [2023-01-04 22:11:23,684] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:24,549] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:11:24,551] {logging_mixin.py:115} INFO - [2023-01-04 22:11:24,551] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:11:24,551] {logging_mixin.py:115} INFO - [2023-01-04 22:11:24,551] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:11:24,558] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:24,580] {logging_mixin.py:115} INFO - [2023-01-04 22:11:24,580] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:11:24,601] {logging_mixin.py:115} INFO - [2023-01-04 22:11:24,601] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:11:24,610] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.933 seconds
[2023-01-04 22:11:54,652] {processor.py:153} INFO - Started process (PID=3826) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:54,653] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:11:54,654] {logging_mixin.py:115} INFO - [2023-01-04 22:11:54,654] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:55,453] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:11:55,454] {logging_mixin.py:115} INFO - [2023-01-04 22:11:55,454] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:11:55,455] {logging_mixin.py:115} INFO - [2023-01-04 22:11:55,454] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:11:55,461] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:11:55,483] {logging_mixin.py:115} INFO - [2023-01-04 22:11:55,482] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:11:55,502] {logging_mixin.py:115} INFO - [2023-01-04 22:11:55,502] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:11:55,511] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 22:12:25,612] {processor.py:153} INFO - Started process (PID=3849) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:25,613] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:12:25,613] {logging_mixin.py:115} INFO - [2023-01-04 22:12:25,613] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:26,445] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:12:26,446] {logging_mixin.py:115} INFO - [2023-01-04 22:12:26,446] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:12:26,447] {logging_mixin.py:115} INFO - [2023-01-04 22:12:26,446] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:12:26,453] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:26,475] {logging_mixin.py:115} INFO - [2023-01-04 22:12:26,475] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:12:26,495] {logging_mixin.py:115} INFO - [2023-01-04 22:12:26,495] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:12:26,504] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.898 seconds
[2023-01-04 22:12:56,603] {processor.py:153} INFO - Started process (PID=3865) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:56,604] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:12:56,605] {logging_mixin.py:115} INFO - [2023-01-04 22:12:56,605] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:57,413] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:12:57,414] {logging_mixin.py:115} INFO - [2023-01-04 22:12:57,414] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:12:57,415] {logging_mixin.py:115} INFO - [2023-01-04 22:12:57,414] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:12:57,421] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:12:57,443] {logging_mixin.py:115} INFO - [2023-01-04 22:12:57,443] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:12:57,463] {logging_mixin.py:115} INFO - [2023-01-04 22:12:57,463] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:12:57,473] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.876 seconds
[2023-01-04 22:13:27,569] {processor.py:153} INFO - Started process (PID=3889) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:27,569] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:13:27,570] {logging_mixin.py:115} INFO - [2023-01-04 22:13:27,570] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:28,361] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:13:28,362] {logging_mixin.py:115} INFO - [2023-01-04 22:13:28,362] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:13:28,363] {logging_mixin.py:115} INFO - [2023-01-04 22:13:28,362] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:13:28,369] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:28,391] {logging_mixin.py:115} INFO - [2023-01-04 22:13:28,390] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:13:28,410] {logging_mixin.py:115} INFO - [2023-01-04 22:13:28,410] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:13:28,419] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:13:58,513] {processor.py:153} INFO - Started process (PID=3912) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:58,515] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:13:58,515] {logging_mixin.py:115} INFO - [2023-01-04 22:13:58,515] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:59,300] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:13:59,301] {logging_mixin.py:115} INFO - [2023-01-04 22:13:59,301] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:13:59,301] {logging_mixin.py:115} INFO - [2023-01-04 22:13:59,301] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:13:59,308] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:13:59,330] {logging_mixin.py:115} INFO - [2023-01-04 22:13:59,329] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:13:59,349] {logging_mixin.py:115} INFO - [2023-01-04 22:13:59,349] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:13:59,359] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.852 seconds
[2023-01-04 22:14:29,432] {processor.py:153} INFO - Started process (PID=3928) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:14:29,432] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:14:29,433] {logging_mixin.py:115} INFO - [2023-01-04 22:14:29,433] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:14:30,217] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:14:30,219] {logging_mixin.py:115} INFO - [2023-01-04 22:14:30,219] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:14:30,219] {logging_mixin.py:115} INFO - [2023-01-04 22:14:30,219] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:14:30,226] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:14:30,249] {logging_mixin.py:115} INFO - [2023-01-04 22:14:30,249] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:14:30,273] {logging_mixin.py:115} INFO - [2023-01-04 22:14:30,273] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:14:30,284] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 22:15:00,357] {processor.py:153} INFO - Started process (PID=3951) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:00,361] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:15:00,361] {logging_mixin.py:115} INFO - [2023-01-04 22:15:00,361] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:01,146] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:15:01,147] {logging_mixin.py:115} INFO - [2023-01-04 22:15:01,147] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:15:01,147] {logging_mixin.py:115} INFO - [2023-01-04 22:15:01,147] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:15:01,154] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:01,176] {logging_mixin.py:115} INFO - [2023-01-04 22:15:01,175] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:15:01,197] {logging_mixin.py:115} INFO - [2023-01-04 22:15:01,197] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:15:01,206] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 22:15:31,281] {processor.py:153} INFO - Started process (PID=3974) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:31,282] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:15:31,283] {logging_mixin.py:115} INFO - [2023-01-04 22:15:31,283] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:32,072] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:15:32,073] {logging_mixin.py:115} INFO - [2023-01-04 22:15:32,073] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:15:32,074] {logging_mixin.py:115} INFO - [2023-01-04 22:15:32,074] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:15:32,080] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:15:32,102] {logging_mixin.py:115} INFO - [2023-01-04 22:15:32,101] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:15:32,121] {logging_mixin.py:115} INFO - [2023-01-04 22:15:32,121] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:15:32,131] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.857 seconds
[2023-01-04 22:16:02,202] {processor.py:153} INFO - Started process (PID=3996) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:02,204] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:16:02,205] {logging_mixin.py:115} INFO - [2023-01-04 22:16:02,205] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:03,006] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:16:03,008] {logging_mixin.py:115} INFO - [2023-01-04 22:16:03,008] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:16:03,008] {logging_mixin.py:115} INFO - [2023-01-04 22:16:03,008] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:16:03,015] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:03,037] {logging_mixin.py:115} INFO - [2023-01-04 22:16:03,037] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:16:03,057] {logging_mixin.py:115} INFO - [2023-01-04 22:16:03,057] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:16:03,069] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.872 seconds
[2023-01-04 22:16:33,169] {processor.py:153} INFO - Started process (PID=4010) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:33,169] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:16:33,170] {logging_mixin.py:115} INFO - [2023-01-04 22:16:33,170] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:34,011] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:16:34,013] {logging_mixin.py:115} INFO - [2023-01-04 22:16:34,013] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:16:34,013] {logging_mixin.py:115} INFO - [2023-01-04 22:16:34,013] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:16:34,020] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:16:34,042] {logging_mixin.py:115} INFO - [2023-01-04 22:16:34,041] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:16:34,063] {logging_mixin.py:115} INFO - [2023-01-04 22:16:34,062] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:16:34,073] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.909 seconds
[2023-01-04 22:17:04,173] {processor.py:153} INFO - Started process (PID=4033) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:04,175] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:17:04,176] {logging_mixin.py:115} INFO - [2023-01-04 22:17:04,175] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:04,972] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:17:04,974] {logging_mixin.py:115} INFO - [2023-01-04 22:17:04,974] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:17:04,974] {logging_mixin.py:115} INFO - [2023-01-04 22:17:04,974] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:17:04,983] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:05,010] {logging_mixin.py:115} INFO - [2023-01-04 22:17:05,009] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:17:05,029] {logging_mixin.py:115} INFO - [2023-01-04 22:17:05,029] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:17:05,038] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.870 seconds
[2023-01-04 22:17:35,133] {processor.py:153} INFO - Started process (PID=4056) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:35,134] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:17:35,135] {logging_mixin.py:115} INFO - [2023-01-04 22:17:35,135] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:35,949] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:17:35,951] {logging_mixin.py:115} INFO - [2023-01-04 22:17:35,951] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:17:35,951] {logging_mixin.py:115} INFO - [2023-01-04 22:17:35,951] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:17:35,958] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:17:35,979] {logging_mixin.py:115} INFO - [2023-01-04 22:17:35,979] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:17:35,999] {logging_mixin.py:115} INFO - [2023-01-04 22:17:35,999] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:17:36,009] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.880 seconds
[2023-01-04 22:18:06,108] {processor.py:153} INFO - Started process (PID=4077) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:06,109] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:18:06,110] {logging_mixin.py:115} INFO - [2023-01-04 22:18:06,110] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:06,915] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:18:06,916] {logging_mixin.py:115} INFO - [2023-01-04 22:18:06,916] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:18:06,916] {logging_mixin.py:115} INFO - [2023-01-04 22:18:06,916] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:18:06,923] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:06,945] {logging_mixin.py:115} INFO - [2023-01-04 22:18:06,945] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:18:06,965] {logging_mixin.py:115} INFO - [2023-01-04 22:18:06,965] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:18:06,975] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.872 seconds
[2023-01-04 22:18:37,069] {processor.py:153} INFO - Started process (PID=4093) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:37,071] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:18:37,071] {logging_mixin.py:115} INFO - [2023-01-04 22:18:37,071] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:38,130] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:18:38,131] {logging_mixin.py:115} INFO - [2023-01-04 22:18:38,131] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:18:38,132] {logging_mixin.py:115} INFO - [2023-01-04 22:18:38,131] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:18:38,138] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:18:38,160] {logging_mixin.py:115} INFO - [2023-01-04 22:18:38,159] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:18:38,179] {logging_mixin.py:115} INFO - [2023-01-04 22:18:38,179] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:18:38,188] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.125 seconds
[2023-01-04 22:19:08,259] {processor.py:153} INFO - Started process (PID=4116) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:08,260] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:19:08,261] {logging_mixin.py:115} INFO - [2023-01-04 22:19:08,261] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:09,068] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:19:09,070] {logging_mixin.py:115} INFO - [2023-01-04 22:19:09,070] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:19:09,070] {logging_mixin.py:115} INFO - [2023-01-04 22:19:09,070] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:19:09,077] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:09,099] {logging_mixin.py:115} INFO - [2023-01-04 22:19:09,099] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:19:09,119] {logging_mixin.py:115} INFO - [2023-01-04 22:19:09,119] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:19:09,128] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 22:19:39,211] {processor.py:153} INFO - Started process (PID=4140) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:39,212] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:19:39,212] {logging_mixin.py:115} INFO - [2023-01-04 22:19:39,212] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:40,004] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:19:40,005] {logging_mixin.py:115} INFO - [2023-01-04 22:19:40,005] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:19:40,006] {logging_mixin.py:115} INFO - [2023-01-04 22:19:40,006] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:19:40,012] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:19:40,034] {logging_mixin.py:115} INFO - [2023-01-04 22:19:40,033] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:19:40,053] {logging_mixin.py:115} INFO - [2023-01-04 22:19:40,053] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:19:40,062] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:20:10,164] {processor.py:153} INFO - Started process (PID=4163) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:10,166] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:20:10,166] {logging_mixin.py:115} INFO - [2023-01-04 22:20:10,166] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:10,963] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:20:10,964] {logging_mixin.py:115} INFO - [2023-01-04 22:20:10,964] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:20:10,965] {logging_mixin.py:115} INFO - [2023-01-04 22:20:10,965] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:20:10,973] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:11,003] {logging_mixin.py:115} INFO - [2023-01-04 22:20:11,003] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:20:11,031] {logging_mixin.py:115} INFO - [2023-01-04 22:20:11,031] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:20:11,041] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.882 seconds
[2023-01-04 22:20:41,140] {processor.py:153} INFO - Started process (PID=4179) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:41,140] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:20:41,141] {logging_mixin.py:115} INFO - [2023-01-04 22:20:41,141] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:41,967] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:20:41,969] {logging_mixin.py:115} INFO - [2023-01-04 22:20:41,968] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:20:41,969] {logging_mixin.py:115} INFO - [2023-01-04 22:20:41,969] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:20:41,976] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:20:42,014] {logging_mixin.py:115} INFO - [2023-01-04 22:20:42,014] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:20:42,036] {logging_mixin.py:115} INFO - [2023-01-04 22:20:42,036] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:20:42,046] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-04 22:21:12,161] {processor.py:153} INFO - Started process (PID=4201) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:12,163] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:21:12,164] {logging_mixin.py:115} INFO - [2023-01-04 22:21:12,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:12,995] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:21:12,996] {logging_mixin.py:115} INFO - [2023-01-04 22:21:12,996] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:21:12,997] {logging_mixin.py:115} INFO - [2023-01-04 22:21:12,996] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:21:13,003] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:13,026] {logging_mixin.py:115} INFO - [2023-01-04 22:21:13,025] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:21:13,046] {logging_mixin.py:115} INFO - [2023-01-04 22:21:13,046] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:21:13,055] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.899 seconds
[2023-01-04 22:21:43,151] {processor.py:153} INFO - Started process (PID=4224) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:43,152] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:21:43,153] {logging_mixin.py:115} INFO - [2023-01-04 22:21:43,152] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:44,050] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:21:44,052] {logging_mixin.py:115} INFO - [2023-01-04 22:21:44,052] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:21:44,052] {logging_mixin.py:115} INFO - [2023-01-04 22:21:44,052] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:21:44,059] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:21:44,084] {logging_mixin.py:115} INFO - [2023-01-04 22:21:44,083] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:21:44,104] {logging_mixin.py:115} INFO - [2023-01-04 22:21:44,104] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:21:44,114] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-04 22:22:14,208] {processor.py:153} INFO - Started process (PID=4248) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:14,210] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:22:14,211] {logging_mixin.py:115} INFO - [2023-01-04 22:22:14,210] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:15,003] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:22:15,005] {logging_mixin.py:115} INFO - [2023-01-04 22:22:15,004] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:22:15,005] {logging_mixin.py:115} INFO - [2023-01-04 22:22:15,005] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:22:15,012] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:15,033] {logging_mixin.py:115} INFO - [2023-01-04 22:22:15,033] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:22:15,053] {logging_mixin.py:115} INFO - [2023-01-04 22:22:15,053] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:22:15,063] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 22:22:45,156] {processor.py:153} INFO - Started process (PID=4264) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:45,158] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:22:45,158] {logging_mixin.py:115} INFO - [2023-01-04 22:22:45,158] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:45,949] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:22:45,951] {logging_mixin.py:115} INFO - [2023-01-04 22:22:45,951] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:22:45,951] {logging_mixin.py:115} INFO - [2023-01-04 22:22:45,951] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:22:45,958] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:22:45,980] {logging_mixin.py:115} INFO - [2023-01-04 22:22:45,979] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:22:46,003] {logging_mixin.py:115} INFO - [2023-01-04 22:22:46,002] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:22:46,016] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 22:23:16,098] {processor.py:153} INFO - Started process (PID=4287) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:16,099] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:23:16,099] {logging_mixin.py:115} INFO - [2023-01-04 22:23:16,099] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:16,901] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:23:16,902] {logging_mixin.py:115} INFO - [2023-01-04 22:23:16,902] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:23:16,903] {logging_mixin.py:115} INFO - [2023-01-04 22:23:16,902] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:23:16,909] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:16,934] {logging_mixin.py:115} INFO - [2023-01-04 22:23:16,934] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:23:16,961] {logging_mixin.py:115} INFO - [2023-01-04 22:23:16,961] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:23:16,971] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.877 seconds
[2023-01-04 22:23:47,043] {processor.py:153} INFO - Started process (PID=4311) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:47,047] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:23:47,047] {logging_mixin.py:115} INFO - [2023-01-04 22:23:47,047] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:47,839] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:23:47,840] {logging_mixin.py:115} INFO - [2023-01-04 22:23:47,840] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:23:47,841] {logging_mixin.py:115} INFO - [2023-01-04 22:23:47,840] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:23:47,847] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:23:47,873] {logging_mixin.py:115} INFO - [2023-01-04 22:23:47,873] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:23:47,893] {logging_mixin.py:115} INFO - [2023-01-04 22:23:47,893] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:23:47,902] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 22:24:17,972] {processor.py:153} INFO - Started process (PID=4335) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:17,973] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:24:17,974] {logging_mixin.py:115} INFO - [2023-01-04 22:24:17,974] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:18,757] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:24:18,758] {logging_mixin.py:115} INFO - [2023-01-04 22:24:18,758] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:24:18,759] {logging_mixin.py:115} INFO - [2023-01-04 22:24:18,758] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:24:18,765] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:18,786] {logging_mixin.py:115} INFO - [2023-01-04 22:24:18,786] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:24:18,805] {logging_mixin.py:115} INFO - [2023-01-04 22:24:18,805] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:24:18,814] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 22:24:48,886] {processor.py:153} INFO - Started process (PID=4351) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:48,887] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:24:48,888] {logging_mixin.py:115} INFO - [2023-01-04 22:24:48,887] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:49,674] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:24:49,675] {logging_mixin.py:115} INFO - [2023-01-04 22:24:49,675] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:24:49,676] {logging_mixin.py:115} INFO - [2023-01-04 22:24:49,676] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:24:49,683] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:24:49,707] {logging_mixin.py:115} INFO - [2023-01-04 22:24:49,707] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:24:49,726] {logging_mixin.py:115} INFO - [2023-01-04 22:24:49,726] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:24:49,736] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 22:25:19,783] {processor.py:153} INFO - Started process (PID=4373) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:19,787] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:25:19,788] {logging_mixin.py:115} INFO - [2023-01-04 22:25:19,788] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:20,589] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:25:20,590] {logging_mixin.py:115} INFO - [2023-01-04 22:25:20,590] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:25:20,591] {logging_mixin.py:115} INFO - [2023-01-04 22:25:20,591] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:25:20,598] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:20,619] {logging_mixin.py:115} INFO - [2023-01-04 22:25:20,618] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:25:20,639] {logging_mixin.py:115} INFO - [2023-01-04 22:25:20,639] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:25:20,649] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.870 seconds
[2023-01-04 22:25:50,746] {processor.py:153} INFO - Started process (PID=4396) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:50,747] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:25:50,748] {logging_mixin.py:115} INFO - [2023-01-04 22:25:50,748] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:51,531] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:25:51,532] {logging_mixin.py:115} INFO - [2023-01-04 22:25:51,532] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:25:51,533] {logging_mixin.py:115} INFO - [2023-01-04 22:25:51,532] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:25:51,539] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:25:51,560] {logging_mixin.py:115} INFO - [2023-01-04 22:25:51,560] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:25:51,579] {logging_mixin.py:115} INFO - [2023-01-04 22:25:51,579] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:25:51,589] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 22:26:21,693] {processor.py:153} INFO - Started process (PID=4419) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:21,694] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:26:21,695] {logging_mixin.py:115} INFO - [2023-01-04 22:26:21,695] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:22,504] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:26:22,505] {logging_mixin.py:115} INFO - [2023-01-04 22:26:22,505] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:26:22,506] {logging_mixin.py:115} INFO - [2023-01-04 22:26:22,505] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:26:22,512] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:22,533] {logging_mixin.py:115} INFO - [2023-01-04 22:26:22,533] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:26:22,553] {logging_mixin.py:115} INFO - [2023-01-04 22:26:22,553] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:26:22,563] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.874 seconds
[2023-01-04 22:26:52,656] {processor.py:153} INFO - Started process (PID=4435) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:52,661] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:26:52,661] {logging_mixin.py:115} INFO - [2023-01-04 22:26:52,661] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:53,470] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:26:53,471] {logging_mixin.py:115} INFO - [2023-01-04 22:26:53,471] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:26:53,472] {logging_mixin.py:115} INFO - [2023-01-04 22:26:53,471] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:26:53,478] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:26:53,500] {logging_mixin.py:115} INFO - [2023-01-04 22:26:53,500] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:26:53,520] {logging_mixin.py:115} INFO - [2023-01-04 22:26:53,520] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:26:53,529] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.878 seconds
[2023-01-04 22:27:23,615] {processor.py:153} INFO - Started process (PID=4458) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:23,616] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:27:23,616] {logging_mixin.py:115} INFO - [2023-01-04 22:27:23,616] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:24,400] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:27:24,402] {logging_mixin.py:115} INFO - [2023-01-04 22:27:24,402] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:27:24,402] {logging_mixin.py:115} INFO - [2023-01-04 22:27:24,402] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:27:24,409] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:24,431] {logging_mixin.py:115} INFO - [2023-01-04 22:27:24,430] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:27:24,450] {logging_mixin.py:115} INFO - [2023-01-04 22:27:24,450] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:27:24,459] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 22:27:54,529] {processor.py:153} INFO - Started process (PID=4482) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:54,533] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:27:54,534] {logging_mixin.py:115} INFO - [2023-01-04 22:27:54,534] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:55,326] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:27:55,327] {logging_mixin.py:115} INFO - [2023-01-04 22:27:55,327] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:27:55,328] {logging_mixin.py:115} INFO - [2023-01-04 22:27:55,327] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:27:55,334] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:27:55,355] {logging_mixin.py:115} INFO - [2023-01-04 22:27:55,355] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:27:55,375] {logging_mixin.py:115} INFO - [2023-01-04 22:27:55,375] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:27:55,384] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 22:28:25,451] {processor.py:153} INFO - Started process (PID=4505) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:25,453] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:28:25,453] {logging_mixin.py:115} INFO - [2023-01-04 22:28:25,453] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:26,236] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:28:26,238] {logging_mixin.py:115} INFO - [2023-01-04 22:28:26,238] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:28:26,238] {logging_mixin.py:115} INFO - [2023-01-04 22:28:26,238] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:28:26,245] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:26,266] {logging_mixin.py:115} INFO - [2023-01-04 22:28:26,266] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:28:26,286] {logging_mixin.py:115} INFO - [2023-01-04 22:28:26,286] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:28:26,295] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 22:28:56,365] {processor.py:153} INFO - Started process (PID=4521) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:56,366] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:28:56,367] {logging_mixin.py:115} INFO - [2023-01-04 22:28:56,367] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:57,165] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:28:57,166] {logging_mixin.py:115} INFO - [2023-01-04 22:28:57,166] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:28:57,167] {logging_mixin.py:115} INFO - [2023-01-04 22:28:57,166] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:28:57,173] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:28:57,195] {logging_mixin.py:115} INFO - [2023-01-04 22:28:57,195] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:28:57,215] {logging_mixin.py:115} INFO - [2023-01-04 22:28:57,215] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:28:57,224] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 22:29:27,293] {processor.py:153} INFO - Started process (PID=4543) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:27,295] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:29:27,295] {logging_mixin.py:115} INFO - [2023-01-04 22:29:27,295] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:28,091] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:29:28,093] {logging_mixin.py:115} INFO - [2023-01-04 22:29:28,093] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:29:28,093] {logging_mixin.py:115} INFO - [2023-01-04 22:29:28,093] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:29:28,100] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:28,121] {logging_mixin.py:115} INFO - [2023-01-04 22:29:28,121] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:29:28,141] {logging_mixin.py:115} INFO - [2023-01-04 22:29:28,141] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:29:28,150] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 22:29:58,253] {processor.py:153} INFO - Started process (PID=4566) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:58,254] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:29:58,255] {logging_mixin.py:115} INFO - [2023-01-04 22:29:58,255] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:59,056] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:29:59,057] {logging_mixin.py:115} INFO - [2023-01-04 22:29:59,057] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:29:59,058] {logging_mixin.py:115} INFO - [2023-01-04 22:29:59,058] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:29:59,064] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:29:59,085] {logging_mixin.py:115} INFO - [2023-01-04 22:29:59,085] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:29:59,105] {logging_mixin.py:115} INFO - [2023-01-04 22:29:59,105] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:29:59,115] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.867 seconds
[2023-01-04 22:30:29,211] {processor.py:153} INFO - Started process (PID=4589) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:30:29,212] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:30:29,213] {logging_mixin.py:115} INFO - [2023-01-04 22:30:29,213] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:30:30,019] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:30:30,020] {logging_mixin.py:115} INFO - [2023-01-04 22:30:30,020] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:30:30,021] {logging_mixin.py:115} INFO - [2023-01-04 22:30:30,020] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:30:30,027] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:30:30,050] {logging_mixin.py:115} INFO - [2023-01-04 22:30:30,049] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:30:30,069] {logging_mixin.py:115} INFO - [2023-01-04 22:30:30,069] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:30:30,079] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.873 seconds
[2023-01-04 22:31:00,172] {processor.py:153} INFO - Started process (PID=4604) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:00,172] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:31:00,173] {logging_mixin.py:115} INFO - [2023-01-04 22:31:00,173] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:00,991] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:31:00,993] {logging_mixin.py:115} INFO - [2023-01-04 22:31:00,993] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:31:00,993] {logging_mixin.py:115} INFO - [2023-01-04 22:31:00,993] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:31:01,002] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:01,033] {logging_mixin.py:115} INFO - [2023-01-04 22:31:01,032] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:31:01,057] {logging_mixin.py:115} INFO - [2023-01-04 22:31:01,057] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:31:01,069] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.902 seconds
[2023-01-04 22:31:31,175] {processor.py:153} INFO - Started process (PID=4627) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:31,176] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:31:31,177] {logging_mixin.py:115} INFO - [2023-01-04 22:31:31,177] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:32,018] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:31:32,019] {logging_mixin.py:115} INFO - [2023-01-04 22:31:32,019] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:31:32,020] {logging_mixin.py:115} INFO - [2023-01-04 22:31:32,020] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:31:32,027] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:31:32,048] {logging_mixin.py:115} INFO - [2023-01-04 22:31:32,047] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:31:32,067] {logging_mixin.py:115} INFO - [2023-01-04 22:31:32,067] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:31:32,077] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.906 seconds
[2023-01-04 22:32:02,166] {processor.py:153} INFO - Started process (PID=4650) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:02,170] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:32:02,170] {logging_mixin.py:115} INFO - [2023-01-04 22:32:02,170] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:03,006] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:32:03,007] {logging_mixin.py:115} INFO - [2023-01-04 22:32:03,007] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:32:03,008] {logging_mixin.py:115} INFO - [2023-01-04 22:32:03,008] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:32:03,015] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:03,038] {logging_mixin.py:115} INFO - [2023-01-04 22:32:03,037] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:32:03,058] {logging_mixin.py:115} INFO - [2023-01-04 22:32:03,058] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:32:03,072] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.911 seconds
[2023-01-04 22:32:20,127] {processor.py:153} INFO - Started process (PID=4666) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:20,128] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:32:20,128] {logging_mixin.py:115} INFO - [2023-01-04 22:32:20,128] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:20,966] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:32:20,967] {logging_mixin.py:115} INFO - [2023-01-04 22:32:20,967] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:32:20,968] {logging_mixin.py:115} INFO - [2023-01-04 22:32:20,968] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:32:20,975] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:21,041] {logging_mixin.py:115} INFO - [2023-01-04 22:32:21,041] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:32:21,062] {logging_mixin.py:115} INFO - [2023-01-04 22:32:21,062] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:32:21,076] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.954 seconds
[2023-01-04 22:32:51,149] {processor.py:153} INFO - Started process (PID=4682) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:51,151] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:32:51,151] {logging_mixin.py:115} INFO - [2023-01-04 22:32:51,151] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:51,940] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:32:51,942] {logging_mixin.py:115} INFO - [2023-01-04 22:32:51,942] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:32:51,942] {logging_mixin.py:115} INFO - [2023-01-04 22:32:51,942] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:32:51,949] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:32:51,970] {logging_mixin.py:115} INFO - [2023-01-04 22:32:51,970] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:32:51,990] {logging_mixin.py:115} INFO - [2023-01-04 22:32:51,990] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:32:52,001] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:33:22,071] {processor.py:153} INFO - Started process (PID=4705) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:22,072] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:33:22,073] {logging_mixin.py:115} INFO - [2023-01-04 22:33:22,073] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:22,925] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:33:22,926] {logging_mixin.py:115} INFO - [2023-01-04 22:33:22,926] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:33:22,926] {logging_mixin.py:115} INFO - [2023-01-04 22:33:22,926] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:33:22,933] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:22,955] {logging_mixin.py:115} INFO - [2023-01-04 22:33:22,955] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:33:22,976] {logging_mixin.py:115} INFO - [2023-01-04 22:33:22,976] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:33:22,986] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-04 22:33:53,059] {processor.py:153} INFO - Started process (PID=4728) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:53,060] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:33:53,061] {logging_mixin.py:115} INFO - [2023-01-04 22:33:53,061] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:53,875] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:33:53,877] {logging_mixin.py:115} INFO - [2023-01-04 22:33:53,876] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:33:53,877] {logging_mixin.py:115} INFO - [2023-01-04 22:33:53,877] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:33:53,884] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:33:53,908] {logging_mixin.py:115} INFO - [2023-01-04 22:33:53,907] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:33:53,929] {logging_mixin.py:115} INFO - [2023-01-04 22:33:53,929] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:33:53,938] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.884 seconds
[2023-01-04 22:34:24,012] {processor.py:153} INFO - Started process (PID=4744) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:24,014] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:34:24,014] {logging_mixin.py:115} INFO - [2023-01-04 22:34:24,014] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:24,921] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:34:24,923] {logging_mixin.py:115} INFO - [2023-01-04 22:34:24,923] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:34:24,924] {logging_mixin.py:115} INFO - [2023-01-04 22:34:24,923] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:34:24,935] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:24,974] {logging_mixin.py:115} INFO - [2023-01-04 22:34:24,974] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:34:25,000] {logging_mixin.py:115} INFO - [2023-01-04 22:34:24,999] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:34:25,011] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.003 seconds
[2023-01-04 22:34:55,086] {processor.py:153} INFO - Started process (PID=4767) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:55,088] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:34:55,088] {logging_mixin.py:115} INFO - [2023-01-04 22:34:55,088] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:56,012] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:34:56,014] {logging_mixin.py:115} INFO - [2023-01-04 22:34:56,014] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:34:56,015] {logging_mixin.py:115} INFO - [2023-01-04 22:34:56,014] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:34:56,021] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:34:56,050] {logging_mixin.py:115} INFO - [2023-01-04 22:34:56,050] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:34:56,085] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.003 seconds
[2023-01-04 22:35:26,163] {processor.py:153} INFO - Started process (PID=4791) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:26,164] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:35:26,164] {logging_mixin.py:115} INFO - [2023-01-04 22:35:26,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:26,980] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:35:26,982] {logging_mixin.py:115} INFO - [2023-01-04 22:35:26,982] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:35:26,982] {logging_mixin.py:115} INFO - [2023-01-04 22:35:26,982] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:35:26,989] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:27,013] {logging_mixin.py:115} INFO - [2023-01-04 22:35:27,012] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:35:27,040] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.881 seconds
[2023-01-04 22:35:57,139] {processor.py:153} INFO - Started process (PID=4815) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:57,140] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:35:57,140] {logging_mixin.py:115} INFO - [2023-01-04 22:35:57,140] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:57,950] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:35:57,951] {logging_mixin.py:115} INFO - [2023-01-04 22:35:57,951] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:35:57,952] {logging_mixin.py:115} INFO - [2023-01-04 22:35:57,951] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:35:57,958] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:35:57,985] {logging_mixin.py:115} INFO - [2023-01-04 22:35:57,985] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:35:58,015] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.881 seconds
[2023-01-04 22:36:28,114] {processor.py:153} INFO - Started process (PID=4832) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:28,115] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:36:28,116] {logging_mixin.py:115} INFO - [2023-01-04 22:36:28,116] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:28,937] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:36:28,939] {logging_mixin.py:115} INFO - [2023-01-04 22:36:28,939] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:36:28,939] {logging_mixin.py:115} INFO - [2023-01-04 22:36:28,939] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:36:28,946] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:28,977] {logging_mixin.py:115} INFO - [2023-01-04 22:36:28,976] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:36:29,009] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.900 seconds
[2023-01-04 22:36:59,100] {processor.py:153} INFO - Started process (PID=4854) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:59,102] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:36:59,102] {logging_mixin.py:115} INFO - [2023-01-04 22:36:59,102] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:59,877] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:36:59,879] {logging_mixin.py:115} INFO - [2023-01-04 22:36:59,879] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:36:59,879] {logging_mixin.py:115} INFO - [2023-01-04 22:36:59,879] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:36:59,886] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:36:59,907] {logging_mixin.py:115} INFO - [2023-01-04 22:36:59,906] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:36:59,933] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.838 seconds
[2023-01-04 22:37:30,019] {processor.py:153} INFO - Started process (PID=4877) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:37:30,019] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:37:30,020] {logging_mixin.py:115} INFO - [2023-01-04 22:37:30,020] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:37:30,802] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:37:30,804] {logging_mixin.py:115} INFO - [2023-01-04 22:37:30,804] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:37:30,804] {logging_mixin.py:115} INFO - [2023-01-04 22:37:30,804] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:37:30,811] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:37:30,833] {logging_mixin.py:115} INFO - [2023-01-04 22:37:30,833] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:37:30,862] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.848 seconds
[2023-01-04 22:38:00,935] {processor.py:153} INFO - Started process (PID=4900) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:00,936] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:38:00,937] {logging_mixin.py:115} INFO - [2023-01-04 22:38:00,937] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:01,714] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:38:01,715] {logging_mixin.py:115} INFO - [2023-01-04 22:38:01,715] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:38:01,716] {logging_mixin.py:115} INFO - [2023-01-04 22:38:01,716] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:38:01,722] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:01,744] {logging_mixin.py:115} INFO - [2023-01-04 22:38:01,744] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:38:01,771] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.842 seconds
[2023-01-04 22:38:31,844] {processor.py:153} INFO - Started process (PID=4917) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:31,844] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:38:31,845] {logging_mixin.py:115} INFO - [2023-01-04 22:38:31,845] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:32,636] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:38:32,637] {logging_mixin.py:115} INFO - [2023-01-04 22:38:32,637] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:38:32,638] {logging_mixin.py:115} INFO - [2023-01-04 22:38:32,637] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:38:32,644] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:38:32,672] {logging_mixin.py:115} INFO - [2023-01-04 22:38:32,672] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:38:32,710] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.871 seconds
[2023-01-04 22:39:02,778] {processor.py:153} INFO - Started process (PID=4941) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:02,779] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:39:02,780] {logging_mixin.py:115} INFO - [2023-01-04 22:39:02,780] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:03,562] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:39:03,563] {logging_mixin.py:115} INFO - [2023-01-04 22:39:03,563] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:39:03,564] {logging_mixin.py:115} INFO - [2023-01-04 22:39:03,564] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:39:03,570] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:03,592] {logging_mixin.py:115} INFO - [2023-01-04 22:39:03,592] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:39:03,612] {logging_mixin.py:115} INFO - [2023-01-04 22:39:03,612] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:39:03,621] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 22:39:33,692] {processor.py:153} INFO - Started process (PID=4963) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:33,694] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:39:33,694] {logging_mixin.py:115} INFO - [2023-01-04 22:39:33,694] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:34,483] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:39:34,484] {logging_mixin.py:115} INFO - [2023-01-04 22:39:34,484] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:39:34,485] {logging_mixin.py:115} INFO - [2023-01-04 22:39:34,484] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:39:34,491] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:39:34,512] {logging_mixin.py:115} INFO - [2023-01-04 22:39:34,512] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:39:34,532] {logging_mixin.py:115} INFO - [2023-01-04 22:39:34,532] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:39:34,541] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 22:40:04,639] {processor.py:153} INFO - Started process (PID=4986) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:04,639] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:40:04,640] {logging_mixin.py:115} INFO - [2023-01-04 22:40:04,640] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:05,444] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:40:05,445] {logging_mixin.py:115} INFO - [2023-01-04 22:40:05,445] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:40:05,446] {logging_mixin.py:115} INFO - [2023-01-04 22:40:05,445] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:40:05,452] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:05,475] {logging_mixin.py:115} INFO - [2023-01-04 22:40:05,475] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:40:05,495] {logging_mixin.py:115} INFO - [2023-01-04 22:40:05,495] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:40:05,504] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.871 seconds
[2023-01-04 22:40:35,600] {processor.py:153} INFO - Started process (PID=5002) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:35,602] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:40:35,602] {logging_mixin.py:115} INFO - [2023-01-04 22:40:35,602] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:36,402] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:40:36,404] {logging_mixin.py:115} INFO - [2023-01-04 22:40:36,403] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:40:36,404] {logging_mixin.py:115} INFO - [2023-01-04 22:40:36,404] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:40:36,411] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:40:36,432] {logging_mixin.py:115} INFO - [2023-01-04 22:40:36,432] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:40:36,451] {logging_mixin.py:115} INFO - [2023-01-04 22:40:36,451] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:40:36,461] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 22:41:06,559] {processor.py:153} INFO - Started process (PID=5026) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:06,559] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:41:06,560] {logging_mixin.py:115} INFO - [2023-01-04 22:41:06,560] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:07,330] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:41:07,332] {logging_mixin.py:115} INFO - [2023-01-04 22:41:07,332] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:41:07,332] {logging_mixin.py:115} INFO - [2023-01-04 22:41:07,332] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:41:07,339] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:07,359] {logging_mixin.py:115} INFO - [2023-01-04 22:41:07,359] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:41:07,379] {logging_mixin.py:115} INFO - [2023-01-04 22:41:07,379] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:41:07,388] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.834 seconds
[2023-01-04 22:41:37,505] {processor.py:153} INFO - Started process (PID=5049) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:37,506] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:41:37,507] {logging_mixin.py:115} INFO - [2023-01-04 22:41:37,507] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:38,293] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:41:38,295] {logging_mixin.py:115} INFO - [2023-01-04 22:41:38,295] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:41:38,295] {logging_mixin.py:115} INFO - [2023-01-04 22:41:38,295] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:41:38,302] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:41:38,323] {logging_mixin.py:115} INFO - [2023-01-04 22:41:38,323] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:41:38,344] {logging_mixin.py:115} INFO - [2023-01-04 22:41:38,344] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:41:38,353] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 22:42:08,428] {processor.py:153} INFO - Started process (PID=5073) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:08,428] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:42:08,429] {logging_mixin.py:115} INFO - [2023-01-04 22:42:08,429] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:09,264] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:42:09,265] {logging_mixin.py:115} INFO - [2023-01-04 22:42:09,265] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:42:09,266] {logging_mixin.py:115} INFO - [2023-01-04 22:42:09,266] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:42:09,272] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:09,294] {logging_mixin.py:115} INFO - [2023-01-04 22:42:09,294] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:42:09,315] {logging_mixin.py:115} INFO - [2023-01-04 22:42:09,315] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:42:09,324] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.902 seconds
[2023-01-04 22:42:39,397] {processor.py:153} INFO - Started process (PID=5089) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:39,398] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:42:39,399] {logging_mixin.py:115} INFO - [2023-01-04 22:42:39,399] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:40,244] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:42:40,245] {logging_mixin.py:115} INFO - [2023-01-04 22:42:40,245] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:42:40,246] {logging_mixin.py:115} INFO - [2023-01-04 22:42:40,245] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:42:40,252] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:42:40,274] {logging_mixin.py:115} INFO - [2023-01-04 22:42:40,274] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:42:40,295] {logging_mixin.py:115} INFO - [2023-01-04 22:42:40,295] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:42:40,305] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.915 seconds
[2023-01-04 22:43:10,376] {processor.py:153} INFO - Started process (PID=5112) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:10,377] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:43:10,377] {logging_mixin.py:115} INFO - [2023-01-04 22:43:10,377] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:11,152] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:43:11,153] {logging_mixin.py:115} INFO - [2023-01-04 22:43:11,153] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:43:11,153] {logging_mixin.py:115} INFO - [2023-01-04 22:43:11,153] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:43:11,160] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:11,181] {logging_mixin.py:115} INFO - [2023-01-04 22:43:11,181] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:43:11,202] {logging_mixin.py:115} INFO - [2023-01-04 22:43:11,202] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:43:11,212] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.842 seconds
[2023-01-04 22:43:41,282] {processor.py:153} INFO - Started process (PID=5133) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:41,284] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:43:41,284] {logging_mixin.py:115} INFO - [2023-01-04 22:43:41,284] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:42,091] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:43:42,092] {logging_mixin.py:115} INFO - [2023-01-04 22:43:42,092] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:43:42,092] {logging_mixin.py:115} INFO - [2023-01-04 22:43:42,092] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:43:42,099] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:43:42,121] {logging_mixin.py:115} INFO - [2023-01-04 22:43:42,121] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:43:42,142] {logging_mixin.py:115} INFO - [2023-01-04 22:43:42,142] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:43:42,151] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.875 seconds
[2023-01-04 22:44:12,218] {processor.py:153} INFO - Started process (PID=5156) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:12,218] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:44:12,219] {logging_mixin.py:115} INFO - [2023-01-04 22:44:12,219] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:12,985] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:44:12,986] {logging_mixin.py:115} INFO - [2023-01-04 22:44:12,986] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:44:12,987] {logging_mixin.py:115} INFO - [2023-01-04 22:44:12,986] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:44:12,993] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:13,014] {logging_mixin.py:115} INFO - [2023-01-04 22:44:13,014] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:44:13,035] {logging_mixin.py:115} INFO - [2023-01-04 22:44:13,035] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:44:13,044] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.832 seconds
[2023-01-04 22:44:43,143] {processor.py:153} INFO - Started process (PID=5172) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:43,145] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:44:43,146] {logging_mixin.py:115} INFO - [2023-01-04 22:44:43,145] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:44,096] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:44:44,098] {logging_mixin.py:115} INFO - [2023-01-04 22:44:44,097] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:44:44,098] {logging_mixin.py:115} INFO - [2023-01-04 22:44:44,098] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:44:44,109] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:44:44,142] {logging_mixin.py:115} INFO - [2023-01-04 22:44:44,141] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:44:44,172] {logging_mixin.py:115} INFO - [2023-01-04 22:44:44,172] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:44:44,187] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.049 seconds
[2023-01-04 22:45:14,257] {processor.py:153} INFO - Started process (PID=5196) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:14,258] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:45:14,258] {logging_mixin.py:115} INFO - [2023-01-04 22:45:14,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:15,091] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:45:15,093] {logging_mixin.py:115} INFO - [2023-01-04 22:45:15,093] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:45:15,093] {logging_mixin.py:115} INFO - [2023-01-04 22:45:15,093] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:45:15,100] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:15,121] {logging_mixin.py:115} INFO - [2023-01-04 22:45:15,121] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:45:15,142] {logging_mixin.py:115} INFO - [2023-01-04 22:45:15,142] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:45:15,152] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.902 seconds
[2023-01-04 22:45:45,255] {processor.py:153} INFO - Started process (PID=5220) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:45,256] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:45:45,256] {logging_mixin.py:115} INFO - [2023-01-04 22:45:45,256] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:46,046] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:45:46,047] {logging_mixin.py:115} INFO - [2023-01-04 22:45:46,047] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:45:46,048] {logging_mixin.py:115} INFO - [2023-01-04 22:45:46,048] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:45:46,054] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:45:46,076] {logging_mixin.py:115} INFO - [2023-01-04 22:45:46,076] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:45:46,097] {logging_mixin.py:115} INFO - [2023-01-04 22:45:46,097] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:45:46,106] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:46:16,218] {processor.py:153} INFO - Started process (PID=5244) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:16,219] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:46:16,219] {logging_mixin.py:115} INFO - [2023-01-04 22:46:16,219] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:17,016] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:46:17,018] {logging_mixin.py:115} INFO - [2023-01-04 22:46:17,017] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:46:17,018] {logging_mixin.py:115} INFO - [2023-01-04 22:46:17,018] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:46:17,025] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:17,045] {logging_mixin.py:115} INFO - [2023-01-04 22:46:17,045] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:46:17,066] {logging_mixin.py:115} INFO - [2023-01-04 22:46:17,066] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 22:46:17,075] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.863 seconds
[2023-01-04 22:46:47,178] {processor.py:153} INFO - Started process (PID=5260) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:47,182] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:46:47,183] {logging_mixin.py:115} INFO - [2023-01-04 22:46:47,183] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:48,435] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:46:48,436] {logging_mixin.py:115} INFO - [2023-01-04 22:46:48,436] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:46:48,437] {logging_mixin.py:115} INFO - [2023-01-04 22:46:48,437] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:46:48,443] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:46:48,465] {logging_mixin.py:115} INFO - [2023-01-04 22:46:48,464] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:46:48,498] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.326 seconds
[2023-01-04 22:47:18,570] {processor.py:153} INFO - Started process (PID=5283) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:18,573] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:47:18,574] {logging_mixin.py:115} INFO - [2023-01-04 22:47:18,573] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:19,374] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:47:19,375] {logging_mixin.py:115} INFO - [2023-01-04 22:47:19,375] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:47:19,376] {logging_mixin.py:115} INFO - [2023-01-04 22:47:19,375] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:47:19,382] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:19,404] {logging_mixin.py:115} INFO - [2023-01-04 22:47:19,404] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:47:19,432] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.867 seconds
[2023-01-04 22:47:49,522] {processor.py:153} INFO - Started process (PID=5308) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:49,523] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:47:49,524] {logging_mixin.py:115} INFO - [2023-01-04 22:47:49,524] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:50,309] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:47:50,310] {logging_mixin.py:115} INFO - [2023-01-04 22:47:50,310] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:47:50,310] {logging_mixin.py:115} INFO - [2023-01-04 22:47:50,310] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:47:50,317] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:47:50,338] {logging_mixin.py:115} INFO - [2023-01-04 22:47:50,337] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:47:50,365] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 22:48:20,459] {processor.py:153} INFO - Started process (PID=5331) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:20,459] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:48:20,460] {logging_mixin.py:115} INFO - [2023-01-04 22:48:20,460] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:21,236] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:48:21,237] {logging_mixin.py:115} INFO - [2023-01-04 22:48:21,237] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:48:21,238] {logging_mixin.py:115} INFO - [2023-01-04 22:48:21,238] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:48:21,244] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:21,268] {logging_mixin.py:115} INFO - [2023-01-04 22:48:21,268] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:48:21,297] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.843 seconds
[2023-01-04 22:48:51,396] {processor.py:153} INFO - Started process (PID=5347) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:51,401] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:48:51,402] {logging_mixin.py:115} INFO - [2023-01-04 22:48:51,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:52,223] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:48:52,224] {logging_mixin.py:115} INFO - [2023-01-04 22:48:52,224] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:48:52,225] {logging_mixin.py:115} INFO - [2023-01-04 22:48:52,225] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:48:52,232] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:48:52,254] {logging_mixin.py:115} INFO - [2023-01-04 22:48:52,254] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:48:52,285] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.894 seconds
[2023-01-04 22:49:22,382] {processor.py:153} INFO - Started process (PID=5370) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:22,383] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:49:22,384] {logging_mixin.py:115} INFO - [2023-01-04 22:49:22,384] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:23,239] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:49:23,241] {logging_mixin.py:115} INFO - [2023-01-04 22:49:23,241] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:49:23,241] {logging_mixin.py:115} INFO - [2023-01-04 22:49:23,241] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:49:23,248] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:23,270] {logging_mixin.py:115} INFO - [2023-01-04 22:49:23,270] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:49:23,301] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.924 seconds
[2023-01-04 22:49:53,391] {processor.py:153} INFO - Started process (PID=5393) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:53,392] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:49:53,393] {logging_mixin.py:115} INFO - [2023-01-04 22:49:53,392] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:54,225] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:49:54,227] {logging_mixin.py:115} INFO - [2023-01-04 22:49:54,226] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:49:54,227] {logging_mixin.py:115} INFO - [2023-01-04 22:49:54,227] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:49:54,234] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:49:54,256] {logging_mixin.py:115} INFO - [2023-01-04 22:49:54,256] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:49:54,284] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.898 seconds
[2023-01-04 22:50:24,365] {processor.py:153} INFO - Started process (PID=5418) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:24,366] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:50:24,366] {logging_mixin.py:115} INFO - [2023-01-04 22:50:24,366] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:25,172] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:50:25,173] {logging_mixin.py:115} INFO - [2023-01-04 22:50:25,173] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:50:25,174] {logging_mixin.py:115} INFO - [2023-01-04 22:50:25,173] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:50:25,181] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:25,204] {logging_mixin.py:115} INFO - [2023-01-04 22:50:25,203] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:50:25,233] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.873 seconds
[2023-01-04 22:50:55,304] {processor.py:153} INFO - Started process (PID=5433) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:55,304] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:50:55,305] {logging_mixin.py:115} INFO - [2023-01-04 22:50:55,305] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:56,128] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:50:56,129] {logging_mixin.py:115} INFO - [2023-01-04 22:50:56,129] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:50:56,130] {logging_mixin.py:115} INFO - [2023-01-04 22:50:56,129] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:50:56,136] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:50:56,162] {logging_mixin.py:115} INFO - [2023-01-04 22:50:56,162] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:50:56,200] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.901 seconds
[2023-01-04 22:51:26,283] {processor.py:153} INFO - Started process (PID=5455) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:26,284] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:51:26,285] {logging_mixin.py:115} INFO - [2023-01-04 22:51:26,285] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:27,077] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:51:27,078] {logging_mixin.py:115} INFO - [2023-01-04 22:51:27,078] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:51:27,079] {logging_mixin.py:115} INFO - [2023-01-04 22:51:27,079] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:51:27,086] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:27,107] {logging_mixin.py:115} INFO - [2023-01-04 22:51:27,107] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:51:27,135] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.856 seconds
[2023-01-04 22:51:57,211] {processor.py:153} INFO - Started process (PID=5479) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:57,212] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:51:57,212] {logging_mixin.py:115} INFO - [2023-01-04 22:51:57,212] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:57,996] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:51:57,997] {logging_mixin.py:115} INFO - [2023-01-04 22:51:57,997] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:51:57,998] {logging_mixin.py:115} INFO - [2023-01-04 22:51:57,997] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:51:58,004] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:51:58,025] {logging_mixin.py:115} INFO - [2023-01-04 22:51:58,025] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:51:58,054] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 22:52:28,122] {processor.py:153} INFO - Started process (PID=5502) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:28,123] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:52:28,124] {logging_mixin.py:115} INFO - [2023-01-04 22:52:28,124] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:28,928] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:52:28,929] {logging_mixin.py:115} INFO - [2023-01-04 22:52:28,929] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:52:28,930] {logging_mixin.py:115} INFO - [2023-01-04 22:52:28,930] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:52:28,937] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:28,958] {logging_mixin.py:115} INFO - [2023-01-04 22:52:28,958] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:52:28,987] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.870 seconds
[2023-01-04 22:52:59,049] {processor.py:153} INFO - Started process (PID=5518) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:59,049] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:52:59,050] {logging_mixin.py:115} INFO - [2023-01-04 22:52:59,050] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:59,837] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:52:59,838] {logging_mixin.py:115} INFO - [2023-01-04 22:52:59,838] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:52:59,839] {logging_mixin.py:115} INFO - [2023-01-04 22:52:59,838] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:52:59,845] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:52:59,866] {logging_mixin.py:115} INFO - [2023-01-04 22:52:59,866] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:52:59,895] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.850 seconds
[2023-01-04 22:53:29,996] {processor.py:153} INFO - Started process (PID=5541) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:53:29,998] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:53:29,998] {logging_mixin.py:115} INFO - [2023-01-04 22:53:29,998] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:53:30,795] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:53:30,796] {logging_mixin.py:115} INFO - [2023-01-04 22:53:30,796] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:53:30,797] {logging_mixin.py:115} INFO - [2023-01-04 22:53:30,796] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:53:30,803] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:53:30,827] {logging_mixin.py:115} INFO - [2023-01-04 22:53:30,827] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:53:30,856] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 22:54:00,956] {processor.py:153} INFO - Started process (PID=5564) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:00,957] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:54:00,957] {logging_mixin.py:115} INFO - [2023-01-04 22:54:00,957] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:01,750] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:54:01,751] {logging_mixin.py:115} INFO - [2023-01-04 22:54:01,751] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:54:01,752] {logging_mixin.py:115} INFO - [2023-01-04 22:54:01,752] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:54:01,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:01,780] {logging_mixin.py:115} INFO - [2023-01-04 22:54:01,780] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:54:01,809] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 22:54:31,903] {processor.py:153} INFO - Started process (PID=5580) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:31,905] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:54:31,905] {logging_mixin.py:115} INFO - [2023-01-04 22:54:31,905] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:32,794] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:54:32,796] {logging_mixin.py:115} INFO - [2023-01-04 22:54:32,795] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:54:32,796] {logging_mixin.py:115} INFO - [2023-01-04 22:54:32,796] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:54:32,803] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:54:32,824] {logging_mixin.py:115} INFO - [2023-01-04 22:54:32,824] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:54:32,856] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-04 22:55:02,955] {processor.py:153} INFO - Started process (PID=5603) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:02,957] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:55:02,957] {logging_mixin.py:115} INFO - [2023-01-04 22:55:02,957] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:03,793] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:55:03,794] {logging_mixin.py:115} INFO - [2023-01-04 22:55:03,794] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:55:03,795] {logging_mixin.py:115} INFO - [2023-01-04 22:55:03,795] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:55:03,801] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:03,823] {logging_mixin.py:115} INFO - [2023-01-04 22:55:03,823] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:55:03,852] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.901 seconds
[2023-01-04 22:55:33,945] {processor.py:153} INFO - Started process (PID=5626) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:33,946] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:55:33,947] {logging_mixin.py:115} INFO - [2023-01-04 22:55:33,947] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:34,735] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:55:34,737] {logging_mixin.py:115} INFO - [2023-01-04 22:55:34,737] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:55:34,737] {logging_mixin.py:115} INFO - [2023-01-04 22:55:34,737] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:55:34,744] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:55:34,766] {logging_mixin.py:115} INFO - [2023-01-04 22:55:34,765] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:55:34,794] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 22:56:04,863] {processor.py:153} INFO - Started process (PID=5650) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:04,863] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:56:04,864] {logging_mixin.py:115} INFO - [2023-01-04 22:56:04,864] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:05,645] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:56:05,646] {logging_mixin.py:115} INFO - [2023-01-04 22:56:05,646] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:56:05,647] {logging_mixin.py:115} INFO - [2023-01-04 22:56:05,646] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:56:05,653] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:05,674] {logging_mixin.py:115} INFO - [2023-01-04 22:56:05,674] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:56:05,702] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.844 seconds
[2023-01-04 22:56:35,785] {processor.py:153} INFO - Started process (PID=5666) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:35,786] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:56:35,787] {logging_mixin.py:115} INFO - [2023-01-04 22:56:35,787] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:36,580] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:56:36,582] {logging_mixin.py:115} INFO - [2023-01-04 22:56:36,582] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:56:36,582] {logging_mixin.py:115} INFO - [2023-01-04 22:56:36,582] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:56:36,589] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:56:36,612] {logging_mixin.py:115} INFO - [2023-01-04 22:56:36,612] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:56:36,641] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.862 seconds
[2023-01-04 22:57:06,713] {processor.py:153} INFO - Started process (PID=5689) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:06,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:57:06,714] {logging_mixin.py:115} INFO - [2023-01-04 22:57:06,714] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:07,519] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:57:07,521] {logging_mixin.py:115} INFO - [2023-01-04 22:57:07,521] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:57:07,521] {logging_mixin.py:115} INFO - [2023-01-04 22:57:07,521] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:57:07,528] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:07,549] {logging_mixin.py:115} INFO - [2023-01-04 22:57:07,548] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:57:07,576] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.868 seconds
[2023-01-04 22:57:37,643] {processor.py:153} INFO - Started process (PID=5711) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:37,647] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:57:37,647] {logging_mixin.py:115} INFO - [2023-01-04 22:57:37,647] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:38,431] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:57:38,432] {logging_mixin.py:115} INFO - [2023-01-04 22:57:38,432] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:57:38,433] {logging_mixin.py:115} INFO - [2023-01-04 22:57:38,433] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:57:38,439] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:57:38,460] {logging_mixin.py:115} INFO - [2023-01-04 22:57:38,460] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:57:38,487] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 22:58:08,584] {processor.py:153} INFO - Started process (PID=5734) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:08,588] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:58:08,589] {logging_mixin.py:115} INFO - [2023-01-04 22:58:08,589] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:09,352] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:58:09,353] {logging_mixin.py:115} INFO - [2023-01-04 22:58:09,353] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:58:09,353] {logging_mixin.py:115} INFO - [2023-01-04 22:58:09,353] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:58:09,360] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:09,381] {logging_mixin.py:115} INFO - [2023-01-04 22:58:09,381] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:58:09,409] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.830 seconds
[2023-01-04 22:58:39,512] {processor.py:153} INFO - Started process (PID=5750) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:39,513] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:58:39,514] {logging_mixin.py:115} INFO - [2023-01-04 22:58:39,514] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:40,383] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:58:40,385] {logging_mixin.py:115} INFO - [2023-01-04 22:58:40,385] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:58:40,385] {logging_mixin.py:115} INFO - [2023-01-04 22:58:40,385] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:58:40,392] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:58:40,413] {logging_mixin.py:115} INFO - [2023-01-04 22:58:40,413] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:58:40,441] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-04 22:59:10,541] {processor.py:153} INFO - Started process (PID=5773) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:10,542] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:59:10,542] {logging_mixin.py:115} INFO - [2023-01-04 22:59:10,542] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:11,330] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:59:11,331] {logging_mixin.py:115} INFO - [2023-01-04 22:59:11,331] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:59:11,332] {logging_mixin.py:115} INFO - [2023-01-04 22:59:11,331] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:59:11,338] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:11,360] {logging_mixin.py:115} INFO - [2023-01-04 22:59:11,359] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:59:11,387] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 22:59:41,482] {processor.py:153} INFO - Started process (PID=5796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:41,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 22:59:41,483] {logging_mixin.py:115} INFO - [2023-01-04 22:59:41,483] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:42,255] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 22:59:42,256] {logging_mixin.py:115} INFO - [2023-01-04 22:59:42,256] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 22:59:42,257] {logging_mixin.py:115} INFO - [2023-01-04 22:59:42,257] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 22:59:42,264] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 22:59:42,285] {logging_mixin.py:115} INFO - [2023-01-04 22:59:42,285] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 22:59:42,313] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 23:00:12,387] {processor.py:153} INFO - Started process (PID=5818) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:12,387] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:00:12,388] {logging_mixin.py:115} INFO - [2023-01-04 23:00:12,388] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:13,265] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:00:13,266] {logging_mixin.py:115} INFO - [2023-01-04 23:00:13,266] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:00:13,267] {logging_mixin.py:115} INFO - [2023-01-04 23:00:13,267] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:00:13,274] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:13,295] {logging_mixin.py:115} INFO - [2023-01-04 23:00:13,295] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:00:13,324] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.942 seconds
[2023-01-04 23:00:43,394] {processor.py:153} INFO - Started process (PID=5834) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:43,395] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:00:43,395] {logging_mixin.py:115} INFO - [2023-01-04 23:00:43,395] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:44,164] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:00:44,166] {logging_mixin.py:115} INFO - [2023-01-04 23:00:44,166] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:00:44,166] {logging_mixin.py:115} INFO - [2023-01-04 23:00:44,166] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:00:44,173] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:00:44,194] {logging_mixin.py:115} INFO - [2023-01-04 23:00:44,193] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:00:44,222] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.832 seconds
[2023-01-04 23:01:14,294] {processor.py:153} INFO - Started process (PID=5856) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:14,295] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:01:14,295] {logging_mixin.py:115} INFO - [2023-01-04 23:01:14,295] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:15,087] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:01:15,088] {logging_mixin.py:115} INFO - [2023-01-04 23:01:15,088] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:01:15,088] {logging_mixin.py:115} INFO - [2023-01-04 23:01:15,088] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:01:15,095] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:15,116] {logging_mixin.py:115} INFO - [2023-01-04 23:01:15,116] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:01:15,143] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 23:01:45,213] {processor.py:153} INFO - Started process (PID=5879) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:45,215] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:01:45,216] {logging_mixin.py:115} INFO - [2023-01-04 23:01:45,216] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:46,013] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:01:46,015] {logging_mixin.py:115} INFO - [2023-01-04 23:01:46,014] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:01:46,015] {logging_mixin.py:115} INFO - [2023-01-04 23:01:46,015] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:01:46,022] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:01:46,043] {logging_mixin.py:115} INFO - [2023-01-04 23:01:46,043] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:01:46,072] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 23:02:16,129] {processor.py:153} INFO - Started process (PID=5905) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:16,132] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:02:16,132] {logging_mixin.py:115} INFO - [2023-01-04 23:02:16,132] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:16,902] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:02:16,903] {logging_mixin.py:115} INFO - [2023-01-04 23:02:16,903] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:02:16,904] {logging_mixin.py:115} INFO - [2023-01-04 23:02:16,903] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:02:16,910] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:16,932] {logging_mixin.py:115} INFO - [2023-01-04 23:02:16,931] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:02:16,960] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 23:02:47,055] {processor.py:153} INFO - Started process (PID=5921) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:47,056] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:02:47,057] {logging_mixin.py:115} INFO - [2023-01-04 23:02:47,057] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:47,826] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:02:47,827] {logging_mixin.py:115} INFO - [2023-01-04 23:02:47,827] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:02:47,828] {logging_mixin.py:115} INFO - [2023-01-04 23:02:47,828] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:02:47,834] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:02:47,856] {logging_mixin.py:115} INFO - [2023-01-04 23:02:47,856] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:02:47,884] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.834 seconds
[2023-01-04 23:03:17,979] {processor.py:153} INFO - Started process (PID=5944) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:17,980] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:03:17,980] {logging_mixin.py:115} INFO - [2023-01-04 23:03:17,980] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:18,793] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:03:18,794] {logging_mixin.py:115} INFO - [2023-01-04 23:03:18,794] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:03:18,795] {logging_mixin.py:115} INFO - [2023-01-04 23:03:18,794] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:03:18,802] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:18,823] {logging_mixin.py:115} INFO - [2023-01-04 23:03:18,823] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:03:18,851] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.878 seconds
[2023-01-04 23:03:48,946] {processor.py:153} INFO - Started process (PID=5967) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:48,949] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:03:48,950] {logging_mixin.py:115} INFO - [2023-01-04 23:03:48,950] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:49,734] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:03:49,735] {logging_mixin.py:115} INFO - [2023-01-04 23:03:49,735] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:03:49,736] {logging_mixin.py:115} INFO - [2023-01-04 23:03:49,736] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:03:49,743] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:03:49,764] {logging_mixin.py:115} INFO - [2023-01-04 23:03:49,764] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:03:49,792] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 23:04:19,865] {processor.py:153} INFO - Started process (PID=5990) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:19,865] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:04:19,866] {logging_mixin.py:115} INFO - [2023-01-04 23:04:19,866] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:20,637] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:04:20,639] {logging_mixin.py:115} INFO - [2023-01-04 23:04:20,639] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:04:20,639] {logging_mixin.py:115} INFO - [2023-01-04 23:04:20,639] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:04:20,646] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:20,670] {logging_mixin.py:115} INFO - [2023-01-04 23:04:20,670] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:04:20,708] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.849 seconds
[2023-01-04 23:04:50,795] {processor.py:153} INFO - Started process (PID=6006) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:50,797] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:04:50,797] {logging_mixin.py:115} INFO - [2023-01-04 23:04:50,797] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:51,624] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:04:51,625] {logging_mixin.py:115} INFO - [2023-01-04 23:04:51,625] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:04:51,626] {logging_mixin.py:115} INFO - [2023-01-04 23:04:51,626] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:04:51,634] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:04:51,656] {logging_mixin.py:115} INFO - [2023-01-04 23:04:51,656] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:04:51,685] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.894 seconds
[2023-01-04 23:05:21,755] {processor.py:153} INFO - Started process (PID=6029) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:21,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:05:21,757] {logging_mixin.py:115} INFO - [2023-01-04 23:05:21,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:22,583] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:05:22,585] {logging_mixin.py:115} INFO - [2023-01-04 23:05:22,585] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:05:22,586] {logging_mixin.py:115} INFO - [2023-01-04 23:05:22,585] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:05:22,592] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:22,614] {logging_mixin.py:115} INFO - [2023-01-04 23:05:22,614] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:05:22,642] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.892 seconds
[2023-01-04 23:05:52,712] {processor.py:153} INFO - Started process (PID=6053) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:52,713] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:05:52,713] {logging_mixin.py:115} INFO - [2023-01-04 23:05:52,713] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:53,541] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:05:53,543] {logging_mixin.py:115} INFO - [2023-01-04 23:05:53,543] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:05:53,543] {logging_mixin.py:115} INFO - [2023-01-04 23:05:53,543] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:05:53,550] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:05:53,571] {logging_mixin.py:115} INFO - [2023-01-04 23:05:53,570] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:05:53,599] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.892 seconds
[2023-01-04 23:06:23,664] {processor.py:153} INFO - Started process (PID=6077) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:23,666] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:06:23,666] {logging_mixin.py:115} INFO - [2023-01-04 23:06:23,666] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:24,439] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:06:24,441] {logging_mixin.py:115} INFO - [2023-01-04 23:06:24,441] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:06:24,441] {logging_mixin.py:115} INFO - [2023-01-04 23:06:24,441] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:06:24,448] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:24,469] {logging_mixin.py:115} INFO - [2023-01-04 23:06:24,468] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:06:24,496] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.837 seconds
[2023-01-04 23:06:54,595] {processor.py:153} INFO - Started process (PID=6093) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:54,596] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:06:54,597] {logging_mixin.py:115} INFO - [2023-01-04 23:06:54,596] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:55,394] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:06:55,395] {logging_mixin.py:115} INFO - [2023-01-04 23:06:55,395] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:06:55,396] {logging_mixin.py:115} INFO - [2023-01-04 23:06:55,395] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:06:55,402] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:06:55,424] {logging_mixin.py:115} INFO - [2023-01-04 23:06:55,424] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:06:55,452] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.861 seconds
[2023-01-04 23:07:25,547] {processor.py:153} INFO - Started process (PID=6117) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:25,548] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:07:25,549] {logging_mixin.py:115} INFO - [2023-01-04 23:07:25,549] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:26,337] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:07:26,338] {logging_mixin.py:115} INFO - [2023-01-04 23:07:26,338] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:07:26,339] {logging_mixin.py:115} INFO - [2023-01-04 23:07:26,339] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:07:26,345] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:26,366] {logging_mixin.py:115} INFO - [2023-01-04 23:07:26,366] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:07:26,394] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 23:07:56,489] {processor.py:153} INFO - Started process (PID=6140) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:56,490] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:07:56,490] {logging_mixin.py:115} INFO - [2023-01-04 23:07:56,490] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:57,271] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:07:57,273] {logging_mixin.py:115} INFO - [2023-01-04 23:07:57,272] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:07:57,273] {logging_mixin.py:115} INFO - [2023-01-04 23:07:57,273] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:07:57,280] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:07:57,304] {logging_mixin.py:115} INFO - [2023-01-04 23:07:57,303] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:07:57,331] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.847 seconds
[2023-01-04 23:08:27,403] {processor.py:153} INFO - Started process (PID=6164) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:27,404] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:08:27,405] {logging_mixin.py:115} INFO - [2023-01-04 23:08:27,405] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:28,194] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:08:28,195] {logging_mixin.py:115} INFO - [2023-01-04 23:08:28,195] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:08:28,196] {logging_mixin.py:115} INFO - [2023-01-04 23:08:28,196] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:08:28,202] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:28,223] {logging_mixin.py:115} INFO - [2023-01-04 23:08:28,223] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:08:28,250] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.852 seconds
[2023-01-04 23:08:58,322] {processor.py:153} INFO - Started process (PID=6179) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:58,323] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:08:58,323] {logging_mixin.py:115} INFO - [2023-01-04 23:08:58,323] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:59,113] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:08:59,115] {logging_mixin.py:115} INFO - [2023-01-04 23:08:59,115] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:08:59,115] {logging_mixin.py:115} INFO - [2023-01-04 23:08:59,115] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:08:59,122] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:08:59,142] {logging_mixin.py:115} INFO - [2023-01-04 23:08:59,142] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:08:59,170] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.853 seconds
[2023-01-04 23:09:29,242] {processor.py:153} INFO - Started process (PID=6202) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:09:29,246] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:09:29,247] {logging_mixin.py:115} INFO - [2023-01-04 23:09:29,247] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:09:30,025] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:09:30,026] {logging_mixin.py:115} INFO - [2023-01-04 23:09:30,026] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:09:30,027] {logging_mixin.py:115} INFO - [2023-01-04 23:09:30,027] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:09:30,033] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:09:30,055] {logging_mixin.py:115} INFO - [2023-01-04 23:09:30,055] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:09:30,083] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.846 seconds
[2023-01-04 23:10:00,152] {processor.py:153} INFO - Started process (PID=6226) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:00,152] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:10:00,153] {logging_mixin.py:115} INFO - [2023-01-04 23:10:00,153] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:00,924] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:10:00,925] {logging_mixin.py:115} INFO - [2023-01-04 23:10:00,925] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:10:00,926] {logging_mixin.py:115} INFO - [2023-01-04 23:10:00,925] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:10:00,932] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:00,953] {logging_mixin.py:115} INFO - [2023-01-04 23:10:00,953] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:10:00,982] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 23:10:31,085] {processor.py:153} INFO - Started process (PID=6250) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:31,087] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:10:31,088] {logging_mixin.py:115} INFO - [2023-01-04 23:10:31,088] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:31,979] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:10:31,980] {logging_mixin.py:115} INFO - [2023-01-04 23:10:31,980] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:10:31,981] {logging_mixin.py:115} INFO - [2023-01-04 23:10:31,980] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:10:31,988] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:10:32,016] {logging_mixin.py:115} INFO - [2023-01-04 23:10:32,016] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:10:32,047] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.967 seconds
[2023-01-04 23:11:02,139] {processor.py:153} INFO - Started process (PID=6265) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:02,140] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:11:02,141] {logging_mixin.py:115} INFO - [2023-01-04 23:11:02,140] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:02,970] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:11:02,971] {logging_mixin.py:115} INFO - [2023-01-04 23:11:02,971] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:11:02,972] {logging_mixin.py:115} INFO - [2023-01-04 23:11:02,972] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:11:02,979] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:03,000] {logging_mixin.py:115} INFO - [2023-01-04 23:11:03,000] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:11:03,028] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.894 seconds
[2023-01-04 23:11:33,145] {processor.py:153} INFO - Started process (PID=6288) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:33,147] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:11:33,149] {logging_mixin.py:115} INFO - [2023-01-04 23:11:33,148] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:33,928] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:11:33,930] {logging_mixin.py:115} INFO - [2023-01-04 23:11:33,930] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:11:33,931] {logging_mixin.py:115} INFO - [2023-01-04 23:11:33,931] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:11:33,941] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:11:33,963] {logging_mixin.py:115} INFO - [2023-01-04 23:11:33,963] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:11:33,996] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.855 seconds
[2023-01-04 23:12:04,091] {processor.py:153} INFO - Started process (PID=6311) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:04,091] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:12:04,093] {logging_mixin.py:115} INFO - [2023-01-04 23:12:04,093] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:04,943] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:12:04,944] {logging_mixin.py:115} INFO - [2023-01-04 23:12:04,944] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:12:04,945] {logging_mixin.py:115} INFO - [2023-01-04 23:12:04,944] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:12:04,951] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:04,974] {logging_mixin.py:115} INFO - [2023-01-04 23:12:04,973] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:12:05,002] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.916 seconds
[2023-01-04 23:12:35,094] {processor.py:153} INFO - Started process (PID=6332) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:35,095] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:12:35,097] {logging_mixin.py:115} INFO - [2023-01-04 23:12:35,097] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:35,887] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:12:35,888] {logging_mixin.py:115} INFO - [2023-01-04 23:12:35,888] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:12:35,889] {logging_mixin.py:115} INFO - [2023-01-04 23:12:35,889] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:12:35,896] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:12:35,917] {logging_mixin.py:115} INFO - [2023-01-04 23:12:35,917] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:12:35,946] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.857 seconds
[2023-01-04 23:13:06,031] {processor.py:153} INFO - Started process (PID=6347) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:06,033] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:13:06,034] {logging_mixin.py:115} INFO - [2023-01-04 23:13:06,034] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:06,829] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:13:06,830] {logging_mixin.py:115} INFO - [2023-01-04 23:13:06,830] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:13:06,830] {logging_mixin.py:115} INFO - [2023-01-04 23:13:06,830] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:13:06,837] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:06,859] {logging_mixin.py:115} INFO - [2023-01-04 23:13:06,859] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:13:06,888] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.861 seconds
[2023-01-04 23:13:36,957] {processor.py:153} INFO - Started process (PID=6370) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:36,958] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:13:36,959] {logging_mixin.py:115} INFO - [2023-01-04 23:13:36,959] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:37,751] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:13:37,752] {logging_mixin.py:115} INFO - [2023-01-04 23:13:37,752] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:13:37,753] {logging_mixin.py:115} INFO - [2023-01-04 23:13:37,752] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:13:37,759] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:13:37,781] {logging_mixin.py:115} INFO - [2023-01-04 23:13:37,781] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:13:37,811] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.859 seconds
[2023-01-04 23:14:07,881] {processor.py:153} INFO - Started process (PID=6394) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:07,882] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:14:07,884] {logging_mixin.py:115} INFO - [2023-01-04 23:14:07,883] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:08,676] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:14:08,677] {logging_mixin.py:115} INFO - [2023-01-04 23:14:08,677] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:14:08,677] {logging_mixin.py:115} INFO - [2023-01-04 23:14:08,677] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:14:08,684] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:08,705] {logging_mixin.py:115} INFO - [2023-01-04 23:14:08,705] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:14:08,734] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.858 seconds
[2023-01-04 23:14:38,804] {processor.py:153} INFO - Started process (PID=6418) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:38,804] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:14:38,806] {logging_mixin.py:115} INFO - [2023-01-04 23:14:38,806] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:39,592] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:14:39,593] {logging_mixin.py:115} INFO - [2023-01-04 23:14:39,593] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:14:39,594] {logging_mixin.py:115} INFO - [2023-01-04 23:14:39,593] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:14:39,600] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:14:39,623] {logging_mixin.py:115} INFO - [2023-01-04 23:14:39,622] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:14:39,651] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.851 seconds
[2023-01-04 23:15:09,724] {processor.py:153} INFO - Started process (PID=6434) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:09,726] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:15:09,728] {logging_mixin.py:115} INFO - [2023-01-04 23:15:09,728] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:10,790] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:15:10,791] {logging_mixin.py:115} INFO - [2023-01-04 23:15:10,791] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:15:10,792] {logging_mixin.py:115} INFO - [2023-01-04 23:15:10,791] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:15:10,798] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:10,820] {logging_mixin.py:115} INFO - [2023-01-04 23:15:10,820] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:15:10,849] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.130 seconds
[2023-01-04 23:15:40,917] {processor.py:153} INFO - Started process (PID=6457) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:40,918] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:15:40,919] {logging_mixin.py:115} INFO - [2023-01-04 23:15:40,919] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:41,740] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:15:41,741] {logging_mixin.py:115} INFO - [2023-01-04 23:15:41,741] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:15:41,742] {logging_mixin.py:115} INFO - [2023-01-04 23:15:41,741] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:15:41,748] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:15:41,769] {logging_mixin.py:115} INFO - [2023-01-04 23:15:41,769] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:15:41,797] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.884 seconds
[2023-01-04 23:16:11,862] {processor.py:153} INFO - Started process (PID=6480) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:11,868] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:16:11,870] {logging_mixin.py:115} INFO - [2023-01-04 23:16:11,869] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:12,667] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:16:12,669] {logging_mixin.py:115} INFO - [2023-01-04 23:16:12,669] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:16:12,669] {logging_mixin.py:115} INFO - [2023-01-04 23:16:12,669] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:16:12,676] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:12,701] {logging_mixin.py:115} INFO - [2023-01-04 23:16:12,701] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:16:12,731] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.873 seconds
[2023-01-04 23:16:42,839] {processor.py:153} INFO - Started process (PID=6502) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:42,840] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:16:42,841] {logging_mixin.py:115} INFO - [2023-01-04 23:16:42,841] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:43,668] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:16:43,670] {logging_mixin.py:115} INFO - [2023-01-04 23:16:43,670] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:16:43,670] {logging_mixin.py:115} INFO - [2023-01-04 23:16:43,670] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:16:43,677] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:16:43,699] {logging_mixin.py:115} INFO - [2023-01-04 23:16:43,699] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:16:43,727] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.893 seconds
[2023-01-04 23:17:13,827] {processor.py:153} INFO - Started process (PID=6520) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:13,829] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:17:13,829] {logging_mixin.py:115} INFO - [2023-01-04 23:17:13,829] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:14,618] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:17:14,619] {logging_mixin.py:115} INFO - [2023-01-04 23:17:14,619] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:17:14,620] {logging_mixin.py:115} INFO - [2023-01-04 23:17:14,620] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:17:14,626] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:14,648] {logging_mixin.py:115} INFO - [2023-01-04 23:17:14,647] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:17:14,677] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 23:17:44,771] {processor.py:153} INFO - Started process (PID=6541) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:44,772] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:17:44,773] {logging_mixin.py:115} INFO - [2023-01-04 23:17:44,773] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:45,563] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:17:45,564] {logging_mixin.py:115} INFO - [2023-01-04 23:17:45,564] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:17:45,565] {logging_mixin.py:115} INFO - [2023-01-04 23:17:45,564] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:17:45,571] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:17:45,592] {logging_mixin.py:115} INFO - [2023-01-04 23:17:45,592] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:17:45,620] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 23:18:15,713] {processor.py:153} INFO - Started process (PID=6564) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:15,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:18:15,714] {logging_mixin.py:115} INFO - [2023-01-04 23:18:15,714] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:16,577] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:18:16,578] {logging_mixin.py:115} INFO - [2023-01-04 23:18:16,578] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:18:16,578] {logging_mixin.py:115} INFO - [2023-01-04 23:18:16,578] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:18:16,585] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:16,608] {logging_mixin.py:115} INFO - [2023-01-04 23:18:16,608] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:18:16,637] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.928 seconds
[2023-01-04 23:18:46,726] {processor.py:153} INFO - Started process (PID=6587) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:46,727] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:18:46,728] {logging_mixin.py:115} INFO - [2023-01-04 23:18:46,727] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:47,505] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:18:47,506] {logging_mixin.py:115} INFO - [2023-01-04 23:18:47,506] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:18:47,507] {logging_mixin.py:115} INFO - [2023-01-04 23:18:47,506] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:18:47,513] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:18:47,534] {logging_mixin.py:115} INFO - [2023-01-04 23:18:47,534] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:18:47,562] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.841 seconds
[2023-01-04 23:19:17,632] {processor.py:153} INFO - Started process (PID=6602) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:17,634] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:19:17,635] {logging_mixin.py:115} INFO - [2023-01-04 23:19:17,635] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:18,434] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:19:18,435] {logging_mixin.py:115} INFO - [2023-01-04 23:19:18,435] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:19:18,436] {logging_mixin.py:115} INFO - [2023-01-04 23:19:18,435] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:19:18,442] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:18,463] {logging_mixin.py:115} INFO - [2023-01-04 23:19:18,463] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:19:18,491] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.864 seconds
[2023-01-04 23:19:48,560] {processor.py:153} INFO - Started process (PID=6624) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:48,562] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:19:48,562] {logging_mixin.py:115} INFO - [2023-01-04 23:19:48,562] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:49,434] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:19:49,436] {logging_mixin.py:115} INFO - [2023-01-04 23:19:49,436] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:19:49,436] {logging_mixin.py:115} INFO - [2023-01-04 23:19:49,436] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:19:49,447] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:19:49,469] {logging_mixin.py:115} INFO - [2023-01-04 23:19:49,469] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:19:49,498] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.942 seconds
[2023-01-04 23:20:19,570] {processor.py:153} INFO - Started process (PID=6647) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:19,570] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:20:19,571] {logging_mixin.py:115} INFO - [2023-01-04 23:20:19,571] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:20,347] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:20:20,349] {logging_mixin.py:115} INFO - [2023-01-04 23:20:20,348] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:20:20,349] {logging_mixin.py:115} INFO - [2023-01-04 23:20:20,349] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:20:20,355] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:20,376] {logging_mixin.py:115} INFO - [2023-01-04 23:20:20,376] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:20:20,404] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.839 seconds
[2023-01-04 23:20:50,474] {processor.py:153} INFO - Started process (PID=6670) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:50,475] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:20:50,476] {logging_mixin.py:115} INFO - [2023-01-04 23:20:50,476] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:51,273] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:20:51,275] {logging_mixin.py:115} INFO - [2023-01-04 23:20:51,274] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:20:51,275] {logging_mixin.py:115} INFO - [2023-01-04 23:20:51,275] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:20:51,282] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:20:51,303] {logging_mixin.py:115} INFO - [2023-01-04 23:20:51,303] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:20:51,334] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.865 seconds
[2023-01-04 23:21:21,408] {processor.py:153} INFO - Started process (PID=6686) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:21,410] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:21:21,410] {logging_mixin.py:115} INFO - [2023-01-04 23:21:21,410] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:22,245] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:21:22,246] {logging_mixin.py:115} INFO - [2023-01-04 23:21:22,246] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:21:22,247] {logging_mixin.py:115} INFO - [2023-01-04 23:21:22,246] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:21:22,253] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:22,274] {logging_mixin.py:115} INFO - [2023-01-04 23:21:22,274] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:21:22,303] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.900 seconds
[2023-01-04 23:21:52,400] {processor.py:153} INFO - Started process (PID=6710) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:52,401] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:21:52,402] {logging_mixin.py:115} INFO - [2023-01-04 23:21:52,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:53,173] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:21:53,174] {logging_mixin.py:115} INFO - [2023-01-04 23:21:53,174] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:21:53,175] {logging_mixin.py:115} INFO - [2023-01-04 23:21:53,174] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:21:53,181] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:21:53,203] {logging_mixin.py:115} INFO - [2023-01-04 23:21:53,202] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:21:53,231] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.835 seconds
[2023-01-04 23:22:23,327] {processor.py:153} INFO - Started process (PID=6733) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:23,329] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:22:23,329] {logging_mixin.py:115} INFO - [2023-01-04 23:22:23,329] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:24,111] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:22:24,112] {logging_mixin.py:115} INFO - [2023-01-04 23:22:24,112] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:22:24,113] {logging_mixin.py:115} INFO - [2023-01-04 23:22:24,112] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:22:24,119] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:24,140] {logging_mixin.py:115} INFO - [2023-01-04 23:22:24,140] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:22:24,168] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.845 seconds
[2023-01-04 23:22:54,261] {processor.py:153} INFO - Started process (PID=6756) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:54,262] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:22:54,263] {logging_mixin.py:115} INFO - [2023-01-04 23:22:54,263] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:55,068] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:22:55,069] {logging_mixin.py:115} INFO - [2023-01-04 23:22:55,069] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:22:55,070] {logging_mixin.py:115} INFO - [2023-01-04 23:22:55,070] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:22:55,077] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:22:55,098] {logging_mixin.py:115} INFO - [2023-01-04 23:22:55,097] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:22:55,125] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.870 seconds
[2023-01-04 23:23:25,210] {processor.py:153} INFO - Started process (PID=6772) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:25,212] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:23:25,212] {logging_mixin.py:115} INFO - [2023-01-04 23:23:25,212] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:26,034] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:23:26,035] {logging_mixin.py:115} INFO - [2023-01-04 23:23:26,035] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:23:26,036] {logging_mixin.py:115} INFO - [2023-01-04 23:23:26,035] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:23:26,042] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:26,063] {logging_mixin.py:115} INFO - [2023-01-04 23:23:26,063] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:23:26,091] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.886 seconds
[2023-01-04 23:23:56,162] {processor.py:153} INFO - Started process (PID=6796) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:56,163] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:23:56,163] {logging_mixin.py:115} INFO - [2023-01-04 23:23:56,163] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:56,954] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:23:56,955] {logging_mixin.py:115} INFO - [2023-01-04 23:23:56,955] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:23:56,956] {logging_mixin.py:115} INFO - [2023-01-04 23:23:56,955] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:23:56,962] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:23:56,983] {logging_mixin.py:115} INFO - [2023-01-04 23:23:56,983] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:23:57,012] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.854 seconds
[2023-01-04 23:24:27,084] {processor.py:153} INFO - Started process (PID=6821) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:27,085] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:24:27,086] {logging_mixin.py:115} INFO - [2023-01-04 23:24:27,086] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:27,884] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:24:27,886] {logging_mixin.py:115} INFO - [2023-01-04 23:24:27,885] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:24:27,886] {logging_mixin.py:115} INFO - [2023-01-04 23:24:27,886] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:24:27,893] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:27,916] {logging_mixin.py:115} INFO - [2023-01-04 23:24:27,916] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:24:27,949] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.869 seconds
[2023-01-04 23:24:58,020] {processor.py:153} INFO - Started process (PID=6843) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:58,021] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:24:58,022] {logging_mixin.py:115} INFO - [2023-01-04 23:24:58,022] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:58,850] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:24:58,851] {logging_mixin.py:115} INFO - [2023-01-04 23:24:58,851] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:24:58,852] {logging_mixin.py:115} INFO - [2023-01-04 23:24:58,852] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:24:58,859] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:24:58,880] {logging_mixin.py:115} INFO - [2023-01-04 23:24:58,880] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:24:58,912] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.897 seconds
[2023-01-04 23:25:28,995] {processor.py:153} INFO - Started process (PID=6858) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:25:28,997] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:25:28,998] {logging_mixin.py:115} INFO - [2023-01-04 23:25:28,998] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:25:29,852] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:25:29,853] {logging_mixin.py:115} INFO - [2023-01-04 23:25:29,853] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:25:29,854] {logging_mixin.py:115} INFO - [2023-01-04 23:25:29,853] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:25:29,860] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:25:29,882] {logging_mixin.py:115} INFO - [2023-01-04 23:25:29,882] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:25:29,911] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.921 seconds
[2023-01-04 23:25:59,981] {processor.py:153} INFO - Started process (PID=6880) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:25:59,981] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:25:59,982] {logging_mixin.py:115} INFO - [2023-01-04 23:25:59,982] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:26:00,795] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:26:00,796] {logging_mixin.py:115} INFO - [2023-01-04 23:26:00,796] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:26:00,797] {logging_mixin.py:115} INFO - [2023-01-04 23:26:00,796] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:26:00,803] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:26:00,826] {logging_mixin.py:115} INFO - [2023-01-04 23:26:00,826] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:26:00,856] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.880 seconds
[2023-01-04 23:26:30,975] {processor.py:153} INFO - Started process (PID=6903) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:26:30,976] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:26:30,976] {logging_mixin.py:115} INFO - [2023-01-04 23:26:30,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:26:31,780] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:26:31,781] {logging_mixin.py:115} INFO - [2023-01-04 23:26:31,781] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:26:31,781] {logging_mixin.py:115} INFO - [2023-01-04 23:26:31,781] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:26:31,788] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:26:31,810] {logging_mixin.py:115} INFO - [2023-01-04 23:26:31,810] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:26:31,838] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.868 seconds
[2023-01-04 23:27:01,939] {processor.py:153} INFO - Started process (PID=6926) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:01,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:27:01,941] {logging_mixin.py:115} INFO - [2023-01-04 23:27:01,941] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:02,777] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:27:02,779] {logging_mixin.py:115} INFO - [2023-01-04 23:27:02,778] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:27:02,779] {logging_mixin.py:115} INFO - [2023-01-04 23:27:02,779] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:27:02,786] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:02,807] {logging_mixin.py:115} INFO - [2023-01-04 23:27:02,807] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:27:02,834] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.901 seconds
[2023-01-04 23:27:32,933] {processor.py:153} INFO - Started process (PID=6943) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:32,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:27:32,939] {logging_mixin.py:115} INFO - [2023-01-04 23:27:32,939] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:33,764] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:27:33,766] {logging_mixin.py:115} INFO - [2023-01-04 23:27:33,765] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:27:33,766] {logging_mixin.py:115} INFO - [2023-01-04 23:27:33,766] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:27:33,773] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:27:33,797] {logging_mixin.py:115} INFO - [2023-01-04 23:27:33,797] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:27:33,827] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.901 seconds
[2023-01-04 23:28:03,923] {processor.py:153} INFO - Started process (PID=6967) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:03,924] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:28:03,925] {logging_mixin.py:115} INFO - [2023-01-04 23:28:03,925] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:04,724] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:28:04,725] {logging_mixin.py:115} INFO - [2023-01-04 23:28:04,725] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:28:04,725] {logging_mixin.py:115} INFO - [2023-01-04 23:28:04,725] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:28:04,732] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:04,759] {logging_mixin.py:115} INFO - [2023-01-04 23:28:04,758] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:28:04,787] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.869 seconds
[2023-01-04 23:28:34,880] {processor.py:153} INFO - Started process (PID=6989) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:34,881] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:28:34,882] {logging_mixin.py:115} INFO - [2023-01-04 23:28:34,881] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:35,693] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:28:35,694] {logging_mixin.py:115} INFO - [2023-01-04 23:28:35,694] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:28:35,695] {logging_mixin.py:115} INFO - [2023-01-04 23:28:35,694] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:28:35,701] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:28:35,723] {logging_mixin.py:115} INFO - [2023-01-04 23:28:35,723] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:28:35,751] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.875 seconds
[2023-01-04 23:29:05,832] {processor.py:153} INFO - Started process (PID=7012) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:05,834] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:29:05,835] {logging_mixin.py:115} INFO - [2023-01-04 23:29:05,834] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:06,692] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:29:06,694] {logging_mixin.py:115} INFO - [2023-01-04 23:29:06,694] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:29:06,694] {logging_mixin.py:115} INFO - [2023-01-04 23:29:06,694] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:29:06,701] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:06,723] {logging_mixin.py:115} INFO - [2023-01-04 23:29:06,723] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:29:06,751] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-04 23:29:36,821] {processor.py:153} INFO - Started process (PID=7028) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:36,827] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:29:36,828] {logging_mixin.py:115} INFO - [2023-01-04 23:29:36,828] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:37,681] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:29:37,682] {logging_mixin.py:115} INFO - [2023-01-04 23:29:37,682] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:29:37,683] {logging_mixin.py:115} INFO - [2023-01-04 23:29:37,683] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:29:37,690] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:29:37,712] {logging_mixin.py:115} INFO - [2023-01-04 23:29:37,711] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:29:37,739] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.923 seconds
[2023-01-04 23:30:07,808] {processor.py:153} INFO - Started process (PID=7052) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:07,809] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:30:07,809] {logging_mixin.py:115} INFO - [2023-01-04 23:30:07,809] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:08,622] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:30:08,623] {logging_mixin.py:115} INFO - [2023-01-04 23:30:08,623] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:30:08,624] {logging_mixin.py:115} INFO - [2023-01-04 23:30:08,624] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:30:08,631] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:08,652] {logging_mixin.py:115} INFO - [2023-01-04 23:30:08,652] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:30:08,673] {logging_mixin.py:115} INFO - [2023-01-04 23:30:08,672] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 23:30:08,682] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.878 seconds
[2023-01-04 23:30:38,753] {processor.py:153} INFO - Started process (PID=7076) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:38,754] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:30:38,755] {logging_mixin.py:115} INFO - [2023-01-04 23:30:38,755] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:39,538] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:30:39,539] {logging_mixin.py:115} INFO - [2023-01-04 23:30:39,539] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:30:39,540] {logging_mixin.py:115} INFO - [2023-01-04 23:30:39,539] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:30:39,546] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:39,568] {logging_mixin.py:115} INFO - [2023-01-04 23:30:39,568] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:30:39,589] {logging_mixin.py:115} INFO - [2023-01-04 23:30:39,589] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 23:30:39,598] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.850 seconds
[2023-01-04 23:30:45,408] {processor.py:153} INFO - Started process (PID=7078) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:45,408] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:30:45,409] {logging_mixin.py:115} INFO - [2023-01-04 23:30:45,409] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:46,207] {logging_mixin.py:115} INFO - [2023-01-04 23:30:46,206] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:30:46,207] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:30:46,226] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.823 seconds
[2023-01-04 23:31:16,324] {processor.py:153} INFO - Started process (PID=7101) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:16,328] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:31:16,329] {logging_mixin.py:115} INFO - [2023-01-04 23:31:16,329] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:17,061] {logging_mixin.py:115} INFO - [2023-01-04 23:31:17,059] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:31:17,061] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:17,085] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.765 seconds
[2023-01-04 23:31:47,167] {processor.py:153} INFO - Started process (PID=7125) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:47,168] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:31:47,169] {logging_mixin.py:115} INFO - [2023-01-04 23:31:47,169] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:47,883] {logging_mixin.py:115} INFO - [2023-01-04 23:31:47,882] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:31:47,883] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:31:47,900] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.738 seconds
[2023-01-04 23:32:17,972] {processor.py:153} INFO - Started process (PID=7147) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:17,976] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:32:17,976] {logging_mixin.py:115} INFO - [2023-01-04 23:32:17,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:18,721] {logging_mixin.py:115} INFO - [2023-01-04 23:32:18,720] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:32:18,721] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:18,739] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.772 seconds
[2023-01-04 23:32:48,847] {processor.py:153} INFO - Started process (PID=7164) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:48,848] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:32:48,849] {logging_mixin.py:115} INFO - [2023-01-04 23:32:48,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:49,612] {logging_mixin.py:115} INFO - [2023-01-04 23:32:49,611] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:32:49,613] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:32:49,630] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.788 seconds
[2023-01-04 23:33:19,728] {processor.py:153} INFO - Started process (PID=7187) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:33:19,729] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:33:19,730] {logging_mixin.py:115} INFO - [2023-01-04 23:33:19,730] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:33:20,465] {logging_mixin.py:115} INFO - [2023-01-04 23:33:20,464] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/stock_data_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/stock_data_pipeline_dag.py", line 3, in <module>
    import Load_Current_SP_500_End_of_Day_Prices_Into_GCS
  File "/opt/airflow/dags/Load_Current_SP_500_End_of_Day_Prices_Into_GCS.py", line 9, in <module>
    from tiingo import TiingoClient
ModuleNotFoundError: No module named 'tiingo'
[2023-01-04 23:33:20,466] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:33:20,483] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.760 seconds
[2023-01-04 23:33:50,557] {processor.py:153} INFO - Started process (PID=7210) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:33:50,559] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:33:50,559] {logging_mixin.py:115} INFO - [2023-01-04 23:33:50,559] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:37:29,049] {processor.py:153} INFO - Started process (PID=32) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:37:29,050] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:37:29,051] {logging_mixin.py:115} INFO - [2023-01-04 23:37:29,051] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:37:32,744] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:37:32,745] {logging_mixin.py:115} INFO - [2023-01-04 23:37:32,745] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:37:32,746] {logging_mixin.py:115} INFO - [2023-01-04 23:37:32,746] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:37:32,753] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:37:33,026] {logging_mixin.py:115} INFO - [2023-01-04 23:37:33,026] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:37:33,059] {logging_mixin.py:115} INFO - [2023-01-04 23:37:33,059] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 23:37:33,080] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 4.035 seconds
[2023-01-04 23:38:03,135] {processor.py:153} INFO - Started process (PID=55) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:03,136] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:38:03,137] {logging_mixin.py:115} INFO - [2023-01-04 23:38:03,137] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:04,125] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:38:04,127] {logging_mixin.py:115} INFO - [2023-01-04 23:38:04,127] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:38:04,127] {logging_mixin.py:115} INFO - [2023-01-04 23:38:04,127] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:38:04,134] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:04,161] {logging_mixin.py:115} INFO - [2023-01-04 23:38:04,161] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:38:04,183] {logging_mixin.py:115} INFO - [2023-01-04 23:38:04,183] {dag.py:2927} INFO - Setting next_dagrun for stock_data_pipeline_dag to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00
[2023-01-04 23:38:04,194] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.067 seconds
[2023-01-04 23:38:34,232] {processor.py:153} INFO - Started process (PID=70) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:34,233] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:38:34,234] {logging_mixin.py:115} INFO - [2023-01-04 23:38:34,234] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:35,328] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:38:35,329] {logging_mixin.py:115} INFO - [2023-01-04 23:38:35,329] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:38:35,330] {logging_mixin.py:115} INFO - [2023-01-04 23:38:35,330] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:38:35,338] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:38:35,361] {logging_mixin.py:115} INFO - [2023-01-04 23:38:35,361] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:38:35,388] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.161 seconds
[2023-01-04 23:39:05,452] {processor.py:153} INFO - Started process (PID=93) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:05,453] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:39:05,454] {logging_mixin.py:115} INFO - [2023-01-04 23:39:05,454] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:06,318] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:39:06,319] {logging_mixin.py:115} INFO - [2023-01-04 23:39:06,319] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:39:06,319] {logging_mixin.py:115} INFO - [2023-01-04 23:39:06,319] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:39:06,326] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:06,347] {logging_mixin.py:115} INFO - [2023-01-04 23:39:06,347] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:39:06,374] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-04 23:39:36,440] {processor.py:153} INFO - Started process (PID=116) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:36,445] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:39:36,446] {logging_mixin.py:115} INFO - [2023-01-04 23:39:36,446] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:37,360] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:39:37,361] {logging_mixin.py:115} INFO - [2023-01-04 23:39:37,361] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:39:37,362] {logging_mixin.py:115} INFO - [2023-01-04 23:39:37,362] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:39:37,371] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:39:37,396] {logging_mixin.py:115} INFO - [2023-01-04 23:39:37,396] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:39:37,422] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.987 seconds
[2023-01-04 23:40:07,509] {processor.py:153} INFO - Started process (PID=139) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:07,513] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:40:07,513] {logging_mixin.py:115} INFO - [2023-01-04 23:40:07,513] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:08,387] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:40:08,389] {logging_mixin.py:115} INFO - [2023-01-04 23:40:08,389] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:40:08,389] {logging_mixin.py:115} INFO - [2023-01-04 23:40:08,389] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:40:08,396] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:08,418] {logging_mixin.py:115} INFO - [2023-01-04 23:40:08,418] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:40:08,445] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.940 seconds
[2023-01-04 23:40:38,542] {processor.py:153} INFO - Started process (PID=154) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:38,548] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:40:38,549] {logging_mixin.py:115} INFO - [2023-01-04 23:40:38,549] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:39,688] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:40:39,689] {logging_mixin.py:115} INFO - [2023-01-04 23:40:39,689] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:40:39,689] {logging_mixin.py:115} INFO - [2023-01-04 23:40:39,689] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:40:39,696] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:40:39,718] {logging_mixin.py:115} INFO - [2023-01-04 23:40:39,718] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:40:39,746] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.209 seconds
[2023-01-04 23:41:09,820] {processor.py:153} INFO - Started process (PID=179) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:09,822] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:41:09,822] {logging_mixin.py:115} INFO - [2023-01-04 23:41:09,822] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:10,791] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:41:10,792] {logging_mixin.py:115} INFO - [2023-01-04 23:41:10,792] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:41:10,793] {logging_mixin.py:115} INFO - [2023-01-04 23:41:10,792] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:41:10,799] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:10,822] {logging_mixin.py:115} INFO - [2023-01-04 23:41:10,822] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:41:10,849] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.034 seconds
[2023-01-04 23:41:40,893] {processor.py:153} INFO - Started process (PID=203) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:40,894] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:41:40,894] {logging_mixin.py:115} INFO - [2023-01-04 23:41:40,894] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:41,764] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:41:41,766] {logging_mixin.py:115} INFO - [2023-01-04 23:41:41,766] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:41:41,766] {logging_mixin.py:115} INFO - [2023-01-04 23:41:41,766] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:41:41,773] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:41:41,796] {logging_mixin.py:115} INFO - [2023-01-04 23:41:41,796] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:41:41,822] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.934 seconds
[2023-01-04 23:42:11,917] {processor.py:153} INFO - Started process (PID=226) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:11,921] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:42:11,921] {logging_mixin.py:115} INFO - [2023-01-04 23:42:11,921] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:12,798] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:42:12,799] {logging_mixin.py:115} INFO - [2023-01-04 23:42:12,799] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:42:12,800] {logging_mixin.py:115} INFO - [2023-01-04 23:42:12,799] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:42:12,807] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:12,828] {logging_mixin.py:115} INFO - [2023-01-04 23:42:12,828] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:42:12,856] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-04 23:42:42,964] {processor.py:153} INFO - Started process (PID=240) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:42,965] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:42:42,965] {logging_mixin.py:115} INFO - [2023-01-04 23:42:42,965] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:44,184] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:42:44,186] {logging_mixin.py:115} INFO - [2023-01-04 23:42:44,186] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:42:44,186] {logging_mixin.py:115} INFO - [2023-01-04 23:42:44,186] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:42:44,193] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:42:44,221] {logging_mixin.py:115} INFO - [2023-01-04 23:42:44,220] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:42:44,259] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.300 seconds
[2023-01-04 23:43:14,333] {processor.py:153} INFO - Started process (PID=263) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:14,338] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:43:14,338] {logging_mixin.py:115} INFO - [2023-01-04 23:43:14,338] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:15,212] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:43:15,214] {logging_mixin.py:115} INFO - [2023-01-04 23:43:15,214] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:43:15,214] {logging_mixin.py:115} INFO - [2023-01-04 23:43:15,214] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:43:15,225] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:15,253] {logging_mixin.py:115} INFO - [2023-01-04 23:43:15,253] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:43:15,289] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.961 seconds
[2023-01-04 23:43:45,359] {processor.py:153} INFO - Started process (PID=287) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:45,359] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:43:45,360] {logging_mixin.py:115} INFO - [2023-01-04 23:43:45,360] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:46,281] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:43:46,282] {logging_mixin.py:115} INFO - [2023-01-04 23:43:46,282] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:43:46,283] {logging_mixin.py:115} INFO - [2023-01-04 23:43:46,283] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:43:46,290] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:43:46,311] {logging_mixin.py:115} INFO - [2023-01-04 23:43:46,311] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:43:46,338] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-04 23:44:16,435] {processor.py:153} INFO - Started process (PID=309) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:16,436] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:44:16,437] {logging_mixin.py:115} INFO - [2023-01-04 23:44:16,437] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:17,357] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:44:17,358] {logging_mixin.py:115} INFO - [2023-01-04 23:44:17,358] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:44:17,358] {logging_mixin.py:115} INFO - [2023-01-04 23:44:17,358] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:44:17,365] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:17,388] {logging_mixin.py:115} INFO - [2023-01-04 23:44:17,387] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:44:17,415] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.984 seconds
[2023-01-04 23:44:47,502] {processor.py:153} INFO - Started process (PID=325) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:47,504] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:44:47,505] {logging_mixin.py:115} INFO - [2023-01-04 23:44:47,504] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:48,418] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:44:48,419] {logging_mixin.py:115} INFO - [2023-01-04 23:44:48,419] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:44:48,420] {logging_mixin.py:115} INFO - [2023-01-04 23:44:48,419] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:44:48,426] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:44:48,449] {logging_mixin.py:115} INFO - [2023-01-04 23:44:48,448] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:44:48,475] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.977 seconds
[2023-01-04 23:45:18,566] {processor.py:153} INFO - Started process (PID=349) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:18,567] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:45:18,568] {logging_mixin.py:115} INFO - [2023-01-04 23:45:18,567] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:19,456] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:45:19,457] {logging_mixin.py:115} INFO - [2023-01-04 23:45:19,457] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:45:19,458] {logging_mixin.py:115} INFO - [2023-01-04 23:45:19,458] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:45:19,465] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:19,486] {logging_mixin.py:115} INFO - [2023-01-04 23:45:19,486] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:45:19,512] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.950 seconds
[2023-01-04 23:45:49,611] {processor.py:153} INFO - Started process (PID=372) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:49,612] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:45:49,613] {logging_mixin.py:115} INFO - [2023-01-04 23:45:49,613] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:50,513] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:45:50,514] {logging_mixin.py:115} INFO - [2023-01-04 23:45:50,514] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:45:50,515] {logging_mixin.py:115} INFO - [2023-01-04 23:45:50,515] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:45:50,522] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:45:50,544] {logging_mixin.py:115} INFO - [2023-01-04 23:45:50,544] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:45:50,572] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.965 seconds
[2023-01-04 23:46:20,664] {processor.py:153} INFO - Started process (PID=395) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:20,665] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:46:20,665] {logging_mixin.py:115} INFO - [2023-01-04 23:46:20,665] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:21,527] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:46:21,528] {logging_mixin.py:115} INFO - [2023-01-04 23:46:21,528] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:46:21,529] {logging_mixin.py:115} INFO - [2023-01-04 23:46:21,528] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:46:21,535] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:21,558] {logging_mixin.py:115} INFO - [2023-01-04 23:46:21,557] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:46:21,585] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.925 seconds
[2023-01-04 23:46:51,684] {processor.py:153} INFO - Started process (PID=411) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:51,685] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:46:51,686] {logging_mixin.py:115} INFO - [2023-01-04 23:46:51,686] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:52,661] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:46:52,662] {logging_mixin.py:115} INFO - [2023-01-04 23:46:52,662] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:46:52,663] {logging_mixin.py:115} INFO - [2023-01-04 23:46:52,662] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:46:52,669] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:46:52,700] {logging_mixin.py:115} INFO - [2023-01-04 23:46:52,699] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:46:52,735] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.055 seconds
[2023-01-04 23:47:22,774] {processor.py:153} INFO - Started process (PID=433) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:22,774] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:47:22,775] {logging_mixin.py:115} INFO - [2023-01-04 23:47:22,775] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:23,710] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:47:23,711] {logging_mixin.py:115} INFO - [2023-01-04 23:47:23,711] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:47:23,712] {logging_mixin.py:115} INFO - [2023-01-04 23:47:23,711] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:47:23,719] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:23,743] {logging_mixin.py:115} INFO - [2023-01-04 23:47:23,743] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:47:23,773] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.004 seconds
[2023-01-04 23:47:53,857] {processor.py:153} INFO - Started process (PID=456) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:53,858] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:47:53,858] {logging_mixin.py:115} INFO - [2023-01-04 23:47:53,858] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:54,761] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:47:54,763] {logging_mixin.py:115} INFO - [2023-01-04 23:47:54,763] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:47:54,763] {logging_mixin.py:115} INFO - [2023-01-04 23:47:54,763] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:47:54,770] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:47:54,792] {logging_mixin.py:115} INFO - [2023-01-04 23:47:54,792] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:47:54,821] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-04 23:48:24,912] {processor.py:153} INFO - Started process (PID=480) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:24,913] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:48:24,914] {logging_mixin.py:115} INFO - [2023-01-04 23:48:24,914] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:25,832] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:48:25,834] {logging_mixin.py:115} INFO - [2023-01-04 23:48:25,833] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:48:25,834] {logging_mixin.py:115} INFO - [2023-01-04 23:48:25,834] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:48:25,841] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:25,863] {logging_mixin.py:115} INFO - [2023-01-04 23:48:25,863] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:48:25,890] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.982 seconds
[2023-01-04 23:48:55,980] {processor.py:153} INFO - Started process (PID=496) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:55,981] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:48:55,982] {logging_mixin.py:115} INFO - [2023-01-04 23:48:55,981] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:56,961] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:48:56,963] {logging_mixin.py:115} INFO - [2023-01-04 23:48:56,963] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:48:56,964] {logging_mixin.py:115} INFO - [2023-01-04 23:48:56,963] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:48:56,975] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:48:57,006] {logging_mixin.py:115} INFO - [2023-01-04 23:48:57,005] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:48:57,044] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.069 seconds
[2023-01-04 23:49:27,111] {processor.py:153} INFO - Started process (PID=518) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:27,112] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:49:27,112] {logging_mixin.py:115} INFO - [2023-01-04 23:49:27,112] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:27,979] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:49:27,980] {logging_mixin.py:115} INFO - [2023-01-04 23:49:27,980] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:49:27,981] {logging_mixin.py:115} INFO - [2023-01-04 23:49:27,981] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:49:27,987] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:28,009] {logging_mixin.py:115} INFO - [2023-01-04 23:49:28,009] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:49:28,035] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.929 seconds
[2023-01-04 23:49:58,118] {processor.py:153} INFO - Started process (PID=542) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:58,120] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:49:58,120] {logging_mixin.py:115} INFO - [2023-01-04 23:49:58,120] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:58,976] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:49:58,977] {logging_mixin.py:115} INFO - [2023-01-04 23:49:58,977] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:49:58,978] {logging_mixin.py:115} INFO - [2023-01-04 23:49:58,977] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:49:58,985] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:49:59,006] {logging_mixin.py:115} INFO - [2023-01-04 23:49:59,006] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:49:59,034] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.920 seconds
[2023-01-04 23:50:29,129] {processor.py:153} INFO - Started process (PID=565) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:50:29,130] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:50:29,130] {logging_mixin.py:115} INFO - [2023-01-04 23:50:29,130] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:50:30,073] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:50:30,074] {logging_mixin.py:115} INFO - [2023-01-04 23:50:30,074] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:50:30,074] {logging_mixin.py:115} INFO - [2023-01-04 23:50:30,074] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:50:30,081] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:50:30,111] {logging_mixin.py:115} INFO - [2023-01-04 23:50:30,110] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:50:30,147] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.023 seconds
[2023-01-04 23:51:00,230] {processor.py:153} INFO - Started process (PID=581) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:00,232] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:51:00,232] {logging_mixin.py:115} INFO - [2023-01-04 23:51:00,232] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:01,225] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:51:01,227] {logging_mixin.py:115} INFO - [2023-01-04 23:51:01,227] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:51:01,227] {logging_mixin.py:115} INFO - [2023-01-04 23:51:01,227] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:51:01,234] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:01,257] {logging_mixin.py:115} INFO - [2023-01-04 23:51:01,257] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:51:01,284] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.058 seconds
[2023-01-04 23:51:31,352] {processor.py:153} INFO - Started process (PID=604) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:31,353] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:51:31,353] {logging_mixin.py:115} INFO - [2023-01-04 23:51:31,353] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:32,216] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:51:32,218] {logging_mixin.py:115} INFO - [2023-01-04 23:51:32,218] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:51:32,219] {logging_mixin.py:115} INFO - [2023-01-04 23:51:32,218] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:51:32,225] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:51:32,247] {logging_mixin.py:115} INFO - [2023-01-04 23:51:32,246] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:51:32,274] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.926 seconds
[2023-01-04 23:52:02,365] {processor.py:153} INFO - Started process (PID=627) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:02,369] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:52:02,369] {logging_mixin.py:115} INFO - [2023-01-04 23:52:02,369] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:03,277] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:52:03,279] {logging_mixin.py:115} INFO - [2023-01-04 23:52:03,278] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:52:03,279] {logging_mixin.py:115} INFO - [2023-01-04 23:52:03,279] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:52:03,286] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:03,308] {logging_mixin.py:115} INFO - [2023-01-04 23:52:03,308] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:52:03,335] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.974 seconds
[2023-01-04 23:52:33,441] {processor.py:153} INFO - Started process (PID=651) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:33,441] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:52:33,442] {logging_mixin.py:115} INFO - [2023-01-04 23:52:33,442] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:34,317] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:52:34,318] {logging_mixin.py:115} INFO - [2023-01-04 23:52:34,318] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:52:34,318] {logging_mixin.py:115} INFO - [2023-01-04 23:52:34,318] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:52:34,325] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:52:34,347] {logging_mixin.py:115} INFO - [2023-01-04 23:52:34,347] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:52:34,374] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.938 seconds
[2023-01-04 23:53:04,526] {processor.py:153} INFO - Started process (PID=667) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:04,528] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:53:04,528] {logging_mixin.py:115} INFO - [2023-01-04 23:53:04,528] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:07,226] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:53:07,228] {logging_mixin.py:115} INFO - [2023-01-04 23:53:07,228] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:53:07,229] {logging_mixin.py:115} INFO - [2023-01-04 23:53:07,229] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:53:07,240] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:07,278] {logging_mixin.py:115} INFO - [2023-01-04 23:53:07,277] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:53:07,321] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 2.820 seconds
[2023-01-04 23:53:37,425] {processor.py:153} INFO - Started process (PID=690) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:37,427] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:53:37,427] {logging_mixin.py:115} INFO - [2023-01-04 23:53:37,427] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:38,320] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:53:38,322] {logging_mixin.py:115} INFO - [2023-01-04 23:53:38,322] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:53:38,322] {logging_mixin.py:115} INFO - [2023-01-04 23:53:38,322] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:53:38,329] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:53:38,351] {logging_mixin.py:115} INFO - [2023-01-04 23:53:38,351] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:53:38,378] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.957 seconds
[2023-01-04 23:54:08,476] {processor.py:153} INFO - Started process (PID=713) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:08,477] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:54:08,477] {logging_mixin.py:115} INFO - [2023-01-04 23:54:08,477] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:09,348] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:54:09,349] {logging_mixin.py:115} INFO - [2023-01-04 23:54:09,349] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:54:09,350] {logging_mixin.py:115} INFO - [2023-01-04 23:54:09,349] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:54:09,356] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:09,379] {logging_mixin.py:115} INFO - [2023-01-04 23:54:09,378] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:54:09,406] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.935 seconds
[2023-01-04 23:54:39,503] {processor.py:153} INFO - Started process (PID=735) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:39,505] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:54:39,505] {logging_mixin.py:115} INFO - [2023-01-04 23:54:39,505] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:40,451] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:54:40,453] {logging_mixin.py:115} INFO - [2023-01-04 23:54:40,453] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:54:40,454] {logging_mixin.py:115} INFO - [2023-01-04 23:54:40,453] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:54:40,460] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:54:40,488] {logging_mixin.py:115} INFO - [2023-01-04 23:54:40,488] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:54:40,520] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.021 seconds
[2023-01-04 23:55:10,573] {processor.py:153} INFO - Started process (PID=751) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:10,574] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:55:10,575] {logging_mixin.py:115} INFO - [2023-01-04 23:55:10,574] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:11,530] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:55:11,532] {logging_mixin.py:115} INFO - [2023-01-04 23:55:11,532] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:55:11,532] {logging_mixin.py:115} INFO - [2023-01-04 23:55:11,532] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:55:11,539] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:11,562] {logging_mixin.py:115} INFO - [2023-01-04 23:55:11,561] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:55:11,589] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.020 seconds
[2023-01-04 23:55:41,657] {processor.py:153} INFO - Started process (PID=773) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:41,658] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:55:41,659] {logging_mixin.py:115} INFO - [2023-01-04 23:55:41,659] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:42,547] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:55:42,549] {logging_mixin.py:115} INFO - [2023-01-04 23:55:42,549] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:55:42,549] {logging_mixin.py:115} INFO - [2023-01-04 23:55:42,549] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:55:42,556] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:55:42,578] {logging_mixin.py:115} INFO - [2023-01-04 23:55:42,577] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:55:42,605] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.953 seconds
[2023-01-04 23:56:12,702] {processor.py:153} INFO - Started process (PID=797) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:12,703] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:56:12,704] {logging_mixin.py:115} INFO - [2023-01-04 23:56:12,704] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:13,704] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:56:13,706] {logging_mixin.py:115} INFO - [2023-01-04 23:56:13,706] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:56:13,707] {logging_mixin.py:115} INFO - [2023-01-04 23:56:13,706] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:56:13,714] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:13,741] {logging_mixin.py:115} INFO - [2023-01-04 23:56:13,741] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:56:13,772] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 1.075 seconds
[2023-01-04 23:56:43,837] {processor.py:153} INFO - Started process (PID=819) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:43,838] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:56:43,838] {logging_mixin.py:115} INFO - [2023-01-04 23:56:43,838] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:44,717] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:56:44,719] {logging_mixin.py:115} INFO - [2023-01-04 23:56:44,719] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:56:44,719] {logging_mixin.py:115} INFO - [2023-01-04 23:56:44,719] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:56:44,726] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:56:44,748] {logging_mixin.py:115} INFO - [2023-01-04 23:56:44,748] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:56:44,776] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.943 seconds
[2023-01-04 23:57:14,851] {processor.py:153} INFO - Started process (PID=833) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:14,852] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:57:14,853] {logging_mixin.py:115} INFO - [2023-01-04 23:57:14,852] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:15,744] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:57:15,746] {logging_mixin.py:115} INFO - [2023-01-04 23:57:15,746] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:57:15,746] {logging_mixin.py:115} INFO - [2023-01-04 23:57:15,746] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:57:15,753] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:15,776] {logging_mixin.py:115} INFO - [2023-01-04 23:57:15,776] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:57:15,804] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.958 seconds
[2023-01-04 23:57:45,913] {processor.py:153} INFO - Started process (PID=857) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:45,914] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:57:45,914] {logging_mixin.py:115} INFO - [2023-01-04 23:57:45,914] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:46,811] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:57:46,812] {logging_mixin.py:115} INFO - [2023-01-04 23:57:46,812] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:57:46,812] {logging_mixin.py:115} INFO - [2023-01-04 23:57:46,812] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:57:46,819] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:57:46,845] {logging_mixin.py:115} INFO - [2023-01-04 23:57:46,845] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:57:46,877] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.968 seconds
[2023-01-04 23:58:16,969] {processor.py:153} INFO - Started process (PID=881) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:16,970] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:58:16,971] {logging_mixin.py:115} INFO - [2023-01-04 23:58:16,970] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:17,856] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:58:17,857] {logging_mixin.py:115} INFO - [2023-01-04 23:58:17,857] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:58:17,858] {logging_mixin.py:115} INFO - [2023-01-04 23:58:17,857] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:58:17,864] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:17,886] {logging_mixin.py:115} INFO - [2023-01-04 23:58:17,886] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:58:17,915] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.950 seconds
[2023-01-04 23:58:48,016] {processor.py:153} INFO - Started process (PID=904) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:48,018] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:58:48,018] {logging_mixin.py:115} INFO - [2023-01-04 23:58:48,018] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:48,907] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:58:48,909] {logging_mixin.py:115} INFO - [2023-01-04 23:58:48,909] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:58:48,909] {logging_mixin.py:115} INFO - [2023-01-04 23:58:48,909] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:58:48,916] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:58:48,938] {logging_mixin.py:115} INFO - [2023-01-04 23:58:48,937] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:58:48,965] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.953 seconds
[2023-01-04 23:59:19,060] {processor.py:153} INFO - Started process (PID=922) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:19,061] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:59:19,061] {logging_mixin.py:115} INFO - [2023-01-04 23:59:19,061] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:19,949] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:59:19,951] {logging_mixin.py:115} INFO - [2023-01-04 23:59:19,951] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:59:19,952] {logging_mixin.py:115} INFO - [2023-01-04 23:59:19,952] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:59:19,963] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:19,993] {logging_mixin.py:115} INFO - [2023-01-04 23:59:19,992] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:59:20,047] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.991 seconds
[2023-01-04 23:59:50,136] {processor.py:153} INFO - Started process (PID=945) to work on /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:50,140] {processor.py:641} INFO - Processing file /opt/airflow/dags/stock_data_pipeline_dag.py for tasks to queue
[2023-01-04 23:59:50,140] {logging_mixin.py:115} INFO - [2023-01-04 23:59:50,140] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:51,052] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py:390 DeprecationWarning: The `DataprocSubmitPySparkJobOperator` operator is deprecated, please use `DataprocSubmitJobOperator` instead. You can use `generate_job` method of `DataprocSubmitPySparkJobOperator` to generate dictionary representing your job and use it with the new operator.
[2023-01-04 23:59:51,054] {logging_mixin.py:115} INFO - [2023-01-04 23:59:51,054] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-01-04 23:59:51,054] {logging_mixin.py:115} INFO - [2023-01-04 23:59:51,054] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-04 23:59:51,061] {processor.py:651} INFO - DAG(s) dict_keys(['stock_data_pipeline_dag']) retrieved from /opt/airflow/dags/stock_data_pipeline_dag.py
[2023-01-04 23:59:51,087] {logging_mixin.py:115} INFO - [2023-01-04 23:59:51,086] {dag.py:2379} INFO - Sync 1 DAGs
[2023-01-04 23:59:51,115] {processor.py:161} INFO - Processing /opt/airflow/dags/stock_data_pipeline_dag.py took 0.983 seconds
